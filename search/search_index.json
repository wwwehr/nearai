{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NEAR AI: Building a Truly Open AI","text":"<p>Welcome! NEAR AI is a toolkit to help build, measure, and deploy AI systems focused on agents.</p> <p>Driven by one of the minds behinds TensorFlow and the Transformer Architecture, NEAR AI puts you back in control. Your data stays yours, and your AI works for you, with no compromises on privacy or ownership.</p> <ul> <li> <p> NEAR AI Agents</p> <p>Autonomous system that can interact with you and use tools to solve tasks</p> <p>  Quickstart  Registry  Tools </p> </li> <li> <p> AI Models</p> <p>Best in class AI models that you can use and fine-tune to solve your tasks</p> <p>  Benchmarks  Fine-Tunning </p> </li> <li> <p> Web Hub </p> <p>Use our Web Hub to discover agents, datasets, and models with ease</p> <p>  Agents  Models  Datasets </p> </li> <li> <p> Community </p> <p>Join our community! Get help and contribute to the future of AI</p> <p> Community</p> </li> </ul> <p>Alpha</p> <p>NEAR AI is currently on its alpha stage. We are actively working on improving the software and would love your help.</p> <p>If you would like to help build our future, please see our contributing guide.</p>"},{"location":"agent_triggers/","title":"Agent Triggers","text":"<p>Agents can be triggered by having them listen to a named event source.</p>"},{"location":"agent_triggers/#twitter-x","title":"Twitter (X)","text":"<p>NearAI maintains a read-only Twitter interface that can be used to trigger agents under certain conditions. </p>"},{"location":"agent_triggers/#the-x_mentions-event-source","title":"The x_mentions event source","text":"<p>The <code>x_mentions</code> event source produces an event when a configured account is mentioned.</p> <p>To have an agent listen for mentions, create trigger metadata in the agent's metadata.json file as in the example below.</p> <p>To trigger an agent, mention the X account configured in the metadata.json. The agent will be invoked.</p> <pre><code>{\n  \"name\": \"near-secret-agent\",\n  \"version\": \"0.0.1\",\n  \"description\": \"An example agent that responds to Twitter mentions\",\n  \"category\": \"agent\",\n  \"tags\": [\"twitter\"],\n  \"details\": {\n    \"agent\": {\n      \"welcome\": {\n        \"title\": \"No chat interface\",\n        \"description\": \"To use tweet a message and mention @nearsecretagent.\"\n      },\n      \"defaults\": {\n        \"max_iterations\": 1,\n        \"model\": \"llama-v3p2-3b-instruct\",\n        \"model_provider\": \"fireworks\",\n        \"model_temperature\": 0.0,\n        \"model_max_tokens\": 1000\n      }\n    },\n    \"triggers\": {\n      \"events\" : {\n        \"x_mentions\": [\"@nearsecretagent\"]\n      }\n    }\n  },\n  \"show_entry\": true\n}\n</code></pre>"},{"location":"agent_triggers/#posting-to-twitter-x","title":"Posting to Twitter (X)","text":"<p>To allow your agent to post to X you will need your own developer api key. Free X developer accounts have low read limits but fairly high write limits.</p> <p>NearAI Runners include the <code>tweepy</code> library, which supports several ways to authenticate with X https://docs.tweepy.org/en/stable/authentication.html</p> <p>The example agent https://app.near.ai/agents/flatirons.near/near-secret-agent/latest/source uses 3 legged Oauth to  authorize an X account other than the developer account to post through the api as described here in the twitter docs. To accomplish this it has four secrets set on the agent: X_ACCESS_TOKEN, X_ACCESS_TOKEN_SECRET, X_CONSUMER_KEY, X_CONSUMER_SECRET.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#nearai","title":"nearai","text":""},{"location":"api/#nearai.EntryLocation","title":"EntryLocation","text":"<p>               Bases: <code>BaseModel</code></p> <p>EntryLocation</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>class EntryLocation(BaseModel):\n    \"\"\"\n    EntryLocation\n    \"\"\" # noqa: E501\n    namespace: StrictStr\n    name: StrictStr\n    version: StrictStr\n    __properties: ClassVar[List[str]] = [\"namespace\", \"name\", \"version\"]\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        validate_assignment=True,\n        protected_namespaces=(),\n    )\n\n\n    def to_str(self) -&gt; str:\n        \"\"\"Returns the string representation of the model using alias\"\"\"\n        return pprint.pformat(self.model_dump(by_alias=True))\n\n    def to_json(self) -&gt; str:\n        \"\"\"Returns the JSON representation of the model using alias\"\"\"\n        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n        return json.dumps(self.to_dict())\n\n    @classmethod\n    def from_json(cls, json_str: str) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the dictionary representation of the model using alias.\n\n        This has the following differences from calling pydantic's\n        `self.model_dump(by_alias=True)`:\n\n        * `None` is only added to the output dict for nullable fields that\n          were set at model initialization. Other fields with value `None`\n          are ignored.\n        \"\"\"\n        excluded_fields: Set[str] = set([\n        ])\n\n        _dict = self.model_dump(\n            by_alias=True,\n            exclude=excluded_fields,\n            exclude_none=True,\n        )\n        return _dict\n\n    @classmethod\n    def from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n        if obj is None:\n            return None\n\n        if not isinstance(obj, dict):\n            return cls.model_validate(obj)\n\n        _obj = cls.model_validate({\n            \"namespace\": obj.get(\"namespace\"),\n            \"name\": obj.get(\"name\"),\n            \"version\": obj.get(\"version\")\n        })\n        return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a dict</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n    if obj is None:\n        return None\n\n    if not isinstance(obj, dict):\n        return cls.model_validate(obj)\n\n    _obj = cls.model_validate({\n        \"namespace\": obj.get(\"namespace\"),\n        \"name\": obj.get(\"name\"),\n        \"version\": obj.get(\"version\")\n    })\n    return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(json_str: str) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a JSON string</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n    return cls.from_dict(json.loads(json_str))\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Return the dictionary representation of the model using alias.</p> <p>This has the following differences from calling pydantic's <code>self.model_dump(by_alias=True)</code>:</p> <ul> <li><code>None</code> is only added to the output dict for nullable fields that   were set at model initialization. Other fields with value <code>None</code>   are ignored.</li> </ul> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the dictionary representation of the model using alias.\n\n    This has the following differences from calling pydantic's\n    `self.model_dump(by_alias=True)`:\n\n    * `None` is only added to the output dict for nullable fields that\n      were set at model initialization. Other fields with value `None`\n      are ignored.\n    \"\"\"\n    excluded_fields: Set[str] = set([\n    ])\n\n    _dict = self.model_dump(\n        by_alias=True,\n        exclude=excluded_fields,\n        exclude_none=True,\n    )\n    return _dict\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Returns the JSON representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Returns the JSON representation of the model using alias\"\"\"\n    # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Returns the string representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Returns the string representation of the model using alias\"\"\"\n    return pprint.pformat(self.model_dump(by_alias=True))\n</code></pre>"},{"location":"api/#nearai.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.agents","title":"agents","text":""},{"location":"api/#nearai.agents.agent","title":"agent","text":""},{"location":"api/#nearai.agents.agent.Agent","title":"Agent","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/agent.py</code> <pre><code>class Agent(object):\n    def __init__(  # noqa: D107\n        self, identifier: str, agent_files: Union[List, Path], metadata: Dict, change_to_temp_dir: bool = True\n    ):  # noqa: D107\n        self.identifier = identifier\n        name_parts = identifier.split(\"/\")\n        self.namespace = name_parts[0]\n        self.name = name_parts[1]\n        self.version = name_parts[2]\n\n        self.metadata = metadata\n        self.env_vars: Dict[str, Any] = {}\n\n        self.model = \"\"\n        self.model_provider = \"\"\n        self.model_temperature: Optional[float] = None\n        self.model_max_tokens: Optional[int] = None\n        self.max_iterations = 1\n        self.welcome_title: Optional[str] = None\n        self.welcome_description: Optional[str] = None\n\n        self.set_agent_metadata(metadata)\n        self.agent_files = agent_files\n        self.original_cwd = os.getcwd()\n\n        self.temp_dir = self.write_agent_files_to_temp(agent_files)\n        self.change_to_temp_dir = change_to_temp_dir\n\n    def get_full_name(self):\n        \"\"\"Returns full agent name.\"\"\"\n        return f\"{self.namespace}/{self.name}/{self.version}\"\n\n    @staticmethod\n    def write_agent_files_to_temp(agent_files):\n        \"\"\"Write agent files to a temporary directory.\"\"\"\n        unique_id = uuid.uuid4().hex\n        temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n        if isinstance(agent_files, List):\n            os.makedirs(temp_dir, exist_ok=True)\n\n            for file_obj in agent_files:\n                file_path = os.path.join(temp_dir, file_obj[\"filename\"])\n\n                try:\n                    if not os.path.exists(os.path.dirname(file_path)):\n                        os.makedirs(os.path.dirname(file_path))\n\n                    content = file_obj[\"content\"]\n\n                    if isinstance(content, dict) or isinstance(content, list):\n                        try:\n                            content = json.dumps(content)\n                        except Exception as e:\n                            print(f\"Error converting content to json: {e}\")\n                        content = str(content)\n\n                    if isinstance(content, str):\n                        content = content.encode(\"utf-8\")\n\n                    with open(file_path, \"wb\") as f:\n                        with io.BytesIO(content) as byte_stream:\n                            shutil.copyfileobj(byte_stream, f)\n                except Exception as e:\n                    print(f\"Error writing file {file_path}: {e}\")\n                    raise e\n\n        else:\n            # if agent files is a PosixPath, it is a path to the agent directory\n            # Copy all agent files including subfolders\n            shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n        return temp_dir\n\n    def set_agent_metadata(self, metadata) -&gt; None:\n        \"\"\"Set agent details from metadata.\"\"\"\n        try:\n            self.name = metadata[\"name\"]\n            self.version = metadata[\"version\"]\n        except KeyError as e:\n            raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n        details = metadata.get(\"details\", {})\n        agent = details.get(\"agent\", {})\n        welcome = agent.get(\"welcome\", {})\n\n        self.env_vars = details.get(\"env_vars\", {})\n        self.welcome_title = welcome.get(\"title\")\n        self.welcome_description = welcome.get(\"description\")\n\n        if agent_metadata := details.get(\"agent\", None):\n            if defaults := agent_metadata.get(\"defaults\", None):\n                self.model = defaults.get(\"model\", self.model)\n                self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n                self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n                self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n                self.max_iterations = defaults.get(\"max_iterations\", self.max_iterations)\n\n        if not self.version or not self.name:\n            raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n\n    def run(self, env: Any, task: Optional[str] = None) -&gt; None:  # noqa: D102\n        agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME)\n        if not os.path.exists(agent_filename):\n            raise ValueError(f\"Agent run error: {AGENT_FILENAME} does not exist\")\n\n        # combine agent.env_vars and env.env_vars\n        total_env_vars = {**self.env_vars, **env.env_vars}\n\n        # save os env vars\n        os.environ.update(total_env_vars)\n        # save env.env_vars\n        env.env_vars = total_env_vars\n\n        namespace = {\n            \"env\": env,\n            \"agent\": self,\n            \"task\": task,\n            \"__name__\": \"__main__\",\n            \"__file__\": agent_filename,\n        }\n\n        def run_agent_code(agent_filename, namespace):\n            # switch to user env.agent_runner_user\n            if env.agent_runner_user:\n                user_info = pwd.getpwnam(env.agent_runner_user)\n                os.setgid(user_info.pw_gid)\n                os.setuid(user_info.pw_uid)\n\n            # Run the code\n            # NOTE: runpy.run_path does not work in a multithreaded environment when running benchmark.\n            #       The performance of runpy.run_path may also change depending on a system, e.g. it may\n            #       work on Linux but not work on Mac.\n            #       `compile` and `exec` have been tested to work properly in a multithreaded environment.\n            with open(agent_filename, \"r\") as f:\n                code = compile(f.read(), agent_filename, \"exec\")\n                exec(code, namespace)\n\n        try:\n            if self.change_to_temp_dir:\n                os.chdir(self.temp_dir)\n            sys.path.insert(0, self.temp_dir)\n\n            if env.agent_runner_user:\n                process = multiprocessing.Process(target=run_agent_code, args=(agent_filename, namespace))\n                process.start()\n                process.join()\n            else:\n                run_agent_code(agent_filename, namespace)\n        finally:\n            sys.path.remove(self.temp_dir)\n            if self.change_to_temp_dir:\n                os.chdir(self.original_cwd)\n\n    @staticmethod\n    def load_agents(agents: str, config: ClientConfig, local: bool = False):\n        \"\"\"Loads agents from the registry.\"\"\"\n        return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n\n    @staticmethod\n    def load_agent(\n        name: str,\n        config: ClientConfig,\n        local: bool = False,\n    ):\n        \"\"\"Loads a single agent from the registry.\"\"\"\n        from nearai.registry import get_registry_folder, registry\n\n        identifier = None\n        if local:\n            agent_files_path = get_registry_folder() / name\n            if config.auth is None:\n                namespace = \"not-logged-in\"\n            else:\n                namespace = config.auth.account_id\n        else:\n            agent_files_path = registry.download(name)\n            identifier = name\n        assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n        metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n        if not os.path.exists(metadata_path):\n            raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        if not identifier:\n            identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n        return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.get_full_name","title":"get_full_name","text":"<pre><code>get_full_name()\n</code></pre> <p>Returns full agent name.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def get_full_name(self):\n    \"\"\"Returns full agent name.\"\"\"\n    return f\"{self.namespace}/{self.name}/{self.version}\"\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agent","title":"load_agent  <code>staticmethod</code>","text":"<pre><code>load_agent(name: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads a single agent from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agent(\n    name: str,\n    config: ClientConfig,\n    local: bool = False,\n):\n    \"\"\"Loads a single agent from the registry.\"\"\"\n    from nearai.registry import get_registry_folder, registry\n\n    identifier = None\n    if local:\n        agent_files_path = get_registry_folder() / name\n        if config.auth is None:\n            namespace = \"not-logged-in\"\n        else:\n            namespace = config.auth.account_id\n    else:\n        agent_files_path = registry.download(name)\n        identifier = name\n    assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n    metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n    if not os.path.exists(metadata_path):\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    if not identifier:\n        identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n    return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agents","title":"load_agents  <code>staticmethod</code>","text":"<pre><code>load_agents(agents: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads agents from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agents(agents: str, config: ClientConfig, local: bool = False):\n    \"\"\"Loads agents from the registry.\"\"\"\n    return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.set_agent_metadata","title":"set_agent_metadata","text":"<pre><code>set_agent_metadata(metadata) -&gt; None\n</code></pre> <p>Set agent details from metadata.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def set_agent_metadata(self, metadata) -&gt; None:\n    \"\"\"Set agent details from metadata.\"\"\"\n    try:\n        self.name = metadata[\"name\"]\n        self.version = metadata[\"version\"]\n    except KeyError as e:\n        raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n    details = metadata.get(\"details\", {})\n    agent = details.get(\"agent\", {})\n    welcome = agent.get(\"welcome\", {})\n\n    self.env_vars = details.get(\"env_vars\", {})\n    self.welcome_title = welcome.get(\"title\")\n    self.welcome_description = welcome.get(\"description\")\n\n    if agent_metadata := details.get(\"agent\", None):\n        if defaults := agent_metadata.get(\"defaults\", None):\n            self.model = defaults.get(\"model\", self.model)\n            self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n            self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n            self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n            self.max_iterations = defaults.get(\"max_iterations\", self.max_iterations)\n\n    if not self.version or not self.name:\n        raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.write_agent_files_to_temp","title":"write_agent_files_to_temp  <code>staticmethod</code>","text":"<pre><code>write_agent_files_to_temp(agent_files)\n</code></pre> <p>Write agent files to a temporary directory.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef write_agent_files_to_temp(agent_files):\n    \"\"\"Write agent files to a temporary directory.\"\"\"\n    unique_id = uuid.uuid4().hex\n    temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n    if isinstance(agent_files, List):\n        os.makedirs(temp_dir, exist_ok=True)\n\n        for file_obj in agent_files:\n            file_path = os.path.join(temp_dir, file_obj[\"filename\"])\n\n            try:\n                if not os.path.exists(os.path.dirname(file_path)):\n                    os.makedirs(os.path.dirname(file_path))\n\n                content = file_obj[\"content\"]\n\n                if isinstance(content, dict) or isinstance(content, list):\n                    try:\n                        content = json.dumps(content)\n                    except Exception as e:\n                        print(f\"Error converting content to json: {e}\")\n                    content = str(content)\n\n                if isinstance(content, str):\n                    content = content.encode(\"utf-8\")\n\n                with open(file_path, \"wb\") as f:\n                    with io.BytesIO(content) as byte_stream:\n                        shutil.copyfileobj(byte_stream, f)\n            except Exception as e:\n                print(f\"Error writing file {file_path}: {e}\")\n                raise e\n\n    else:\n        # if agent files is a PosixPath, it is a path to the agent directory\n        # Copy all agent files including subfolders\n        shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n    return temp_dir\n</code></pre>"},{"location":"api/#nearai.agents.environment","title":"environment","text":""},{"location":"api/#nearai.agents.environment.Environment","title":"Environment","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/environment.py</code> <pre><code>class Environment(object):\n    def __init__(  # noqa: D107\n        self,\n        path: str,\n        agents: List[Agent],\n        client: InferenceClient,\n        hub_client: OpenAI,\n        thread_id: str,\n        run_id: str,\n        create_files: bool = True,\n        env_vars: Optional[Dict[str, Any]] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        print_system_log: bool = False,\n        agent_runner_user: Optional[str] = None,\n        approvals: Optional[Dict[str, Any]] = default_approvals,\n    ) -&gt; None:\n        # Warning: never expose `client` or `_hub_client` to agent's environment\n\n        # Placeholder for solver\n        self.client: Optional[InferenceClient] = None\n\n        self._path = path\n        self._agents = agents\n        self._done = False\n        self._pending_ext_agent = False\n        self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n        self._last_used_model = \"\"\n        self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n        self.print_system_log = print_system_log\n        self.agent_runner_user = agent_runner_user\n        self._approvals = approvals\n        self._thread_id = thread_id\n        self._run_id = run_id\n        self._debug_mode = True if self.env_vars.get(\"DEBUG\") else False\n\n        self._tools = ToolRegistry()\n\n        if create_files:\n            os.makedirs(self._path, exist_ok=True)\n            open(os.path.join(self._path, CHAT_FILENAME), \"a\").close()\n        os.chdir(self._path)\n\n        def signer_account_id() -&gt; Optional[str]:\n            \"\"\"Expose the NEAR account_id of a user that signs this request to run an agent.\"\"\"\n            try:\n                return client._config.auth.account_id if client._config.auth else None\n            except (AttributeError, TypeError):\n                return None\n\n        self.signer_account_id = signer_account_id()\n\n        # Client methods\n        def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n            \"\"\"Queries a vector store.\n\n            vector_store_id: The id of the vector store to query.\n            query: The query to search for.\n            \"\"\"\n            return client.query_vector_store(vector_store_id, query, full_files)\n\n        self.query_vector_store = query_vector_store\n\n        def upload_file(\n            file_content: str,\n            purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n        ):\n            \"\"\"Uploads a file to the registry.\"\"\"\n            return client.upload_file(file_content, purpose)\n\n        self.upload_file = upload_file\n\n        def create_vector_store_from_source(\n            name: str,\n            source: Union[GitHubSource, GitLabSource],\n            source_auth: Optional[str] = None,\n            chunking_strategy: Optional[ChunkingStrategy] = None,\n            expires_after: Optional[ExpiresAfter] = None,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store from the given source.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                source: The source from which to create the vector store.\n                source_auth: The source authentication token.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store_from_source(\n                name=name,\n                source=source,\n                source_auth=source_auth,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store_from_source = create_vector_store_from_source\n\n        def add_file_to_vector_store(vector_store_id: str, file_id: str):\n            \"\"\"Adds a file to the vector store.\"\"\"\n            return client.add_file_to_vector_store(vector_store_id, file_id)\n\n        self.add_file_to_vector_store = add_file_to_vector_store\n\n        def create_vector_store(\n            name: str,\n            file_ids: list,\n            expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n            chunking_strategy: Union[\n                AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, NotGiven\n            ] = NOT_GIVEN,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                file_ids: List of file ids to create the vector store.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store(\n                name=name,\n                file_ids=file_ids,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store = create_vector_store\n\n        def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n            \"\"\"Gets a vector store by id.\"\"\"\n            return client.get_vector_store(vector_store_id)\n\n        self.get_vector_store = get_vector_store\n\n        # Save cache of requested models for inference to avoid extra server calls\n        self.cached_models_for_inference: Dict[str, str] = {}\n\n        def get_model_for_inference(model: str = \"\") -&gt; str:\n            \"\"\"Returns 'provider::model_full_path'.\"\"\"\n            if self.cached_models_for_inference.get(model, None) is None:\n                provider = self._agents[0].model_provider if self._agents else \"\"\n                if model == \"\":\n                    model = self._agents[0].model if self._agents else \"\"\n                if model == \"\":\n                    return DEFAULT_PROVIDER_MODEL\n\n                _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n                self.cached_models_for_inference[model] = model_for_inference\n\n            return self.cached_models_for_inference[model]\n\n        self.get_model_for_inference = get_model_for_inference\n\n        def _run_inference_completions(\n            messages: Union[Iterable[ChatCompletionMessageParam], str],\n            model: Union[Iterable[ChatCompletionMessageParam], str],\n            stream: bool,\n            **kwargs: Any,\n        ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n            \"\"\"Run inference completions for given parameters.\"\"\"\n            params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n            completions = client.completions(\n                params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n            )\n\n            return completions\n\n        self._run_inference_completions = _run_inference_completions\n\n        def get_agent_public_key():\n            \"\"\"Returns public key of the agent.\"\"\"\n            agent_name = self.get_primary_agent().get_full_name()\n\n            return client.get_agent_public_key(agent_name)\n\n        self.get_agent_public_key = get_agent_public_key\n\n        def run_agent(\n            owner: str,\n            agent_name: str,\n            version: str,\n            model: Optional[str] = None,\n            query: Optional[str] = None,\n            fork_thread: bool = True,\n        ):\n            \"\"\"Runs a child agent on the thread.\"\"\"\n            child_thread_id = self._thread_id\n            if fork_thread:\n                child_thread_id = client.threads_fork(self._thread_id).id\n                self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n\n            if query:\n                client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n            assistant_id = f\"{owner}/{agent_name}/{version}\"\n            model = model or DEFAULT_PROVIDER_MODEL\n            self.add_system_log(f\"Running agent {assistant_id}\", logging.INFO)\n            client.run_agent(\n                current_run_id=self._run_id,\n                child_thread_id=child_thread_id,\n                assistant_id=assistant_id,\n            )\n            self._pending_ext_agent = True\n\n            return child_thread_id\n\n        self.run_agent = run_agent\n\n        # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n        def add_user_memory(memory: str):\n            \"\"\"Add user memory.\"\"\"\n            return client.add_user_memory(memory)\n\n        self.add_user_memory = add_user_memory\n\n        def query_user_memory(query: str):\n            \"\"\"Query user memory.\"\"\"\n            return client.query_user_memory(query)\n\n        self.query_user_memory = query_user_memory\n\n        def generate_image(prompt: str):\n            \"\"\"Generate an image.\"\"\"\n            return client.generate_image(prompt)\n\n        self.generate_image = generate_image\n\n        def save_agent_data(key, data: Dict[str, Any]):\n            \"\"\"Save agent data.\"\"\"\n            return client.save_agent_data(key, data)\n\n        self.save_agent_data = save_agent_data\n\n        def get_agent_data():\n            \"\"\"Get agent data.\"\"\"\n            return client.get_agent_data()\n\n        self.get_agent_data = get_agent_data\n\n        def get_agent_data_by_key(key, default=None):\n            \"\"\"Get agent data by key.\"\"\"\n            namespace = self._agents[0].namespace\n            name = self._agents[0].name\n            result = client.get_agent_data_by_key(key)\n            return (\n                result\n                if result\n                else {\n                    \"value\": default,\n                    \"namespace\": namespace,\n                    \"key\": key,\n                    \"name\": name,\n                    \"updated_at\": \"\",\n                    \"created_at\": \"\",\n                }\n            )\n\n        self.get_agent_data_by_key = get_agent_data_by_key\n\n        # HubClient methods\n        def add_reply(\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            message_type: Optional[str] = None,\n        ):\n            \"\"\"Assistant adds a message to the environment.\"\"\"\n            # NOTE: message from `user` are not stored in the memory\n\n            return hub_client.beta.threads.messages.create(\n                thread_id=self._thread_id,\n                role=\"assistant\",\n                content=message,\n                extra_body={\n                    \"assistant_id\": self._agents[0].identifier,\n                    \"run_id\": self._run_id,\n                },\n                attachments=attachments,\n                metadata={\"message_type\": message_type} if message_type else None,\n            )\n\n        self.add_reply = add_reply\n\n        def _add_message(\n            role: str,\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            **kwargs: Any,\n        ):\n            return hub_client.beta.threads.messages.create(\n                thread_id=self._thread_id,\n                role=role,  # type: ignore\n                content=message,\n                extra_body={\n                    \"assistant_id\": self._agents[0].identifier,\n                    \"run_id\": self._run_id,\n                },\n                metadata=kwargs,\n                attachments=attachments,\n            )\n\n        self._add_message = _add_message\n\n        def _list_messages(\n            limit: Union[int, NotGiven] = NOT_GIVEN,\n            order: Literal[\"asc\", \"desc\"] = \"asc\",\n            thread_id: Optional[str] = None,\n        ) -&gt; List[Message]:\n            \"\"\"Returns messages from the environment.\"\"\"\n            messages = hub_client.beta.threads.messages.list(\n                thread_id=thread_id or self._thread_id, limit=limit, order=order\n            )\n            self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n            return messages.data\n\n        self._list_messages = _list_messages\n\n        def list_files_from_thread(\n            order: Literal[\"asc\", \"desc\"] = \"asc\", thread_id: Optional[str] = None\n        ) -&gt; List[FileObject]:\n            \"\"\"Lists files in the thread.\"\"\"\n            messages = self._list_messages(order=order)\n            # Extract attachments from messages\n            attachments = [a for m in messages if m.attachments for a in m.attachments]\n            # Extract files from attachments\n            file_ids = [a.file_id for a in attachments]\n            files = [hub_client.files.retrieve(f) for f in file_ids if f]\n            return files\n\n        self.list_files_from_thread = list_files_from_thread\n\n        def read_file_by_id(file_id: str):\n            \"\"\"Read a file from the thread.\"\"\"\n            content = hub_client.files.content(file_id).content.decode(\"utf-8\")\n            print(\"file content returned by api\", content)\n            return content\n\n        self.read_file_by_id = read_file_by_id\n\n        def write_file(\n            filename: str,\n            content: Union[str, bytes],\n            encoding: str = \"utf-8\",\n            filetype: str = \"text/plain\",\n            write_to_disk: bool = True,\n        ) -&gt; FileObject:\n            \"\"\"Writes a file to the environment.\n\n            filename: The name of the file to write to\n            content: The content to write to the file\n            encoding: The encoding to use when writing the file (default is utf-8)\n            filetype: The MIME type of the file (default is text/plain)\n            write_to_disk: If True, write locally to disk (default is True)\n            \"\"\"\n            if write_to_disk:\n                # Write locally\n                path = Path(self.get_primary_agent_temp_dir()) / filename\n                path.parent.mkdir(parents=True, exist_ok=True)\n                if isinstance(content, bytes):\n                    with open(path, \"wb\") as f:\n                        f.write(content)\n                else:\n                    with open(path, \"w\", encoding=encoding) as f:\n                        f.write(content)\n\n            if isinstance(content, bytes):\n                file_data = content\n            else:\n                file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n            # Upload to Hub\n            file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n            res = self.add_reply(\n                message=f\"Successfully wrote {len(content) if content else 0} characters to {filename}\",\n                attachments=[{\"file_id\": file.id, \"tools\": [{\"type\": \"file_search\"}]}],\n                message_type=\"system:file_write\",\n            )\n            self.add_system_log(\n                f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}. Added in thread as: {res.id}\"\n            )\n            return file\n\n        self.write_file = write_file\n\n        def mark_done() -&gt; Run:  # noqa: D102\n            self._done = True\n            res = hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\n                    \"status\": \"completed\",\n                    \"completed_at\": datetime.now().isoformat(),\n                },\n            )\n            return res\n\n        self.mark_done = mark_done\n\n        def mark_failed() -&gt; Run:\n            \"\"\"Marks the environment run as failed.\"\"\"\n            self._done = True\n            self.add_system_log(\"Environment run failed\", logging.ERROR)\n            res = hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"failed\", \"failed_at\": datetime.now().isoformat()},\n            )\n            return res\n\n        self.mark_failed = mark_failed\n\n        def request_user_input() -&gt; Run:\n            \"\"\"Must be called to request input from the user.\"\"\"\n            return hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"requires_action\"},\n            )\n\n        self.request_user_input = request_user_input\n\n        # Must be placed after method definitions\n        self.register_standard_tools()\n\n    def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n        \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n        if new:\n            self._tools = ToolRegistry()\n        return self._tools\n\n    def register_standard_tools(self) -&gt; None:  # noqa: D102\n        reg = self.get_tool_registry()\n        reg.register_tool(self.exec_command)\n        reg.register_tool(self.read_file)\n        reg.register_tool(self.write_file)\n        reg.register_tool(self.request_user_input)\n        reg.register_tool(self.list_files)\n        reg.register_tool(self.query_vector_store)\n\n    def get_last_message(self, role: str = \"user\"):\n        \"\"\"Reads last message from the given role and returns it.\"\"\"\n        for message in reversed(self.list_messages()):\n            if message.get(\"role\") == role:\n                return message\n\n        return None\n\n    def add_message(\n        self,\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n        # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n        role = \"assistant\"\n\n        return self._add_message(role, message, attachments, **kwargs)\n\n    def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add system log with timestamp and log level.\"\"\"\n        logger = logging.getLogger(\"system_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self._path, SYSTEM_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            if self.print_system_log:\n                console_handler = logging.StreamHandler()\n                console_handler.setFormatter(formatter)\n                logger.addHandler(console_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"system\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n    def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add agent log with timestamp and log level.\"\"\"\n        logger = logging.getLogger(\"agent_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self._path, AGENT_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n    def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n        \"\"\"Adds agent start system log.\"\"\"\n        agent = self._agents[agent_idx]\n        message = f\"Running agent {agent.name}\"\n        if agent.model != \"\":\n            model = self.get_model_for_inference(agent.model)\n            self._last_used_model = model\n            message += f\" that will connect to {model}\"\n            if agent.model_temperature:\n                message += f\", temperature={agent.model_temperature}\"\n            if agent.model_max_tokens:\n                message += f\", max_tokens={agent.model_max_tokens}\"\n        self.add_system_log(message)\n\n    def list_terminal_commands(self, filename: str = TERMINAL_FILENAME) -&gt; List[Any]:\n        \"\"\"Returns the terminal commands from the terminal file.\"\"\"\n        path = os.path.join(self._path, filename)\n\n        if not os.path.exists(path):\n            return []\n\n        with open(path, \"r\") as f:\n            return [json.loads(message) for message in f.read().split(DELIMITER) if message]\n\n    def list_messages(self, thread_id: Optional[str] = None):\n        \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n        messages = self._list_messages(thread_id=thread_id)\n\n        # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n        if self._debug_mode:\n            messages = [\n                m\n                for m in messages\n                if not (m.metadata and m.metadata[\"message_type\"] in [\"system:log\", \"agent:log\"])  # type: ignore\n            ]\n        legacy_messages = [\n            {\n                \"id\": m.id,\n                \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n                \"role\": m.role,\n            }\n            for m in messages\n        ]\n        return legacy_messages\n\n    def verify_message(\n        self,\n        account_id: str,\n        public_key: str,\n        signature: str,\n        message: str,\n        nonce: str,\n        callback_url: str,\n    ) -&gt; near.SignatureVerificationResult:\n        \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n        return near.verify_signed_message(\n            account_id,\n            public_key,\n            signature,\n            message,\n            nonce,\n            self._agents[0].name,\n            callback_url,\n        )\n\n    def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n        \"\"\"Lists files in the environment.\"\"\"\n        return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n\n    def get_system_path(self) -&gt; Path:\n        \"\"\"Returns the system path where chat.txt &amp; system_log are stored.\"\"\"\n        return Path(self._path)\n\n    def get_agent_temp_path(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n        return self.get_primary_agent_temp_dir()\n\n    def read_file(self, filename: str):\n        \"\"\"Reads a file from the environment or thread.\"\"\"\n        file_content = None\n        # First try to read from local filesystem\n        local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n        if os.path.exists(local_path):\n            with open(local_path, \"r\") as local_file:\n                file_content = local_file.read()\n\n        thread_files = self.list_files_from_thread(order=\"desc\")\n\n        # Then try to read from thread, starting from the most recent\n        for f in thread_files:\n            if f.filename == filename:\n                file_content = self.read_file_by_id(f.id)\n                break\n\n        # Write the file content to the local filesystem\n        if file_content:\n            with open(local_path, \"w\") as local_file:\n                local_file.write(file_content)\n        else:\n            self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n        return file_content\n\n    def exec_command(self, command: str) -&gt; Dict[str, Union[str, int]]:\n        \"\"\"Executes a command in the environment and logs the output.\n\n        The environment does not allow running interactive programs.\n        It will run a program for 1 second then will interrupt it if it is still running\n        or if it is waiting for user input.\n        command: The command to execute, like 'ls -l' or 'python3 tests.py'\n        \"\"\"\n        approval_function = self._approvals[\"confirm_execution\"] if self._approvals else None\n        if not approval_function:\n            return {\n                \"stderr\": \"Agent runner misconfiguration. No command execution approval function found.\",\n            }\n        if not approval_function(command):\n            return {\n                \"command\": command,\n                \"returncode\": 999,\n                \"stdout\": \"\",\n                \"stderr\": \"Command execution was not approved.\",\n            }\n\n        try:\n            process = subprocess.Popen(\n                shlex.split(command),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                bufsize=0,\n                universal_newlines=True,\n                cwd=self._path,\n            )\n        except Exception as e:\n            return {\n                \"command\": command,\n                \"returncode\": 999,\n                \"stdout\": \"\",\n                \"stderr\": \"Failed to execute: \" + str(e),\n            }\n\n        msg = \"\"\n\n        def kill_process_tree(p: Any) -&gt; None:\n            nonlocal msg\n            msg = \"Killing process due to timeout\"\n\n            process = psutil.Process(p.pid)\n            for proc in process.children(recursive=True):\n                proc.kill()\n            process.kill()\n\n        timer = threading.Timer(2, kill_process_tree, (process,))\n        timer.start()\n        process.wait()\n        timer.cancel()\n\n        result = {\n            \"command\": command,\n            \"stdout\": process.stdout.read() if process.stdout and hasattr(process.stdout, \"read\") else \"\",\n            \"stderr\": process.stderr.read() if process.stderr and hasattr(process.stderr, \"read\") else \"\",\n            \"returncode\": process.returncode,\n            \"msg\": msg,\n        }\n        with open(os.path.join(self._path, TERMINAL_FILENAME), \"a\") as f:\n            f.write(json.dumps(result) + DELIMITER)\n        return result\n\n    def get_inference_parameters(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Tuple[InferenceParameters, Any]:\n        \"\"\"Run inference parameters to run completions.\"\"\"\n        if isinstance(messages, str):\n            self.add_system_log(\n                \"Deprecated completions call. Pass `messages` as a first parameter.\",\n                logging.WARNING,\n            )\n            messages_or_model = messages\n            model_or_messages = model\n            model = cast(str, messages_or_model)\n            messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n        else:\n            model = cast(str, model)\n            messages = cast(Iterable[ChatCompletionMessageParam], messages)\n        model = self.get_model_for_inference(model)\n        if model != self._last_used_model:\n            self._last_used_model = model\n            self.add_system_log(f\"Connecting to {model}\")\n\n        temperature = kwargs.pop(\"temperature\", self._agents[0].model_temperature if self._agents else None)\n        max_tokens = kwargs.pop(\"max_tokens\", self._agents[0].model_max_tokens if self._agents else None)\n\n        params = InferenceParameters(\n            model=model,\n            messages=messages,\n            stream=stream,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n\n        return params, kwargs\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completions(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Returns all completions for given messages using the given model.\"\"\"\n        return self._run_inference_completions(messages, model, stream, **kwargs)\n\n    def verify_signed_message(\n        self,\n        completion: str,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        public_key: Union[str, None] = None,\n        signature: Union[str, None] = None,\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; bool:\n        \"\"\"Verifies a signed message.\"\"\"\n        if public_key is None or signature is None:\n            return False\n\n        params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n        messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n        ordered_messages_without_ids = [\n            {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n        ]\n\n        return validate_completion_signature(\n            public_key,\n            signature,\n            CompletionSignaturePayload(\n                agent_name=self.get_primary_agent().get_full_name(),\n                completion=completion,\n                model=params.model,\n                messages=ordered_messages_without_ids,\n                temperature=params.temperature,\n                max_tokens=params.max_tokens,\n            ),\n        )\n\n    def completions_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        add_responses_to_messages: bool = True,\n        agent_role_name=\"assistant\",\n        tool_role_name=\"tool\",\n        **kwargs: Any,\n    ) -&gt; ModelResponse:\n        \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n        if self._use_llama_tool_syntax(model, tools):\n            tool_prompt = self._llama_tool_prompt(tools)\n            messages.append({\"role\": \"system\", \"content\": tool_prompt})\n        raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message\n\n        self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n        return response\n\n    def _handle_tool_calls(\n        self,\n        response_message,\n        add_responses_to_messages,\n        agent_role_name,\n        tool_role_name,\n    ):\n        (message_without_tool_call, tool_calls) = self._parse_tool_call(response_message)\n        if add_responses_to_messages and response_message.content:\n            self.add_message(agent_role_name, message_without_tool_call)\n        if tool_calls:\n            for tool_call in tool_calls:\n                function_name = tool_call.function.name\n                try:\n                    assert function_name, \"Tool call must have a function name\"\n                    function_signature = self.get_tool_registry().get_tool_definition(function_name)\n                    assert function_signature, f\"Tool {function_name} not found\"\n                    args = tool_call.function.arguments\n                    function_args = tool_json_helper.parse_json_args(function_signature, args)\n                    self.add_system_log(f\"Calling tool {function_name} with args {function_args}\")\n                    function_response = self._tools.call_tool(function_name, **function_args if function_args else {})\n\n                    if function_response:\n                        function_response_json = json.dumps(function_response) if function_response else \"\"\n                        if add_responses_to_messages:\n                            self.add_message(\n                                tool_role_name,\n                                function_response_json,\n                                tool_call_id=tool_call.id,\n                                name=function_name,\n                            )\n                except Exception as e:\n                    error_message = f\"Error calling tool {function_name}: {e}\"\n                    self.add_system_log(error_message, level=logging.ERROR)\n                    if add_responses_to_messages:\n                        self.add_message(\n                            tool_role_name,\n                            error_message,\n                            tool_call_id=tool_call.id,\n                            name=function_name,\n                        )\n\n    @staticmethod\n    def _parse_tool_call(\n        response_message,\n    ) -&gt; Tuple[Optional[str], Optional[List[ChatCompletionMessageToolCall]]]:\n        if hasattr(response_message, \"tool_calls\") and response_message.tool_calls:\n            return response_message.content, response_message.tool_calls\n        content = response_message.content\n        if content is None:\n            return None, None\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name, args, end_tag, after_call_text = llama_match\n                function = Function(name=function_name, arguments=args)\n                tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                text += before_call_text + after_call_text\n                tool_calls.append(tool_call)\n            return text, tool_calls\n\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN2.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name_and_args, after_call_text = llama_match\n                try:\n                    parsed_function_name_and_args = json.loads(function_name_and_args)\n                    function_name = parsed_function_name_and_args.get(\"name\")\n                    args = parsed_function_name_and_args.get(\"arguments\")\n                    function = Function(name=function_name, arguments=args)\n                    tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                    text += before_call_text + after_call_text\n                    tool_calls.append(tool_call)\n                except json.JSONDecodeError:\n                    print(f\"Error parsing tool_call function name and args: {function_name_and_args}\")\n                    continue\n            return text, tool_calls\n\n        return content, None\n\n    @staticmethod\n    def _use_llama_tool_syntax(model: str, tools: Optional[List]) -&gt; bool:\n        return tools is not None and \"llama\" in model\n\n    @staticmethod\n    def _llama_tool_prompt(tools: Optional[List]) -&gt; str:\n        return (\n            \"\"\"Answer the user's question by making use of the following functions if needed.\n            If none of the function can be used, please say so.\n            Here is a list of functions in JSON format:\"\"\"\n            + json.dumps(tools)\n            + \"\"\"Think very carefully before calling functions.\n            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n\n            &lt;function=example_function_name&gt;{\"example_name\": \"example_value\"}&lt;/function&gt;\n\n            Reminder:\n            - Function calls MUST follow the specified format, start with &lt;function= and end with &lt;/function&gt;\n            - Function arguments MUST be in JSON format using double quotes\n            - Required parameters MUST be specified\n            - Multiple functions can be called in one message as long as they are on separate lines.\n            - Put the entire function call reply on one line\n        \"\"\"\n        )\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n        raw_response = self.completions(messages, model, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n        return response_message\n\n    def signed_completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; Dict[str, str]:\n        \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n        # TODO Return signed completions for non-latest versions only?\n        agent_name = self.get_primary_agent().get_full_name()\n        raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n\n        signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n\n        return {\n            \"response\": response_message,\n            \"signature\": signature_data.get(\"signature\", None),\n            \"public_key\": signature_data.get(\"public_key\", None),\n        }\n\n    def completion_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[str]:\n        \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n        completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n        assert all(\n            map(\n                lambda choice: isinstance(choice, Choices),\n                completion_tools_response.choices,\n            )\n        ), \"Expected Choices\"\n        choices: List[Choices] = completion_tools_response.choices  # type: ignore\n        response_content = choices[0].message.content\n        return response_content\n\n    def call_agent(self, agent_index: int, task: str) -&gt; None:\n        \"\"\"Calls agent with given task.\"\"\"\n        self._agents[agent_index].run(self, task=task)\n\n    def get_agents(self) -&gt; List[Agent]:\n        \"\"\"Returns list of agents available in environment.\"\"\"\n        return self._agents\n\n    def get_primary_agent(self) -&gt; Agent:\n        \"\"\"Returns the agent that is invoked first.\"\"\"\n        return self._agents[0]\n\n    def get_primary_agent_temp_dir(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent.\"\"\"\n        return self._agents[0].temp_dir\n\n    def is_done(self) -&gt; bool:  # noqa: D102\n        return self._done\n\n    def create_snapshot(self) -&gt; bytes:\n        \"\"\"Create an in memory snapshot.\"\"\"\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n            with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n                tar.add(self._path, arcname=\".\")\n            f.flush()\n            f.seek(0)\n            snapshot = f.read()\n        return snapshot\n\n    def environment_run_info(self, base_id, run_type) -&gt; dict:\n        \"\"\"Returns the environment run information.\"\"\"\n        if not self._agents or not self._agents[0]:\n            raise ValueError(\"Agent not found\")\n        primary_agent = self._agents[0]\n\n        full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n        safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n        uid = uuid.uuid4().hex\n        generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n        name = generated_name\n\n        timestamp = datetime.now(timezone.utc).isoformat()\n        return {\n            \"name\": name,\n            \"version\": \"0\",\n            \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n            \"category\": \"environment\",\n            \"tags\": [\"environment\"],\n            \"details\": {\n                \"base_id\": base_id,\n                \"timestamp\": timestamp,\n                \"agents\": [agent.name for agent in self._agents],\n                \"primary_agent_namespace\": primary_agent.namespace,\n                \"primary_agent_name\": primary_agent.name,\n                \"primary_agent_version\": primary_agent.version,\n                \"run_id\": self._run_id,\n                \"run_type\": run_type,\n            },\n            \"show_entry\": True,\n        }\n\n    def load_snapshot(self, snapshot: bytes) -&gt; None:\n        \"\"\"Load Environment from Snapshot.\"\"\"\n        shutil.rmtree(self._path, ignore_errors=True)\n\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n            f.write(snapshot)\n            f.flush()\n            f.seek(0)\n\n            with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n                tar.extractall(self._path)\n\n    def __str__(self) -&gt; str:  # noqa: D105\n        return f\"Environment({self._path})\"\n\n    def clear_temp_agent_files(self, verbose=True) -&gt; None:\n        \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n        for agent in self._agents:\n            if os.path.exists(agent.temp_dir):\n                if verbose:\n                    print(\"removed agent.temp_files\", agent.temp_dir)\n                shutil.rmtree(agent.temp_dir)\n\n    def set_next_actor(self, who: str) -&gt; None:\n        \"\"\"Set the next actor / action in the dialogue.\"\"\"\n        next_action_fn = os.path.join(self._path, \".next_action\")\n        if who == \"agent\":\n            self._done = False\n\n        with open(next_action_fn, \"w\") as f:\n            f.write(who)\n\n    def get_next_actor(self) -&gt; str:  # noqa: D102\n        next_action_fn = os.path.join(self._path, \".next_action\")\n\n        if os.path.exists(next_action_fn):\n            with open(next_action_fn) as f:\n                return f.read().strip(\" \\n\")\n        else:\n            # By default the user starts the conversation.\n            return \"user\"\n\n    def run(\n        self,\n        new_message: Optional[str] = None,\n        max_iterations: int = 10,\n    ) -&gt; None:\n        \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n        if new_message:\n            self._add_message(\"user\", new_message)\n\n        iteration = 0\n        self.set_next_actor(\"agent\")\n\n        while iteration &lt; max_iterations and not self.is_done() and self.get_next_actor() != \"user\":\n            iteration += 1\n            if max_iterations &gt; 1:\n                self.add_system_log(\n                    f\"Running agent, iteration {iteration}/{max_iterations}\",\n                    logging.INFO,\n                )\n            try:\n                self._agents[0].run(self, task=new_message)\n            except Exception as e:\n                self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n                self.mark_failed()\n                raise e\n\n        if not self._pending_ext_agent:\n            # If no external agent was called, mark the whole run as done.\n            # Else this environment will stop for now but this run will be continued later.\n            self.mark_done()\n\n    def generate_folder_hash_id(self, path: str) -&gt; str:\n        \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n        hash_obj = hashlib.md5()\n\n        for root, _dirs, files in os.walk(path):\n            for file in sorted(files):\n                file_path = os.path.join(root, file)\n                with open(file_path, \"rb\") as f:\n                    while chunk := f.read(8192):\n                        hash_obj.update(chunk)\n\n        return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.__init__","title":"__init__","text":"<pre><code>__init__(path: str, agents: List[Agent], client: InferenceClient, hub_client: OpenAI, thread_id: str, run_id: str, create_files: bool = True, env_vars: Optional[Dict[str, Any]] = None, tool_resources: Optional[Dict[str, Any]] = None, print_system_log: bool = False, agent_runner_user: Optional[str] = None, approvals: Optional[Dict[str, Any]] = default_approvals) -&gt; None\n</code></pre> Source code in <code>nearai/agents/environment.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    path: str,\n    agents: List[Agent],\n    client: InferenceClient,\n    hub_client: OpenAI,\n    thread_id: str,\n    run_id: str,\n    create_files: bool = True,\n    env_vars: Optional[Dict[str, Any]] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    print_system_log: bool = False,\n    agent_runner_user: Optional[str] = None,\n    approvals: Optional[Dict[str, Any]] = default_approvals,\n) -&gt; None:\n    # Warning: never expose `client` or `_hub_client` to agent's environment\n\n    # Placeholder for solver\n    self.client: Optional[InferenceClient] = None\n\n    self._path = path\n    self._agents = agents\n    self._done = False\n    self._pending_ext_agent = False\n    self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n    self._last_used_model = \"\"\n    self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n    self.print_system_log = print_system_log\n    self.agent_runner_user = agent_runner_user\n    self._approvals = approvals\n    self._thread_id = thread_id\n    self._run_id = run_id\n    self._debug_mode = True if self.env_vars.get(\"DEBUG\") else False\n\n    self._tools = ToolRegistry()\n\n    if create_files:\n        os.makedirs(self._path, exist_ok=True)\n        open(os.path.join(self._path, CHAT_FILENAME), \"a\").close()\n    os.chdir(self._path)\n\n    def signer_account_id() -&gt; Optional[str]:\n        \"\"\"Expose the NEAR account_id of a user that signs this request to run an agent.\"\"\"\n        try:\n            return client._config.auth.account_id if client._config.auth else None\n        except (AttributeError, TypeError):\n            return None\n\n    self.signer_account_id = signer_account_id()\n\n    # Client methods\n    def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n        \"\"\"Queries a vector store.\n\n        vector_store_id: The id of the vector store to query.\n        query: The query to search for.\n        \"\"\"\n        return client.query_vector_store(vector_store_id, query, full_files)\n\n    self.query_vector_store = query_vector_store\n\n    def upload_file(\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n    ):\n        \"\"\"Uploads a file to the registry.\"\"\"\n        return client.upload_file(file_content, purpose)\n\n    self.upload_file = upload_file\n\n    def create_vector_store_from_source(\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            source: The source from which to create the vector store.\n            source_auth: The source authentication token.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store_from_source(\n            name=name,\n            source=source,\n            source_auth=source_auth,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store_from_source = create_vector_store_from_source\n\n    def add_file_to_vector_store(vector_store_id: str, file_id: str):\n        \"\"\"Adds a file to the vector store.\"\"\"\n        return client.add_file_to_vector_store(vector_store_id, file_id)\n\n    self.add_file_to_vector_store = add_file_to_vector_store\n\n    def create_vector_store(\n        name: str,\n        file_ids: list,\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[\n            AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, NotGiven\n        ] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            file_ids: List of file ids to create the vector store.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store(\n            name=name,\n            file_ids=file_ids,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store = create_vector_store\n\n    def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        return client.get_vector_store(vector_store_id)\n\n    self.get_vector_store = get_vector_store\n\n    # Save cache of requested models for inference to avoid extra server calls\n    self.cached_models_for_inference: Dict[str, str] = {}\n\n    def get_model_for_inference(model: str = \"\") -&gt; str:\n        \"\"\"Returns 'provider::model_full_path'.\"\"\"\n        if self.cached_models_for_inference.get(model, None) is None:\n            provider = self._agents[0].model_provider if self._agents else \"\"\n            if model == \"\":\n                model = self._agents[0].model if self._agents else \"\"\n            if model == \"\":\n                return DEFAULT_PROVIDER_MODEL\n\n            _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n            self.cached_models_for_inference[model] = model_for_inference\n\n        return self.cached_models_for_inference[model]\n\n    self.get_model_for_inference = get_model_for_inference\n\n    def _run_inference_completions(\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Run inference completions for given parameters.\"\"\"\n        params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n        completions = client.completions(\n            params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n        )\n\n        return completions\n\n    self._run_inference_completions = _run_inference_completions\n\n    def get_agent_public_key():\n        \"\"\"Returns public key of the agent.\"\"\"\n        agent_name = self.get_primary_agent().get_full_name()\n\n        return client.get_agent_public_key(agent_name)\n\n    self.get_agent_public_key = get_agent_public_key\n\n    def run_agent(\n        owner: str,\n        agent_name: str,\n        version: str,\n        model: Optional[str] = None,\n        query: Optional[str] = None,\n        fork_thread: bool = True,\n    ):\n        \"\"\"Runs a child agent on the thread.\"\"\"\n        child_thread_id = self._thread_id\n        if fork_thread:\n            child_thread_id = client.threads_fork(self._thread_id).id\n            self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n\n        if query:\n            client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n        assistant_id = f\"{owner}/{agent_name}/{version}\"\n        model = model or DEFAULT_PROVIDER_MODEL\n        self.add_system_log(f\"Running agent {assistant_id}\", logging.INFO)\n        client.run_agent(\n            current_run_id=self._run_id,\n            child_thread_id=child_thread_id,\n            assistant_id=assistant_id,\n        )\n        self._pending_ext_agent = True\n\n        return child_thread_id\n\n    self.run_agent = run_agent\n\n    # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n    def add_user_memory(memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return client.add_user_memory(memory)\n\n    self.add_user_memory = add_user_memory\n\n    def query_user_memory(query: str):\n        \"\"\"Query user memory.\"\"\"\n        return client.query_user_memory(query)\n\n    self.query_user_memory = query_user_memory\n\n    def generate_image(prompt: str):\n        \"\"\"Generate an image.\"\"\"\n        return client.generate_image(prompt)\n\n    self.generate_image = generate_image\n\n    def save_agent_data(key, data: Dict[str, Any]):\n        \"\"\"Save agent data.\"\"\"\n        return client.save_agent_data(key, data)\n\n    self.save_agent_data = save_agent_data\n\n    def get_agent_data():\n        \"\"\"Get agent data.\"\"\"\n        return client.get_agent_data()\n\n    self.get_agent_data = get_agent_data\n\n    def get_agent_data_by_key(key, default=None):\n        \"\"\"Get agent data by key.\"\"\"\n        namespace = self._agents[0].namespace\n        name = self._agents[0].name\n        result = client.get_agent_data_by_key(key)\n        return (\n            result\n            if result\n            else {\n                \"value\": default,\n                \"namespace\": namespace,\n                \"key\": key,\n                \"name\": name,\n                \"updated_at\": \"\",\n                \"created_at\": \"\",\n            }\n        )\n\n    self.get_agent_data_by_key = get_agent_data_by_key\n\n    # HubClient methods\n    def add_reply(\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        message_type: Optional[str] = None,\n    ):\n        \"\"\"Assistant adds a message to the environment.\"\"\"\n        # NOTE: message from `user` are not stored in the memory\n\n        return hub_client.beta.threads.messages.create(\n            thread_id=self._thread_id,\n            role=\"assistant\",\n            content=message,\n            extra_body={\n                \"assistant_id\": self._agents[0].identifier,\n                \"run_id\": self._run_id,\n            },\n            attachments=attachments,\n            metadata={\"message_type\": message_type} if message_type else None,\n        )\n\n    self.add_reply = add_reply\n\n    def _add_message(\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        return hub_client.beta.threads.messages.create(\n            thread_id=self._thread_id,\n            role=role,  # type: ignore\n            content=message,\n            extra_body={\n                \"assistant_id\": self._agents[0].identifier,\n                \"run_id\": self._run_id,\n            },\n            metadata=kwargs,\n            attachments=attachments,\n        )\n\n    self._add_message = _add_message\n\n    def _list_messages(\n        limit: Union[int, NotGiven] = NOT_GIVEN,\n        order: Literal[\"asc\", \"desc\"] = \"asc\",\n        thread_id: Optional[str] = None,\n    ) -&gt; List[Message]:\n        \"\"\"Returns messages from the environment.\"\"\"\n        messages = hub_client.beta.threads.messages.list(\n            thread_id=thread_id or self._thread_id, limit=limit, order=order\n        )\n        self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n        return messages.data\n\n    self._list_messages = _list_messages\n\n    def list_files_from_thread(\n        order: Literal[\"asc\", \"desc\"] = \"asc\", thread_id: Optional[str] = None\n    ) -&gt; List[FileObject]:\n        \"\"\"Lists files in the thread.\"\"\"\n        messages = self._list_messages(order=order)\n        # Extract attachments from messages\n        attachments = [a for m in messages if m.attachments for a in m.attachments]\n        # Extract files from attachments\n        file_ids = [a.file_id for a in attachments]\n        files = [hub_client.files.retrieve(f) for f in file_ids if f]\n        return files\n\n    self.list_files_from_thread = list_files_from_thread\n\n    def read_file_by_id(file_id: str):\n        \"\"\"Read a file from the thread.\"\"\"\n        content = hub_client.files.content(file_id).content.decode(\"utf-8\")\n        print(\"file content returned by api\", content)\n        return content\n\n    self.read_file_by_id = read_file_by_id\n\n    def write_file(\n        filename: str,\n        content: Union[str, bytes],\n        encoding: str = \"utf-8\",\n        filetype: str = \"text/plain\",\n        write_to_disk: bool = True,\n    ) -&gt; FileObject:\n        \"\"\"Writes a file to the environment.\n\n        filename: The name of the file to write to\n        content: The content to write to the file\n        encoding: The encoding to use when writing the file (default is utf-8)\n        filetype: The MIME type of the file (default is text/plain)\n        write_to_disk: If True, write locally to disk (default is True)\n        \"\"\"\n        if write_to_disk:\n            # Write locally\n            path = Path(self.get_primary_agent_temp_dir()) / filename\n            path.parent.mkdir(parents=True, exist_ok=True)\n            if isinstance(content, bytes):\n                with open(path, \"wb\") as f:\n                    f.write(content)\n            else:\n                with open(path, \"w\", encoding=encoding) as f:\n                    f.write(content)\n\n        if isinstance(content, bytes):\n            file_data = content\n        else:\n            file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n        # Upload to Hub\n        file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n        res = self.add_reply(\n            message=f\"Successfully wrote {len(content) if content else 0} characters to {filename}\",\n            attachments=[{\"file_id\": file.id, \"tools\": [{\"type\": \"file_search\"}]}],\n            message_type=\"system:file_write\",\n        )\n        self.add_system_log(\n            f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}. Added in thread as: {res.id}\"\n        )\n        return file\n\n    self.write_file = write_file\n\n    def mark_done() -&gt; Run:  # noqa: D102\n        self._done = True\n        res = hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\n                \"status\": \"completed\",\n                \"completed_at\": datetime.now().isoformat(),\n            },\n        )\n        return res\n\n    self.mark_done = mark_done\n\n    def mark_failed() -&gt; Run:\n        \"\"\"Marks the environment run as failed.\"\"\"\n        self._done = True\n        self.add_system_log(\"Environment run failed\", logging.ERROR)\n        res = hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"failed\", \"failed_at\": datetime.now().isoformat()},\n        )\n        return res\n\n    self.mark_failed = mark_failed\n\n    def request_user_input() -&gt; Run:\n        \"\"\"Must be called to request input from the user.\"\"\"\n        return hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"requires_action\"},\n        )\n\n    self.request_user_input = request_user_input\n\n    # Must be placed after method definitions\n    self.register_standard_tools()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_log","title":"add_agent_log","text":"<pre><code>add_agent_log(log: str, level: int = logging.INFO) -&gt; None\n</code></pre> <p>Add agent log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add agent log with timestamp and log level.\"\"\"\n    logger = logging.getLogger(\"agent_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self._path, AGENT_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_start_system_log","title":"add_agent_start_system_log","text":"<pre><code>add_agent_start_system_log(agent_idx: int) -&gt; None\n</code></pre> <p>Adds agent start system log.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n    \"\"\"Adds agent start system log.\"\"\"\n    agent = self._agents[agent_idx]\n    message = f\"Running agent {agent.name}\"\n    if agent.model != \"\":\n        model = self.get_model_for_inference(agent.model)\n        self._last_used_model = model\n        message += f\" that will connect to {model}\"\n        if agent.model_temperature:\n            message += f\", temperature={agent.model_temperature}\"\n        if agent.model_max_tokens:\n            message += f\", max_tokens={agent.model_max_tokens}\"\n    self.add_system_log(message)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_message","title":"add_message","text":"<pre><code>add_message(role: str, message: str, attachments: Optional[Iterable[Attachment]] = None, **kwargs: Any)\n</code></pre> <p>Deprecated. Please use <code>add_reply</code> instead. Assistant adds a message to the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_message(\n    self,\n    role: str,\n    message: str,\n    attachments: Optional[Iterable[Attachment]] = None,\n    **kwargs: Any,\n):\n    \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n    # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n    role = \"assistant\"\n\n    return self._add_message(role, message, attachments, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_system_log","title":"add_system_log","text":"<pre><code>add_system_log(log: str, level: int = logging.INFO) -&gt; None\n</code></pre> <p>Add system log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add system log with timestamp and log level.\"\"\"\n    logger = logging.getLogger(\"system_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self._path, SYSTEM_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        if self.print_system_log:\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(formatter)\n            logger.addHandler(console_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"system\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.call_agent","title":"call_agent","text":"<pre><code>call_agent(agent_index: int, task: str) -&gt; None\n</code></pre> <p>Calls agent with given task.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def call_agent(self, agent_index: int, task: str) -&gt; None:\n    \"\"\"Calls agent with given task.\"\"\"\n    self._agents[agent_index].run(self, task=task)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.clear_temp_agent_files","title":"clear_temp_agent_files","text":"<pre><code>clear_temp_agent_files(verbose=True) -&gt; None\n</code></pre> <p>Remove temp agent files created to be used in <code>runpy</code>.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def clear_temp_agent_files(self, verbose=True) -&gt; None:\n    \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n    for agent in self._agents:\n        if os.path.exists(agent.temp_dir):\n            if verbose:\n                print(\"removed agent.temp_files\", agent.temp_dir)\n            shutil.rmtree(agent.temp_dir)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion","title":"completion","text":"<pre><code>completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; str\n</code></pre> <p>Returns a completion for the given messages using the given model.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n    raw_response = self.completions(messages, model, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n    return response_message\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion_and_run_tools","title":"completion_and_run_tools","text":"<pre><code>completion_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, **kwargs: Any) -&gt; Optional[str]\n</code></pre> <p>Returns a completion for the given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    **kwargs: Any,\n) -&gt; Optional[str]:\n    \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n    completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n    assert all(\n        map(\n            lambda choice: isinstance(choice, Choices),\n            completion_tools_response.choices,\n        )\n    ), \"Expected Choices\"\n    choices: List[Choices] = completion_tools_response.choices  # type: ignore\n    response_content = choices[0].message.content\n    return response_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions","title":"completions","text":"<pre><code>completions(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', stream: bool = False, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Returns all completions for given messages using the given model.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Returns all completions for given messages using the given model.\"\"\"\n    return self._run_inference_completions(messages, model, stream, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions_and_run_tools","title":"completions_and_run_tools","text":"<pre><code>completions_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, add_responses_to_messages: bool = True, agent_role_name='assistant', tool_role_name='tool', **kwargs: Any) -&gt; ModelResponse\n</code></pre> <p>Returns all completions for given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    add_responses_to_messages: bool = True,\n    agent_role_name=\"assistant\",\n    tool_role_name=\"tool\",\n    **kwargs: Any,\n) -&gt; ModelResponse:\n    \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n    if self._use_llama_tool_syntax(model, tools):\n        tool_prompt = self._llama_tool_prompt(tools)\n        messages.append({\"role\": \"system\", \"content\": tool_prompt})\n    raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message\n\n    self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n    return response\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.create_snapshot","title":"create_snapshot","text":"<pre><code>create_snapshot() -&gt; bytes\n</code></pre> <p>Create an in memory snapshot.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def create_snapshot(self) -&gt; bytes:\n    \"\"\"Create an in memory snapshot.\"\"\"\n    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n        with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n            tar.add(self._path, arcname=\".\")\n        f.flush()\n        f.seek(0)\n        snapshot = f.read()\n    return snapshot\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.environment_run_info","title":"environment_run_info","text":"<pre><code>environment_run_info(base_id, run_type) -&gt; dict\n</code></pre> <p>Returns the environment run information.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def environment_run_info(self, base_id, run_type) -&gt; dict:\n    \"\"\"Returns the environment run information.\"\"\"\n    if not self._agents or not self._agents[0]:\n        raise ValueError(\"Agent not found\")\n    primary_agent = self._agents[0]\n\n    full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n    safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n    uid = uuid.uuid4().hex\n    generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n    name = generated_name\n\n    timestamp = datetime.now(timezone.utc).isoformat()\n    return {\n        \"name\": name,\n        \"version\": \"0\",\n        \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n        \"category\": \"environment\",\n        \"tags\": [\"environment\"],\n        \"details\": {\n            \"base_id\": base_id,\n            \"timestamp\": timestamp,\n            \"agents\": [agent.name for agent in self._agents],\n            \"primary_agent_namespace\": primary_agent.namespace,\n            \"primary_agent_name\": primary_agent.name,\n            \"primary_agent_version\": primary_agent.version,\n            \"run_id\": self._run_id,\n            \"run_type\": run_type,\n        },\n        \"show_entry\": True,\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.exec_command","title":"exec_command","text":"<pre><code>exec_command(command: str) -&gt; Dict[str, Union[str, int]]\n</code></pre> <p>Executes a command in the environment and logs the output.</p> <p>The environment does not allow running interactive programs. It will run a program for 1 second then will interrupt it if it is still running or if it is waiting for user input. command: The command to execute, like 'ls -l' or 'python3 tests.py'</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def exec_command(self, command: str) -&gt; Dict[str, Union[str, int]]:\n    \"\"\"Executes a command in the environment and logs the output.\n\n    The environment does not allow running interactive programs.\n    It will run a program for 1 second then will interrupt it if it is still running\n    or if it is waiting for user input.\n    command: The command to execute, like 'ls -l' or 'python3 tests.py'\n    \"\"\"\n    approval_function = self._approvals[\"confirm_execution\"] if self._approvals else None\n    if not approval_function:\n        return {\n            \"stderr\": \"Agent runner misconfiguration. No command execution approval function found.\",\n        }\n    if not approval_function(command):\n        return {\n            \"command\": command,\n            \"returncode\": 999,\n            \"stdout\": \"\",\n            \"stderr\": \"Command execution was not approved.\",\n        }\n\n    try:\n        process = subprocess.Popen(\n            shlex.split(command),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=0,\n            universal_newlines=True,\n            cwd=self._path,\n        )\n    except Exception as e:\n        return {\n            \"command\": command,\n            \"returncode\": 999,\n            \"stdout\": \"\",\n            \"stderr\": \"Failed to execute: \" + str(e),\n        }\n\n    msg = \"\"\n\n    def kill_process_tree(p: Any) -&gt; None:\n        nonlocal msg\n        msg = \"Killing process due to timeout\"\n\n        process = psutil.Process(p.pid)\n        for proc in process.children(recursive=True):\n            proc.kill()\n        process.kill()\n\n    timer = threading.Timer(2, kill_process_tree, (process,))\n    timer.start()\n    process.wait()\n    timer.cancel()\n\n    result = {\n        \"command\": command,\n        \"stdout\": process.stdout.read() if process.stdout and hasattr(process.stdout, \"read\") else \"\",\n        \"stderr\": process.stderr.read() if process.stderr and hasattr(process.stderr, \"read\") else \"\",\n        \"returncode\": process.returncode,\n        \"msg\": msg,\n    }\n    with open(os.path.join(self._path, TERMINAL_FILENAME), \"a\") as f:\n        f.write(json.dumps(result) + DELIMITER)\n    return result\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.generate_folder_hash_id","title":"generate_folder_hash_id","text":"<pre><code>generate_folder_hash_id(path: str) -&gt; str\n</code></pre> <p>Returns hash based on files and their contents in path, including subfolders.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def generate_folder_hash_id(self, path: str) -&gt; str:\n    \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n    hash_obj = hashlib.md5()\n\n    for root, _dirs, files in os.walk(path):\n        for file in sorted(files):\n            file_path = os.path.join(root, file)\n            with open(file_path, \"rb\") as f:\n                while chunk := f.read(8192):\n                    hash_obj.update(chunk)\n\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agent_temp_path","title":"get_agent_temp_path","text":"<pre><code>get_agent_temp_path() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent where execution happens.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agent_temp_path(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n    return self.get_primary_agent_temp_dir()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agents","title":"get_agents","text":"<pre><code>get_agents() -&gt; List[Agent]\n</code></pre> <p>Returns list of agents available in environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agents(self) -&gt; List[Agent]:\n    \"\"\"Returns list of agents available in environment.\"\"\"\n    return self._agents\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_inference_parameters","title":"get_inference_parameters","text":"<pre><code>get_inference_parameters(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str], stream: bool, **kwargs: Any) -&gt; Tuple[InferenceParameters, Any]\n</code></pre> <p>Run inference parameters to run completions.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_inference_parameters(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str],\n    stream: bool,\n    **kwargs: Any,\n) -&gt; Tuple[InferenceParameters, Any]:\n    \"\"\"Run inference parameters to run completions.\"\"\"\n    if isinstance(messages, str):\n        self.add_system_log(\n            \"Deprecated completions call. Pass `messages` as a first parameter.\",\n            logging.WARNING,\n        )\n        messages_or_model = messages\n        model_or_messages = model\n        model = cast(str, messages_or_model)\n        messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n    else:\n        model = cast(str, model)\n        messages = cast(Iterable[ChatCompletionMessageParam], messages)\n    model = self.get_model_for_inference(model)\n    if model != self._last_used_model:\n        self._last_used_model = model\n        self.add_system_log(f\"Connecting to {model}\")\n\n    temperature = kwargs.pop(\"temperature\", self._agents[0].model_temperature if self._agents else None)\n    max_tokens = kwargs.pop(\"max_tokens\", self._agents[0].model_max_tokens if self._agents else None)\n\n    params = InferenceParameters(\n        model=model,\n        messages=messages,\n        stream=stream,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    return params, kwargs\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_last_message","title":"get_last_message","text":"<pre><code>get_last_message(role: str = 'user')\n</code></pre> <p>Reads last message from the given role and returns it.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_last_message(self, role: str = \"user\"):\n    \"\"\"Reads last message from the given role and returns it.\"\"\"\n    for message in reversed(self.list_messages()):\n        if message.get(\"role\") == role:\n            return message\n\n    return None\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent","title":"get_primary_agent","text":"<pre><code>get_primary_agent() -&gt; Agent\n</code></pre> <p>Returns the agent that is invoked first.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent(self) -&gt; Agent:\n    \"\"\"Returns the agent that is invoked first.\"\"\"\n    return self._agents[0]\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent_temp_dir","title":"get_primary_agent_temp_dir","text":"<pre><code>get_primary_agent_temp_dir() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent_temp_dir(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent.\"\"\"\n    return self._agents[0].temp_dir\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_system_path","title":"get_system_path","text":"<pre><code>get_system_path() -&gt; Path\n</code></pre> <p>Returns the system path where chat.txt &amp; system_log are stored.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_system_path(self) -&gt; Path:\n    \"\"\"Returns the system path where chat.txt &amp; system_log are stored.\"\"\"\n    return Path(self._path)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_tool_registry","title":"get_tool_registry","text":"<pre><code>get_tool_registry(new: bool = False) -&gt; ToolRegistry\n</code></pre> <p>Returns the tool registry, a dictionary of tools that can be called by the agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n    \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n    if new:\n        self._tools = ToolRegistry()\n    return self._tools\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_files","title":"list_files","text":"<pre><code>list_files(path: str, order: Literal['asc', 'desc'] = 'asc') -&gt; List[str]\n</code></pre> <p>Lists files in the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n    \"\"\"Lists files in the environment.\"\"\"\n    return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_messages","title":"list_messages","text":"<pre><code>list_messages(thread_id: Optional[str] = None)\n</code></pre> <p>Backwards compatibility for chat_completions messages.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_messages(self, thread_id: Optional[str] = None):\n    \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n    messages = self._list_messages(thread_id=thread_id)\n\n    # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n    if self._debug_mode:\n        messages = [\n            m\n            for m in messages\n            if not (m.metadata and m.metadata[\"message_type\"] in [\"system:log\", \"agent:log\"])  # type: ignore\n        ]\n    legacy_messages = [\n        {\n            \"id\": m.id,\n            \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n            \"role\": m.role,\n        }\n        for m in messages\n    ]\n    return legacy_messages\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_terminal_commands","title":"list_terminal_commands","text":"<pre><code>list_terminal_commands(filename: str = TERMINAL_FILENAME) -&gt; List[Any]\n</code></pre> <p>Returns the terminal commands from the terminal file.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_terminal_commands(self, filename: str = TERMINAL_FILENAME) -&gt; List[Any]:\n    \"\"\"Returns the terminal commands from the terminal file.\"\"\"\n    path = os.path.join(self._path, filename)\n\n    if not os.path.exists(path):\n        return []\n\n    with open(path, \"r\") as f:\n        return [json.loads(message) for message in f.read().split(DELIMITER) if message]\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.load_snapshot","title":"load_snapshot","text":"<pre><code>load_snapshot(snapshot: bytes) -&gt; None\n</code></pre> <p>Load Environment from Snapshot.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def load_snapshot(self, snapshot: bytes) -&gt; None:\n    \"\"\"Load Environment from Snapshot.\"\"\"\n    shutil.rmtree(self._path, ignore_errors=True)\n\n    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n        f.write(snapshot)\n        f.flush()\n        f.seek(0)\n\n        with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n            tar.extractall(self._path)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.read_file","title":"read_file","text":"<pre><code>read_file(filename: str)\n</code></pre> <p>Reads a file from the environment or thread.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def read_file(self, filename: str):\n    \"\"\"Reads a file from the environment or thread.\"\"\"\n    file_content = None\n    # First try to read from local filesystem\n    local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n    if os.path.exists(local_path):\n        with open(local_path, \"r\") as local_file:\n            file_content = local_file.read()\n\n    thread_files = self.list_files_from_thread(order=\"desc\")\n\n    # Then try to read from thread, starting from the most recent\n    for f in thread_files:\n        if f.filename == filename:\n            file_content = self.read_file_by_id(f.id)\n            break\n\n    # Write the file content to the local filesystem\n    if file_content:\n        with open(local_path, \"w\") as local_file:\n            local_file.write(file_content)\n    else:\n        self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n    return file_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.run","title":"run","text":"<pre><code>run(new_message: Optional[str] = None, max_iterations: int = 10) -&gt; None\n</code></pre> <p>Runs agent(s) against a new or previously created environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def run(\n    self,\n    new_message: Optional[str] = None,\n    max_iterations: int = 10,\n) -&gt; None:\n    \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n    if new_message:\n        self._add_message(\"user\", new_message)\n\n    iteration = 0\n    self.set_next_actor(\"agent\")\n\n    while iteration &lt; max_iterations and not self.is_done() and self.get_next_actor() != \"user\":\n        iteration += 1\n        if max_iterations &gt; 1:\n            self.add_system_log(\n                f\"Running agent, iteration {iteration}/{max_iterations}\",\n                logging.INFO,\n            )\n        try:\n            self._agents[0].run(self, task=new_message)\n        except Exception as e:\n            self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n            self.mark_failed()\n            raise e\n\n    if not self._pending_ext_agent:\n        # If no external agent was called, mark the whole run as done.\n        # Else this environment will stop for now but this run will be continued later.\n        self.mark_done()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.set_next_actor","title":"set_next_actor","text":"<pre><code>set_next_actor(who: str) -&gt; None\n</code></pre> <p>Set the next actor / action in the dialogue.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def set_next_actor(self, who: str) -&gt; None:\n    \"\"\"Set the next actor / action in the dialogue.\"\"\"\n    next_action_fn = os.path.join(self._path, \".next_action\")\n    if who == \"agent\":\n        self._done = False\n\n    with open(next_action_fn, \"w\") as f:\n        f.write(who)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.signed_completion","title":"signed_completion","text":"<pre><code>signed_completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; Dict[str, str]\n</code></pre> <p>Returns a completion for the given messages using the given model with the agent signature.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def signed_completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; Dict[str, str]:\n    \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n    # TODO Return signed completions for non-latest versions only?\n    agent_name = self.get_primary_agent().get_full_name()\n    raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n\n    signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n\n    return {\n        \"response\": response_message,\n        \"signature\": signature_data.get(\"signature\", None),\n        \"public_key\": signature_data.get(\"public_key\", None),\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_message","title":"verify_message","text":"<pre><code>verify_message(account_id: str, public_key: str, signature: str, message: str, nonce: str, callback_url: str) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies that the user message is signed with NEAR Account.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_message(\n    self,\n    account_id: str,\n    public_key: str,\n    signature: str,\n    message: str,\n    nonce: str,\n    callback_url: str,\n) -&gt; near.SignatureVerificationResult:\n    \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n    return near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        message,\n        nonce,\n        self._agents[0].name,\n        callback_url,\n    )\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(completion: str, messages: Union[Iterable[ChatCompletionMessageParam], str], public_key: Union[str, None] = None, signature: Union[str, None] = None, model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; bool\n</code></pre> <p>Verifies a signed message.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_signed_message(\n    self,\n    completion: str,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    public_key: Union[str, None] = None,\n    signature: Union[str, None] = None,\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; bool:\n    \"\"\"Verifies a signed message.\"\"\"\n    if public_key is None or signature is None:\n        return False\n\n    params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n    messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n    ordered_messages_without_ids = [\n        {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n    ]\n\n    return validate_completion_signature(\n        public_key,\n        signature,\n        CompletionSignaturePayload(\n            agent_name=self.get_primary_agent().get_full_name(),\n            completion=completion,\n            model=params.model,\n            messages=ordered_messages_without_ids,\n            temperature=params.temperature,\n            max_tokens=params.max_tokens,\n        ),\n    )\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper","title":"tool_json_helper","text":""},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args","title":"parse_json_args","text":"<pre><code>parse_json_args(signature: dict, args: str)\n</code></pre> <p>Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args(signature: dict, args: str):\n    \"\"\"Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.\"\"\"\n    # if args is empty or an empty json object check if the function has no arguments\n    if not args or args == \"{}\":\n        if not signature[\"function\"][\"parameters\"][\"required\"]:\n            return {}\n        else:\n            raise ValueError(\"Function requires arguments\")\n\n    transforms = [\n        lambda x: json.loads(x),\n        _ending_transform,\n        lambda x: parse_json_args_based_on_signature(signature, x),\n    ]\n\n    for transform in transforms:\n        try:\n            result = transform(args)\n            # check that all result keys are valid properties in the signature\n            for key in result.keys():\n                if key not in signature[\"function\"][\"parameters\"][\"properties\"]:\n                    raise json.JSONDecodeError(f\"Unknown parameter {key}\", args, 0)\n            return result\n        except json.JSONDecodeError:\n            continue\n        except Exception as err:\n            raise json.JSONDecodeError(\"Error parsing function args\", args, 0) from err\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args_based_on_signature","title":"parse_json_args_based_on_signature","text":"<pre><code>parse_json_args_based_on_signature(signature: dict, args: str)\n</code></pre> <p>Finds parameter names based on the signature and tries to extract the values in between from the args string.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args_based_on_signature(signature: dict, args: str):\n    \"\"\"Finds parameter names based on the signature and tries to extract the values in between from the args string.\"\"\"\n    parameter_names = list(signature[\"function\"][\"parameters\"][\"properties\"].keys())\n    # find each parameter name in the args string\n    #   assuming each parameter name is surrounded by \"s, followed by a colon and optionally preceded by a comma,\n    #   extract the intervening values as values\n    parameter_positions = {}\n    parameter_values = {}\n    for param in parameter_names:\n        match = re.search(f',?\\\\s*\"({param})\"\\\\s*:', args)\n        if not match:\n            raise ValueError(f\"Parameter {param} not found in args {args}\")\n        parameter_positions[param] = (match.start(), match.end())\n    # sort the parameter positions by start position\n    sorted_positions = sorted(parameter_positions.items(), key=lambda x: x[1][0])\n    # for each parameter, extract the value from the args string\n    for i, (param, (start, end)) in enumerate(sorted_positions):  # noqa B007\n        # if this is the last parameter, extract the value from the start position to the end of the string\n        if i == len(sorted_positions) - 1:\n            raw_value = args[end:-1]\n            if raw_value.endswith(\"}\"):\n                raw_value = raw_value[:-1]\n        # otherwise, extract the value from the start position to the start position of the next parameter\n        else:\n            next_start = sorted_positions[i + 1][1][0]\n            raw_value = args[end:next_start]\n        raw_value = raw_value.strip()\n        if raw_value.startswith('\"') and raw_value.endswith('\"'):\n            raw_value = raw_value[1:-1]\n        parameter_values[param] = raw_value\n    return parameter_values\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry","title":"tool_registry","text":""},{"location":"api/#nearai.agents.tool_registry.ToolRegistry","title":"ToolRegistry","text":"<p>A registry for tools that can be called by the agent.</p> <p>Tool definitions follow this structure:</p> <pre><code>{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n            \"required\": [\"location\"],\n        },\n    },\n}\n</code></pre> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"A registry for tools that can be called by the agent.\n\n    Tool definitions follow this structure:\n\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                    },\n                    \"required\": [\"location\"],\n                },\n            },\n        }\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.tools: Dict[str, Callable] = {}\n\n    def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n        \"\"\"Register a tool.\"\"\"\n        self.tools[tool.__name__] = tool\n\n    def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n        \"\"\"Get a tool by name.\"\"\"\n        return self.tools.get(name)\n\n    def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n        \"\"\"Get all tools.\"\"\"\n        return self.tools\n\n    def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n        \"\"\"Call a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            raise ValueError(f\"Tool '{name}' not found.\")\n        return tool(**kwargs)\n\n    def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n        \"\"\"Get the definition of a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            return None\n\n        assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n        docstring = tool.__doc__.strip().split(\"\\n\")\n\n        # The first line of the docstring is the function description\n        function_description = docstring[0].strip()\n\n        # The rest of the lines contain parameter descriptions\n        param_descriptions = docstring[1:]\n\n        # Extract parameter names and types\n        signature = inspect.signature(tool)\n        type_hints = get_type_hints(tool)\n\n        parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n        # Iterate through function parameters\n        for param in signature.parameters.values():\n            param_name = param.name\n            param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n            param_description = \"\"\n\n            # Find the parameter description in the docstring\n            for line in param_descriptions:\n                if line.strip().startswith(param_name):\n                    param_description = line.strip().split(\":\", 1)[1].strip()\n                    break\n\n            # Convert type hint to JSON Schema type\n            if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n                json_type = \"string\"\n            else:\n                json_type = param_type.__name__.lower()\n\n            json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n                json_type, json_type\n            )\n\n            # Add parameter to the definition\n            parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n            # Params without default values are required params\n            if param.default == inspect.Parameter.empty:\n                parameters[\"required\"].append(param_name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n        }\n\n    def get_all_tool_definitions(self) -&gt; list[Dict]:  # noqa: D102\n        definitions = []\n        for tool_name, _tool in self.tools.items():\n            definition = self.get_tool_definition(tool_name)\n            if definition is not None:\n                definitions.append(definition)\n        return definitions\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.call_tool","title":"call_tool","text":"<pre><code>call_tool(name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Call a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n    \"\"\"Call a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        raise ValueError(f\"Tool '{name}' not found.\")\n    return tool(**kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_all_tools","title":"get_all_tools","text":"<pre><code>get_all_tools() -&gt; Dict[str, Callable]\n</code></pre> <p>Get all tools.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n    \"\"\"Get all tools.\"\"\"\n    return self.tools\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool","title":"get_tool","text":"<pre><code>get_tool(name: str) -&gt; Optional[Callable]\n</code></pre> <p>Get a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n    \"\"\"Get a tool by name.\"\"\"\n    return self.tools.get(name)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool_definition","title":"get_tool_definition","text":"<pre><code>get_tool_definition(name: str) -&gt; Optional[Dict]\n</code></pre> <p>Get the definition of a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n    \"\"\"Get the definition of a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        return None\n\n    assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n    docstring = tool.__doc__.strip().split(\"\\n\")\n\n    # The first line of the docstring is the function description\n    function_description = docstring[0].strip()\n\n    # The rest of the lines contain parameter descriptions\n    param_descriptions = docstring[1:]\n\n    # Extract parameter names and types\n    signature = inspect.signature(tool)\n    type_hints = get_type_hints(tool)\n\n    parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n    # Iterate through function parameters\n    for param in signature.parameters.values():\n        param_name = param.name\n        param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n        param_description = \"\"\n\n        # Find the parameter description in the docstring\n        for line in param_descriptions:\n            if line.strip().startswith(param_name):\n                param_description = line.strip().split(\":\", 1)[1].strip()\n                break\n\n        # Convert type hint to JSON Schema type\n        if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n            json_type = \"string\"\n        else:\n            json_type = param_type.__name__.lower()\n\n        json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n            json_type, json_type\n        )\n\n        # Add parameter to the definition\n        parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n        # Params without default values are required params\n        if param.default == inspect.Parameter.empty:\n            parameters[\"required\"].append(param_name)\n\n    return {\n        \"type\": \"function\",\n        \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n    }\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.register_tool","title":"register_tool","text":"<pre><code>register_tool(tool: Callable) -&gt; None\n</code></pre> <p>Register a tool.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n    \"\"\"Register a tool.\"\"\"\n    self.tools[tool.__name__] = tool\n</code></pre>"},{"location":"api/#nearai.cli","title":"cli","text":""},{"location":"api/#nearai.cli.AgentCli","title":"AgentCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class AgentCli:\n    def dev(self) -&gt; int:\n        \"\"\"Run local UI for development of agents that have their own UI.\"\"\"\n        if not os.path.exists(\"hub/demo/.env\"):\n            shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n        ret_val = os.system(\"npm install --prefix hub/demo\")\n        if ret_val != 0:\n            print(\"Node.js is required to run the development server.\")\n            print(\"Please install Node.js from https://nodejs.org/\")\n        ret_val = os.system(\"npm run dev --prefix hub/demo\")\n        return ret_val\n\n    def inspect(self, path: str) -&gt; None:\n        \"\"\"Inspect environment from given path.\"\"\"\n        import subprocess\n\n        filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n        subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n\n    def interactive(\n        self,\n        agent: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        local: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Runs agent interactively.\"\"\"\n        last_message_id = None\n        while True:\n            new_message = input(\"&gt; \")\n            if new_message.lower() == \"exit\":\n                break\n\n            last_message_id = self._task(\n                agent=agent,\n                task=new_message,\n                thread_id=thread_id,\n                tool_resources=tool_resources,\n                last_message_id=last_message_id,\n                local=local,\n                env_vars=env_vars,\n            )\n\n            # Update thread_id for the next iteration\n            if thread_id is None:\n                thread_id = self.last_thread_id\n\n    def task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        local: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"CLI wrapper for the _task method.\"\"\"\n        last_message_id = self._task(\n            agent=agent,\n            task=task,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            file_ids=file_ids,\n            local=local,\n            env_vars=env_vars,\n        )\n        if last_message_id:\n            print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n            print(f\"Last message ID: {last_message_id}\")\n\n    def _task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        last_message_id: Optional[str] = None,\n        local: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; Optional[str]:\n        \"\"\"Runs agent non-interactively with a single task.\"\"\"\n        hub_client = get_hub_client()\n        if thread_id:\n            thread = hub_client.beta.threads.retrieve(thread_id)\n        else:\n            thread = hub_client.beta.threads.create(\n                tool_resources=tool_resources,\n            )\n\n        hub_client.beta.threads.messages.create(\n            thread_id=thread.id,\n            role=\"user\",\n            content=task,\n            attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n        )\n\n        if not local:\n            hub_client.beta.threads.runs.create_and_poll(\n                thread_id=thread.id,\n                assistant_id=agent,\n            )\n        else:\n            run = hub_client.beta.threads.runs.create(\n                thread_id=thread.id,\n                assistant_id=agent,\n                extra_body={\"delegate_execution\": True},\n            )\n            params = {\n                \"api_url\": CONFIG.api_url,\n                \"tool_resources\": run.tools,\n                \"data_source\": \"local_files\",\n                \"user_env_vars\": env_vars,\n                \"agent_env_vars\": {},\n            }\n            auth = CONFIG.auth\n            assert auth is not None\n            LocalRunner(agent, agent, thread.id, run.id, auth, params)\n\n        # List new messages\n        messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n        message_list = list(messages)\n        if message_list:\n            for msg in message_list:\n                if msg.metadata and msg.metadata.get(\"message_type\"):\n                    continue\n                if msg.role == \"assistant\":\n                    print(f\"Assistant: {msg.content[0].text.value}\")\n            last_message_id = message_list[-1].id\n        else:\n            print(\"No new messages\")\n\n        # Store the thread_id for potential use in interactive mode\n        self.last_thread_id = thread.id\n\n        return last_message_id\n\n    def create(self, name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None:\n        \"\"\"Create a new agent or fork an existing one.\n\n        Usage:\n          nearai agent create\n          nearai agent create --name &lt;agent_name&gt; --description &lt;description&gt;\n          nearai agent create --fork &lt;namespace/agent_name/version&gt; [--name &lt;new_agent_name&gt;]\n\n        Options:\n          --name          Name of the new agent.\n          --description   Description of the new agent.\n          --fork          Fork an existing agent specified by namespace/agent_name/version.\n\n        Examples\n        --------\n          nearai agent create\n          nearai agent create --name my_agent --description \"My new agent\"\n          nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent\n\n        \"\"\"\n        # Check if the user is authenticated\n        if CONFIG.auth is None or CONFIG.auth.namespace is None:\n            print(\"Please login with `nearai login` before creating an agent.\")\n            return\n\n        namespace = CONFIG.auth.namespace\n\n        if fork:\n            # Fork an existing agent\n            self._fork_agent(fork, namespace, name)\n        else:\n            # Create a new agent from scratch\n            self._create_new_agent(namespace, name, description)\n\n    def _create_new_agent(self, namespace: str, name: Optional[str], description: Optional[str]) -&gt; None:\n        \"\"\"Create a new agent from scratch.\"\"\"\n        # Prompt for agent name if not provided\n        if not name or not isinstance(name, str):\n            name = input(\"Name: \").strip()\n            while not name or not isinstance(name, str):\n                print(\"Agent name cannot be empty.\")\n                name = input(\"Name: \").strip()\n\n        # Prompt for description if not provided\n        while not description or not isinstance(description, str):\n            print(\"A description is needed for agent matching and cannot be empty.\")\n            description = input(\"Description: \").strip()\n\n        # Set the agent path\n        agent_path = get_registry_folder() / namespace / name / \"0.0.1\"\n        agent_path.mkdir(parents=True, exist_ok=True)\n\n        # Create metadata.json\n        metadata = {\n            \"name\": name,\n            \"version\": \"0.0.1\",\n            \"description\": description,\n            \"category\": \"agent\",\n            \"tags\": [],\n            \"details\": {\n                \"agent\": {\n                    \"defaults\": {\n                        \"model\": DEFAULT_MODEL,\n                        \"model_provider\": DEFAULT_PROVIDER,\n                        \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                        \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    }\n                }\n            },\n            \"show_entry\": True,\n        }\n\n        metadata_path = agent_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        # Create a default agent.py\n        agent_py_content = \"\"\"from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Your agent code here\n    # Example:\n    prompt = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    result = env.completion([prompt] + env.list_messages())\n    env.add_reply(result)\n    env.request_user_input()\n\nrun(env)\n\n\"\"\"\n        agent_py_path = agent_path / \"agent.py\"\n        with open(agent_py_path, \"w\") as f:\n            f.write(agent_py_content)\n\n        print(f\"\\nAgent created at: {agent_path}\")\n        print(\"Consider editing:\")\n        print(f\"\\t{agent_path}/agent.py\")\n        print(f\"\\t{agent_path}/metadata.json\")\n        print(\"\\nUseful commands:\")\n        print(f\"  &gt; nearai agent interactive {agent_path} --local\")\n        print(f\"  &gt; nearai registry upload {agent_path}\")\n\n    def _fork_agent(self, fork: str, namespace: str, new_name: Optional[str]) -&gt; None:\n        \"\"\"Fork an existing agent.\"\"\"\n        import shutil\n\n        # Parse the fork parameter\n        try:\n            entry_location = parse_location(fork)\n            fork_namespace = entry_location.namespace\n            fork_name = entry_location.name\n            fork_version = entry_location.version\n        except ValueError:\n            print(\"Invalid fork parameter format. Expected format: &lt;namespace&gt;/&lt;agent-name&gt;/&lt;version&gt;\")\n            return\n\n        # Download the agent from the registry\n        agent_location = f\"{fork_namespace}/{fork_name}/{fork_version}\"\n        print(f\"Downloading agent '{agent_location}'...\")\n        registry.download(agent_location, force=False, show_progress=True)\n        source_path = get_registry_folder() / fork_namespace / fork_name / fork_version\n\n        # Prompt for the new agent name if not provided\n        if not new_name:\n            new_name = input(\"Enter the new agent name: \").strip()\n            if not new_name:\n                print(\"Agent name cannot be empty.\")\n                return\n\n            # confirm pattern is ok\n            identifier_pattern = re.compile(r\"^[a-zA-Z0-9_\\-.]+$\")\n            if identifier_pattern.match(new_name) is None:\n                print(\"Invalid Name, please choose something different\")\n                return\n\n        # Set the destination path\n        dest_path = get_registry_folder() / namespace / new_name / \"0.0.1\"\n\n        # Copy the agent files\n        shutil.copytree(source_path, dest_path)\n\n        # Update metadata.json\n        metadata_path = dest_path / \"metadata.json\"\n        with open(metadata_path, \"r\") as file:\n            metadata = json.load(file)\n\n        metadata[\"name\"] = new_name\n        metadata[\"version\"] = \"0.0.1\"\n\n        with open(metadata_path, \"w\") as file:\n            json.dump(metadata, file, indent=2)\n\n        print(f\"\\nForked agent '{agent_location}' to '{dest_path}'\")\n        print(f\"Agent '{new_name}' created at '{dest_path}' with updated metadata.\")\n        print(\"\\nUseful commands:\")\n        print(f\"  &gt; nearai agent interactive {new_name} --local\")\n        print(f\"  &gt; nearai registry upload {dest_path}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._create_new_agent","title":"_create_new_agent","text":"<pre><code>_create_new_agent(namespace: str, name: Optional[str], description: Optional[str]) -&gt; None\n</code></pre> <p>Create a new agent from scratch.</p> Source code in <code>nearai/cli.py</code> <pre><code>    def _create_new_agent(self, namespace: str, name: Optional[str], description: Optional[str]) -&gt; None:\n        \"\"\"Create a new agent from scratch.\"\"\"\n        # Prompt for agent name if not provided\n        if not name or not isinstance(name, str):\n            name = input(\"Name: \").strip()\n            while not name or not isinstance(name, str):\n                print(\"Agent name cannot be empty.\")\n                name = input(\"Name: \").strip()\n\n        # Prompt for description if not provided\n        while not description or not isinstance(description, str):\n            print(\"A description is needed for agent matching and cannot be empty.\")\n            description = input(\"Description: \").strip()\n\n        # Set the agent path\n        agent_path = get_registry_folder() / namespace / name / \"0.0.1\"\n        agent_path.mkdir(parents=True, exist_ok=True)\n\n        # Create metadata.json\n        metadata = {\n            \"name\": name,\n            \"version\": \"0.0.1\",\n            \"description\": description,\n            \"category\": \"agent\",\n            \"tags\": [],\n            \"details\": {\n                \"agent\": {\n                    \"defaults\": {\n                        \"model\": DEFAULT_MODEL,\n                        \"model_provider\": DEFAULT_PROVIDER,\n                        \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                        \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    }\n                }\n            },\n            \"show_entry\": True,\n        }\n\n        metadata_path = agent_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        # Create a default agent.py\n        agent_py_content = \"\"\"from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Your agent code here\n    # Example:\n    prompt = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    result = env.completion([prompt] + env.list_messages())\n    env.add_reply(result)\n    env.request_user_input()\n\nrun(env)\n\n\"\"\"\n        agent_py_path = agent_path / \"agent.py\"\n        with open(agent_py_path, \"w\") as f:\n            f.write(agent_py_content)\n\n        print(f\"\\nAgent created at: {agent_path}\")\n        print(\"Consider editing:\")\n        print(f\"\\t{agent_path}/agent.py\")\n        print(f\"\\t{agent_path}/metadata.json\")\n        print(\"\\nUseful commands:\")\n        print(f\"  &gt; nearai agent interactive {agent_path} --local\")\n        print(f\"  &gt; nearai registry upload {agent_path}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._fork_agent","title":"_fork_agent","text":"<pre><code>_fork_agent(fork: str, namespace: str, new_name: Optional[str]) -&gt; None\n</code></pre> <p>Fork an existing agent.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _fork_agent(self, fork: str, namespace: str, new_name: Optional[str]) -&gt; None:\n    \"\"\"Fork an existing agent.\"\"\"\n    import shutil\n\n    # Parse the fork parameter\n    try:\n        entry_location = parse_location(fork)\n        fork_namespace = entry_location.namespace\n        fork_name = entry_location.name\n        fork_version = entry_location.version\n    except ValueError:\n        print(\"Invalid fork parameter format. Expected format: &lt;namespace&gt;/&lt;agent-name&gt;/&lt;version&gt;\")\n        return\n\n    # Download the agent from the registry\n    agent_location = f\"{fork_namespace}/{fork_name}/{fork_version}\"\n    print(f\"Downloading agent '{agent_location}'...\")\n    registry.download(agent_location, force=False, show_progress=True)\n    source_path = get_registry_folder() / fork_namespace / fork_name / fork_version\n\n    # Prompt for the new agent name if not provided\n    if not new_name:\n        new_name = input(\"Enter the new agent name: \").strip()\n        if not new_name:\n            print(\"Agent name cannot be empty.\")\n            return\n\n        # confirm pattern is ok\n        identifier_pattern = re.compile(r\"^[a-zA-Z0-9_\\-.]+$\")\n        if identifier_pattern.match(new_name) is None:\n            print(\"Invalid Name, please choose something different\")\n            return\n\n    # Set the destination path\n    dest_path = get_registry_folder() / namespace / new_name / \"0.0.1\"\n\n    # Copy the agent files\n    shutil.copytree(source_path, dest_path)\n\n    # Update metadata.json\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    metadata[\"name\"] = new_name\n    metadata[\"version\"] = \"0.0.1\"\n\n    with open(metadata_path, \"w\") as file:\n        json.dump(metadata, file, indent=2)\n\n    print(f\"\\nForked agent '{agent_location}' to '{dest_path}'\")\n    print(f\"Agent '{new_name}' created at '{dest_path}' with updated metadata.\")\n    print(\"\\nUseful commands:\")\n    print(f\"  &gt; nearai agent interactive {new_name} --local\")\n    print(f\"  &gt; nearai registry upload {dest_path}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._task","title":"_task","text":"<pre><code>_task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, last_message_id: Optional[str] = None, local: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; Optional[str]\n</code></pre> <p>Runs agent non-interactively with a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    last_message_id: Optional[str] = None,\n    local: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; Optional[str]:\n    \"\"\"Runs agent non-interactively with a single task.\"\"\"\n    hub_client = get_hub_client()\n    if thread_id:\n        thread = hub_client.beta.threads.retrieve(thread_id)\n    else:\n        thread = hub_client.beta.threads.create(\n            tool_resources=tool_resources,\n        )\n\n    hub_client.beta.threads.messages.create(\n        thread_id=thread.id,\n        role=\"user\",\n        content=task,\n        attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n    )\n\n    if not local:\n        hub_client.beta.threads.runs.create_and_poll(\n            thread_id=thread.id,\n            assistant_id=agent,\n        )\n    else:\n        run = hub_client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=agent,\n            extra_body={\"delegate_execution\": True},\n        )\n        params = {\n            \"api_url\": CONFIG.api_url,\n            \"tool_resources\": run.tools,\n            \"data_source\": \"local_files\",\n            \"user_env_vars\": env_vars,\n            \"agent_env_vars\": {},\n        }\n        auth = CONFIG.auth\n        assert auth is not None\n        LocalRunner(agent, agent, thread.id, run.id, auth, params)\n\n    # List new messages\n    messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n    message_list = list(messages)\n    if message_list:\n        for msg in message_list:\n            if msg.metadata and msg.metadata.get(\"message_type\"):\n                continue\n            if msg.role == \"assistant\":\n                print(f\"Assistant: {msg.content[0].text.value}\")\n        last_message_id = message_list[-1].id\n    else:\n        print(\"No new messages\")\n\n    # Store the thread_id for potential use in interactive mode\n    self.last_thread_id = thread.id\n\n    return last_message_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.create","title":"create","text":"<pre><code>create(name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None\n</code></pre> <p>Create a new agent or fork an existing one.</p> Usage <p>nearai agent create nearai agent create --name  --description  nearai agent create --fork  [--name ] Options <p>--name          Name of the new agent. --description   Description of the new agent. --fork          Fork an existing agent specified by namespace/agent_name/version.</p>"},{"location":"api/#nearai.cli.AgentCli.create--examples","title":"Examples","text":"<p>nearai agent create   nearai agent create --name my_agent --description \"My new agent\"   nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent</p> Source code in <code>nearai/cli.py</code> <pre><code>def create(self, name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None:\n    \"\"\"Create a new agent or fork an existing one.\n\n    Usage:\n      nearai agent create\n      nearai agent create --name &lt;agent_name&gt; --description &lt;description&gt;\n      nearai agent create --fork &lt;namespace/agent_name/version&gt; [--name &lt;new_agent_name&gt;]\n\n    Options:\n      --name          Name of the new agent.\n      --description   Description of the new agent.\n      --fork          Fork an existing agent specified by namespace/agent_name/version.\n\n    Examples\n    --------\n      nearai agent create\n      nearai agent create --name my_agent --description \"My new agent\"\n      nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent\n\n    \"\"\"\n    # Check if the user is authenticated\n    if CONFIG.auth is None or CONFIG.auth.namespace is None:\n        print(\"Please login with `nearai login` before creating an agent.\")\n        return\n\n    namespace = CONFIG.auth.namespace\n\n    if fork:\n        # Fork an existing agent\n        self._fork_agent(fork, namespace, name)\n    else:\n        # Create a new agent from scratch\n        self._create_new_agent(namespace, name, description)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.dev","title":"dev","text":"<pre><code>dev() -&gt; int\n</code></pre> <p>Run local UI for development of agents that have their own UI.</p> Source code in <code>nearai/cli.py</code> <pre><code>def dev(self) -&gt; int:\n    \"\"\"Run local UI for development of agents that have their own UI.\"\"\"\n    if not os.path.exists(\"hub/demo/.env\"):\n        shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n    ret_val = os.system(\"npm install --prefix hub/demo\")\n    if ret_val != 0:\n        print(\"Node.js is required to run the development server.\")\n        print(\"Please install Node.js from https://nodejs.org/\")\n    ret_val = os.system(\"npm run dev --prefix hub/demo\")\n    return ret_val\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.inspect","title":"inspect","text":"<pre><code>inspect(path: str) -&gt; None\n</code></pre> <p>Inspect environment from given path.</p> Source code in <code>nearai/cli.py</code> <pre><code>def inspect(self, path: str) -&gt; None:\n    \"\"\"Inspect environment from given path.\"\"\"\n    import subprocess\n\n    filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n    subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.interactive","title":"interactive","text":"<pre><code>interactive(agent: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, local: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Runs agent interactively.</p> Source code in <code>nearai/cli.py</code> <pre><code>def interactive(\n    self,\n    agent: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    local: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Runs agent interactively.\"\"\"\n    last_message_id = None\n    while True:\n        new_message = input(\"&gt; \")\n        if new_message.lower() == \"exit\":\n            break\n\n        last_message_id = self._task(\n            agent=agent,\n            task=new_message,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            last_message_id=last_message_id,\n            local=local,\n            env_vars=env_vars,\n        )\n\n        # Update thread_id for the next iteration\n        if thread_id is None:\n            thread_id = self.last_thread_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.task","title":"task","text":"<pre><code>task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, local: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>CLI wrapper for the _task method.</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    local: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"CLI wrapper for the _task method.\"\"\"\n    last_message_id = self._task(\n        agent=agent,\n        task=task,\n        thread_id=thread_id,\n        tool_resources=tool_resources,\n        file_ids=file_ids,\n        local=local,\n        env_vars=env_vars,\n    )\n    if last_message_id:\n        print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n        print(f\"Last message ID: {last_message_id}\")\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli","title":"BenchmarkCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class BenchmarkCli:\n    def __init__(self):\n        \"\"\"Initialize Benchmark API.\"\"\"\n        self.client = BenchmarkApi()\n\n    def _get_or_create_benchmark(self, benchmark_name: str, solver_name: str, args: Dict[str, Any], force: bool) -&gt; int:\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n        namespace = CONFIG.auth.namespace\n\n        # Sort the args to have a consistent representation.\n        solver_args = json.dumps(OrderedDict(sorted(args.items())))\n\n        benchmark_id = self.client.get_benchmark_v1_benchmark_get_get(\n            namespace=namespace,\n            benchmark_name=benchmark_name,\n            solver_name=solver_name,\n            solver_args=solver_args,\n        )\n\n        if benchmark_id == -1 or force:\n            benchmark_id = self.client.create_benchmark_v1_benchmark_create_get(\n                benchmark_name=benchmark_name,\n                solver_name=solver_name,\n                solver_args=solver_args,\n            )\n\n        assert benchmark_id != -1\n        return benchmark_id\n\n    def run(\n        self,\n        dataset: str,\n        solver_strategy: str,\n        max_concurrent: int = 2,\n        force: bool = False,\n        subset: Optional[str] = None,\n        check_compatibility: bool = True,\n        record: bool = False,\n        num_inference_retries: int = 10,\n        **solver_args: Any,\n    ) -&gt; None:\n        \"\"\"Run benchmark on a dataset with a solver strategy.\n\n        It will cache the results in the database and subsequent runs will pull the results from the cache.\n        If force is set to True, it will run the benchmark again and update the cache.\n        \"\"\"\n        from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n        from nearai.dataset import get_dataset, load_dataset\n        from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n        CONFIG.num_inference_retries = num_inference_retries\n\n        args = dict(solver_args)\n        if subset is not None:\n            args[\"subset\"] = subset\n\n        benchmark_id = self._get_or_create_benchmark(\n            benchmark_name=dataset,\n            solver_name=solver_strategy,\n            args=args,\n            force=force,\n        )\n\n        solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n        assert (\n            solver_strategy_class\n        ), f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n\n        name = dataset\n        if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n            dataset = str(get_dataset(dataset))\n        else:\n            dataset = load_dataset(dataset)\n\n        solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n        if check_compatibility:\n            assert name in solver_strategy_obj.compatible_datasets() or any(\n                map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n            ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n        dest_path = get_registry_folder() / name\n        metadata_path = dest_path / \"metadata.json\"\n        with open(metadata_path, \"r\") as file:\n            metadata = json.load(file)\n\n        be = BenchmarkExecutor(\n            DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n        )\n\n        cpu_count = os.cpu_count()\n        max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n        be.run(max_concurrent=max_concurrent, record=record)\n\n    def list(\n        self,\n        namespace: Optional[str] = None,\n        benchmark: Optional[str] = None,\n        solver: Optional[str] = None,\n        args: Optional[str] = None,\n        total: int = 32,\n        offset: int = 0,\n    ):\n        \"\"\"List all executed benchmarks.\"\"\"\n        result = self.client.list_benchmarks_v1_benchmark_list_get(\n            namespace=namespace,\n            benchmark_name=benchmark,\n            solver_name=solver,\n            solver_args=args,\n            total=total,\n            offset=offset,\n        )\n\n        header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n        table = []\n        for benchmark_output in result:\n            score = 100 * benchmark_output.solved / benchmark_output.total\n            table.append(\n                [\n                    fill(str(benchmark_output.id)),\n                    fill(benchmark_output.namespace),\n                    fill(benchmark_output.benchmark),\n                    fill(benchmark_output.solver),\n                    fill(benchmark_output.args),\n                    fill(f\"{score:.2f}%\"),\n                    fill(str(benchmark_output.solved)),\n                    fill(str(benchmark_output.total)),\n                ]\n            )\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize Benchmark API.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize Benchmark API.\"\"\"\n    self.client = BenchmarkApi()\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.list","title":"list","text":"<pre><code>list(namespace: Optional[str] = None, benchmark: Optional[str] = None, solver: Optional[str] = None, args: Optional[str] = None, total: int = 32, offset: int = 0)\n</code></pre> <p>List all executed benchmarks.</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: Optional[str] = None,\n    benchmark: Optional[str] = None,\n    solver: Optional[str] = None,\n    args: Optional[str] = None,\n    total: int = 32,\n    offset: int = 0,\n):\n    \"\"\"List all executed benchmarks.\"\"\"\n    result = self.client.list_benchmarks_v1_benchmark_list_get(\n        namespace=namespace,\n        benchmark_name=benchmark,\n        solver_name=solver,\n        solver_args=args,\n        total=total,\n        offset=offset,\n    )\n\n    header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n    table = []\n    for benchmark_output in result:\n        score = 100 * benchmark_output.solved / benchmark_output.total\n        table.append(\n            [\n                fill(str(benchmark_output.id)),\n                fill(benchmark_output.namespace),\n                fill(benchmark_output.benchmark),\n                fill(benchmark_output.solver),\n                fill(benchmark_output.args),\n                fill(f\"{score:.2f}%\"),\n                fill(str(benchmark_output.solved)),\n                fill(str(benchmark_output.total)),\n            ]\n        )\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.run","title":"run","text":"<pre><code>run(dataset: str, solver_strategy: str, max_concurrent: int = 2, force: bool = False, subset: Optional[str] = None, check_compatibility: bool = True, record: bool = False, num_inference_retries: int = 10, **solver_args: Any) -&gt; None\n</code></pre> <p>Run benchmark on a dataset with a solver strategy.</p> <p>It will cache the results in the database and subsequent runs will pull the results from the cache. If force is set to True, it will run the benchmark again and update the cache.</p> Source code in <code>nearai/cli.py</code> <pre><code>def run(\n    self,\n    dataset: str,\n    solver_strategy: str,\n    max_concurrent: int = 2,\n    force: bool = False,\n    subset: Optional[str] = None,\n    check_compatibility: bool = True,\n    record: bool = False,\n    num_inference_retries: int = 10,\n    **solver_args: Any,\n) -&gt; None:\n    \"\"\"Run benchmark on a dataset with a solver strategy.\n\n    It will cache the results in the database and subsequent runs will pull the results from the cache.\n    If force is set to True, it will run the benchmark again and update the cache.\n    \"\"\"\n    from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n    from nearai.dataset import get_dataset, load_dataset\n    from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n    CONFIG.num_inference_retries = num_inference_retries\n\n    args = dict(solver_args)\n    if subset is not None:\n        args[\"subset\"] = subset\n\n    benchmark_id = self._get_or_create_benchmark(\n        benchmark_name=dataset,\n        solver_name=solver_strategy,\n        args=args,\n        force=force,\n    )\n\n    solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n    assert (\n        solver_strategy_class\n    ), f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n\n    name = dataset\n    if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n        dataset = str(get_dataset(dataset))\n    else:\n        dataset = load_dataset(dataset)\n\n    solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n    if check_compatibility:\n        assert name in solver_strategy_obj.compatible_datasets() or any(\n            map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n        ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n    dest_path = get_registry_folder() / name\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    be = BenchmarkExecutor(\n        DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n    )\n\n    cpu_count = os.cpu_count()\n    max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n    be.run(max_concurrent=max_concurrent, record=record)\n</code></pre>"},{"location":"api/#nearai.cli.CLI","title":"CLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class CLI:\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.registry = RegistryCli()\n        self.login = LoginCLI()\n        self.logout = LogoutCLI()\n        self.hub = HubCLI()\n        self.log = LogCLI()\n\n        self.config = ConfigCli()\n        self.benchmark = BenchmarkCli()\n        self.evaluation = EvaluationCli()\n        self.agent = AgentCli()\n        self.finetune = FinetuneCli()\n        self.tensorboard = TensorboardCli()\n        self.vllm = VllmCli()\n        self.permission = PermissionCli()\n\n    def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n        \"\"\"Submit a task to be executed by a worker.\"\"\"\n        if path is None:\n            path = os.getcwd()\n\n        worker_kind_t = WorkerKind(worker_kind)\n\n        location = self.registry.upload(path)\n\n        delegation_api = DelegationApi()\n        delegation_api.delegate_v1_delegation_delegate_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n            expires_at=datetime.now() + timedelta(days=1),\n        )\n\n        try:\n            client = JobsApi()\n            client.add_job_v1_jobs_add_job_post(\n                worker_kind_t,\n                BodyAddJobV1JobsAddJobPost(entry_location=location),\n            )\n        except Exception as e:\n            print(\"Error: \", e)\n            delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n                delegate_account_id=CONFIG.scheduler_account_id,\n            )\n\n    def location(self) -&gt; None:  # noqa: D102\n        \"\"\"Show location where nearai is installed.\"\"\"\n        from nearai import cli_path\n\n        print(cli_path())\n\n    def version(self):\n        \"\"\"Show nearai version.\"\"\"\n        print(importlib.metadata.version(\"nearai\"))\n\n    def task(self, *args, **kwargs):\n        \"\"\"CLI command for running a single task.\"\"\"\n        self.agent.task_cli(*args, **kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.location","title":"location","text":"<pre><code>location() -&gt; None\n</code></pre> <p>Show location where nearai is installed.</p> Source code in <code>nearai/cli.py</code> <pre><code>def location(self) -&gt; None:  # noqa: D102\n    \"\"\"Show location where nearai is installed.\"\"\"\n    from nearai import cli_path\n\n    print(cli_path())\n</code></pre>"},{"location":"api/#nearai.cli.CLI.submit","title":"submit","text":"<pre><code>submit(path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value)\n</code></pre> <p>Submit a task to be executed by a worker.</p> Source code in <code>nearai/cli.py</code> <pre><code>def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n    \"\"\"Submit a task to be executed by a worker.\"\"\"\n    if path is None:\n        path = os.getcwd()\n\n    worker_kind_t = WorkerKind(worker_kind)\n\n    location = self.registry.upload(path)\n\n    delegation_api = DelegationApi()\n    delegation_api.delegate_v1_delegation_delegate_post(\n        delegate_account_id=CONFIG.scheduler_account_id,\n        expires_at=datetime.now() + timedelta(days=1),\n    )\n\n    try:\n        client = JobsApi()\n        client.add_job_v1_jobs_add_job_post(\n            worker_kind_t,\n            BodyAddJobV1JobsAddJobPost(entry_location=location),\n        )\n    except Exception as e:\n        print(\"Error: \", e)\n        delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n        )\n</code></pre>"},{"location":"api/#nearai.cli.CLI.task","title":"task","text":"<pre><code>task(*args, **kwargs)\n</code></pre> <p>CLI command for running a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(self, *args, **kwargs):\n    \"\"\"CLI command for running a single task.\"\"\"\n    self.agent.task_cli(*args, **kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.version","title":"version","text":"<pre><code>version()\n</code></pre> <p>Show nearai version.</p> Source code in <code>nearai/cli.py</code> <pre><code>def version(self):\n    \"\"\"Show nearai version.\"\"\"\n    print(importlib.metadata.version(\"nearai\"))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli","title":"ConfigCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class ConfigCli:\n    def set(self, key: str, value: str, local: bool = False) -&gt; None:\n        \"\"\"Add key-value pair to the config file.\"\"\"\n        update_config(key, value, local)\n\n    def get(self, key: str) -&gt; None:\n        \"\"\"Get value of a key in the config file.\"\"\"\n        print(CONFIG.get(key))\n\n    def show(self) -&gt; None:  # noqa: D102\n        for key, value in asdict(CONFIG).items():\n            print(f\"{key}: {value}\")\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.get","title":"get","text":"<pre><code>get(key: str) -&gt; None\n</code></pre> <p>Get value of a key in the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def get(self, key: str) -&gt; None:\n    \"\"\"Get value of a key in the config file.\"\"\"\n    print(CONFIG.get(key))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.set","title":"set","text":"<pre><code>set(key: str, value: str, local: bool = False) -&gt; None\n</code></pre> <p>Add key-value pair to the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def set(self, key: str, value: str, local: bool = False) -&gt; None:\n    \"\"\"Add key-value pair to the config file.\"\"\"\n    update_config(key, value, local)\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli","title":"EvaluationCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class EvaluationCli:\n    def table(\n        self,\n        all_key_columns: bool = False,\n        all_metrics: bool = False,\n        num_columns: int = 6,\n        metric_name_max_length: int = 30,\n    ) -&gt; None:\n        \"\"\"Prints table of evaluations.\"\"\"\n        from nearai.evaluation import print_evaluation_table\n\n        api = EvaluationApi()\n        table = api.table_v1_evaluation_table_get()\n\n        print_evaluation_table(\n            table.rows,\n            table.columns,\n            table.important_columns,\n            all_key_columns,\n            all_metrics,\n            num_columns,\n            metric_name_max_length,\n        )\n\n    def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n        \"\"\"Reads solutions.json from evaluation entry.\"\"\"\n        entry_path = registry.download(entry)\n        solutions_file = entry_path / \"solutions.json\"\n\n        if not solutions_file.exists():\n            print(f\"No solutions file found for entry: {entry}\")\n            return\n\n        try:\n            with open(solutions_file) as f:\n                solutions = json.load(f)\n        except json.JSONDecodeError:\n            print(f\"Error reading solutions file for entry: {entry}\")\n            return\n\n        # Filter solutions if status is specified\n        if status is not None:\n            solutions = [s for s in solutions if s.get(\"status\") == status]\n        if not solutions:\n            print(\"No solutions found matching criteria\")\n            return\n        print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n        for i, solution in enumerate(solutions, 1):\n            print(\"-\" * 80)\n            print(f\"\\nSolution {i}/{len(solutions)}:\")\n            datum = solution.get(\"datum\")\n            print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n            status = solution.get(\"status\")\n            print(f\"status: {status}\")\n            info: dict = solution.get(\"info\", {})\n            if not verbose:\n                info.pop(\"verbose\")\n            print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n            if i == 1:\n                print(\"Enter to continue, type 'exit' to quit.\")\n            new_message = input(\"&gt; \")\n            if new_message.lower() == \"exit\":\n                break\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.read_solutions","title":"read_solutions","text":"<pre><code>read_solutions(entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None\n</code></pre> <p>Reads solutions.json from evaluation entry.</p> Source code in <code>nearai/cli.py</code> <pre><code>def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n    \"\"\"Reads solutions.json from evaluation entry.\"\"\"\n    entry_path = registry.download(entry)\n    solutions_file = entry_path / \"solutions.json\"\n\n    if not solutions_file.exists():\n        print(f\"No solutions file found for entry: {entry}\")\n        return\n\n    try:\n        with open(solutions_file) as f:\n            solutions = json.load(f)\n    except json.JSONDecodeError:\n        print(f\"Error reading solutions file for entry: {entry}\")\n        return\n\n    # Filter solutions if status is specified\n    if status is not None:\n        solutions = [s for s in solutions if s.get(\"status\") == status]\n    if not solutions:\n        print(\"No solutions found matching criteria\")\n        return\n    print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n    for i, solution in enumerate(solutions, 1):\n        print(\"-\" * 80)\n        print(f\"\\nSolution {i}/{len(solutions)}:\")\n        datum = solution.get(\"datum\")\n        print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n        status = solution.get(\"status\")\n        print(f\"status: {status}\")\n        info: dict = solution.get(\"info\", {})\n        if not verbose:\n            info.pop(\"verbose\")\n        print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n        if i == 1:\n            print(\"Enter to continue, type 'exit' to quit.\")\n        new_message = input(\"&gt; \")\n        if new_message.lower() == \"exit\":\n            break\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.table","title":"table","text":"<pre><code>table(all_key_columns: bool = False, all_metrics: bool = False, num_columns: int = 6, metric_name_max_length: int = 30) -&gt; None\n</code></pre> <p>Prints table of evaluations.</p> Source code in <code>nearai/cli.py</code> <pre><code>def table(\n    self,\n    all_key_columns: bool = False,\n    all_metrics: bool = False,\n    num_columns: int = 6,\n    metric_name_max_length: int = 30,\n) -&gt; None:\n    \"\"\"Prints table of evaluations.\"\"\"\n    from nearai.evaluation import print_evaluation_table\n\n    api = EvaluationApi()\n    table = api.table_v1_evaluation_table_get()\n\n    print_evaluation_table(\n        table.rows,\n        table.columns,\n        table.important_columns,\n        all_key_columns,\n        all_metrics,\n        num_columns,\n        metric_name_max_length,\n    )\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI","title":"HubCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class HubCLI:\n    def chat(self, **kwargs):\n        \"\"\"Chat with model from NEAR AI hub.\n\n        Args:\n        ----\n            query (str): User's query to model\n            endpoint (str): NEAR AI HUB's url\n            model (str): Name of a model\n            provider (str): Name of a provider\n            info (bool): Display system info\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.hub import Hub\n\n        hub = Hub(CONFIG)\n        hub.chat(kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI.chat","title":"chat","text":"<pre><code>chat(**kwargs)\n</code></pre> <p>Chat with model from NEAR AI hub.</p> <pre><code>query (str): User's query to model\nendpoint (str): NEAR AI HUB's url\nmodel (str): Name of a model\nprovider (str): Name of a provider\ninfo (bool): Display system info\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def chat(self, **kwargs):\n    \"\"\"Chat with model from NEAR AI hub.\n\n    Args:\n    ----\n        query (str): User's query to model\n        endpoint (str): NEAR AI HUB's url\n        model (str): Name of a model\n        provider (str): Name of a provider\n        info (bool): Display system info\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.hub import Hub\n\n    hub = Hub(CONFIG)\n    hub.chat(kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI","title":"LoginCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class LoginCLI:\n    def __call__(self, **kwargs):\n        \"\"\"Login with NEAR Mainnet account.\n\n        Args:\n        ----\n            remote (bool): Remote login allows signing message with NEAR Account on a remote machine\n            auth_url (str): Url to the auth portal\n            accountId (str): AccountId in .near-credentials folder to signMessage\n            privateKey (str): Private Key to sign a message\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n        remote = kwargs.get(\"remote\", False)\n        account_id = kwargs.get(\"accountId\", None)\n        private_key = kwargs.get(\"privateKey\", None)\n\n        if not remote and account_id and private_key:\n            generate_and_save_signature(account_id, private_key)\n        elif not remote and account_id:\n            login_with_file_credentials(account_id)\n        else:\n            auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n            login_with_near_auth(remote, auth_url)\n\n    def status(self):\n        \"\"\"Load NEAR account authorization data.\"\"\"\n        from nearai.login import print_login_status\n\n        print_login_status()\n\n    def save(self, **kwargs):\n        \"\"\"Save NEAR account authorization data.\n\n        Args:\n        ----\n            accountId (str): Near Account\n            signature (str): Signature\n            publicKey (str): Public Key used to sign\n            callbackUrl (str): Callback Url\n            nonce (str): nonce\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.login import update_auth_config\n\n        account_id = kwargs.get(\"accountId\")\n        signature = kwargs.get(\"signature\")\n        public_key = kwargs.get(\"publicKey\")\n        callback_url = kwargs.get(\"callbackUrl\")\n        nonce = kwargs.get(\"nonce\")\n\n        if account_id and signature and public_key and callback_url and nonce:\n            update_auth_config(account_id, signature, public_key, callback_url, nonce)\n        else:\n            print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Login with NEAR Mainnet account.</p> <pre><code>remote (bool): Remote login allows signing message with NEAR Account on a remote machine\nauth_url (str): Url to the auth portal\naccountId (str): AccountId in .near-credentials folder to signMessage\nprivateKey (str): Private Key to sign a message\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Login with NEAR Mainnet account.\n\n    Args:\n    ----\n        remote (bool): Remote login allows signing message with NEAR Account on a remote machine\n        auth_url (str): Url to the auth portal\n        accountId (str): AccountId in .near-credentials folder to signMessage\n        privateKey (str): Private Key to sign a message\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n    remote = kwargs.get(\"remote\", False)\n    account_id = kwargs.get(\"accountId\", None)\n    private_key = kwargs.get(\"privateKey\", None)\n\n    if not remote and account_id and private_key:\n        generate_and_save_signature(account_id, private_key)\n    elif not remote and account_id:\n        login_with_file_credentials(account_id)\n    else:\n        auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n        login_with_near_auth(remote, auth_url)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.save","title":"save","text":"<pre><code>save(**kwargs)\n</code></pre> <p>Save NEAR account authorization data.</p> <pre><code>accountId (str): Near Account\nsignature (str): Signature\npublicKey (str): Public Key used to sign\ncallbackUrl (str): Callback Url\nnonce (str): nonce\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def save(self, **kwargs):\n    \"\"\"Save NEAR account authorization data.\n\n    Args:\n    ----\n        accountId (str): Near Account\n        signature (str): Signature\n        publicKey (str): Public Key used to sign\n        callbackUrl (str): Callback Url\n        nonce (str): nonce\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.login import update_auth_config\n\n    account_id = kwargs.get(\"accountId\")\n    signature = kwargs.get(\"signature\")\n    public_key = kwargs.get(\"publicKey\")\n    callback_url = kwargs.get(\"callbackUrl\")\n    nonce = kwargs.get(\"nonce\")\n\n    if account_id and signature and public_key and callback_url and nonce:\n        update_auth_config(account_id, signature, public_key, callback_url, nonce)\n    else:\n        print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.status","title":"status","text":"<pre><code>status()\n</code></pre> <p>Load NEAR account authorization data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def status(self):\n    \"\"\"Load NEAR account authorization data.\"\"\"\n    from nearai.login import print_login_status\n\n    print_login_status()\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI","title":"LogoutCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class LogoutCLI:\n    def __call__(self, **kwargs):\n        \"\"\"Clear NEAR account auth data.\"\"\"\n        from nearai.config import load_config_file, save_config_file\n\n        config = load_config_file()\n        if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n            print(\"Auth data does not exist.\")\n        else:\n            config.pop(\"auth\", None)\n            save_config_file(config)\n            print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Clear NEAR account auth data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Clear NEAR account auth data.\"\"\"\n    from nearai.config import load_config_file, save_config_file\n\n    config = load_config_file()\n    if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n        print(\"Auth data does not exist.\")\n    else:\n        config.pop(\"auth\", None)\n        save_config_file(config)\n        print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli","title":"PermissionCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class PermissionCli:\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.client = PermissionsApi()\n\n    def grant(self, account_id: str, permission: str):\n        \"\"\"Grant permission to an account.\"\"\"\n        self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n\n    def revoke(self, account_id: str, permission: str = \"\"):\n        \"\"\"Revoke permission from an account. If permission is empty all permissions are revoked.\"\"\"\n        self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.grant","title":"grant","text":"<pre><code>grant(account_id: str, permission: str)\n</code></pre> <p>Grant permission to an account.</p> Source code in <code>nearai/cli.py</code> <pre><code>def grant(self, account_id: str, permission: str):\n    \"\"\"Grant permission to an account.\"\"\"\n    self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.revoke","title":"revoke","text":"<pre><code>revoke(account_id: str, permission: str = '')\n</code></pre> <p>Revoke permission from an account. If permission is empty all permissions are revoked.</p> Source code in <code>nearai/cli.py</code> <pre><code>def revoke(self, account_id: str, permission: str = \"\"):\n    \"\"\"Revoke permission from an account. If permission is empty all permissions are revoked.\"\"\"\n    self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli","title":"RegistryCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class RegistryCli:\n    def info(self, entry: str) -&gt; None:\n        \"\"\"Show information about an item.\"\"\"\n        entry_location = parse_location(entry)\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            print(f\"Entry {entry} not found.\")\n            return\n\n        print(metadata.model_dump_json(indent=2))\n        if metadata.category == \"model\":\n            available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n                NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n            )\n            if len(available_provider_matches) &gt; 0:\n                header = [\"provider\", \"name\"]\n\n                table = []\n                for provider, name in available_provider_matches.items():\n                    table.append(\n                        [\n                            fill(provider),\n                            fill(name),\n                        ]\n                    )\n                print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n        \"\"\"Create a metadata template.\"\"\"\n        path = Path(local_path)\n\n        metadata_path = path / \"metadata.json\"\n\n        version = path.name\n        pattern = r\"^(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-((?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\\+([0-9a-zA-Z-]+(?:\\.[0-9a-zA-Z-]+)*))?$\"  # noqa: E501\n        assert re.match(pattern, version), f\"Invalid semantic version format: {version}\"\n        name = path.parent.name\n        assert not re.match(pattern, name), f\"Invalid agent name: {name}\"\n\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": category,\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n\n            if category == \"agent\":\n                metadata[\"details\"][\"agent\"] = {}\n                metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                    \"title\": name,\n                    \"description\": description,\n                }\n                metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                    \"model\": DEFAULT_MODEL,\n                    \"model_provider\": DEFAULT_PROVIDER,\n                    \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                    \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    \"max_iterations\": 1,\n                }\n                metadata[\"details\"][\"agent\"][\"framework\"] = \"base\"\n\n            json.dump(metadata, f, indent=2)\n\n    def list(\n        self,\n        namespace: str = \"\",\n        category: str = \"\",\n        tags: str = \"\",\n        total: int = 32,\n        offset: int = 0,\n        show_all: bool = False,\n        show_latest_version: bool = True,\n        star: str = \"\",\n    ) -&gt; None:\n        \"\"\"List available items.\"\"\"\n        # Make sure tags is a comma-separated list of tags\n        tags_l = parse_tags(tags)\n        tags = \",\".join(tags_l)\n\n        entries = registry.list(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total + 1,\n            offset=offset,\n            show_all=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=star,\n        )\n\n        more_rows = len(entries) &gt; total\n        entries = entries[:total]\n\n        header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n        table = []\n        for entry in entries:\n            table.append(\n                [\n                    fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                    fill(entry.category, 20),\n                    fill(entry.description, 50),\n                    fill(\", \".join(entry.tags), 20),\n                ]\n            )\n\n        if more_rows:\n            table.append([\"...\", \"...\", \"...\", \"...\"])\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n        if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n            unregistered_common_provider_models = ProviderModels(\n                CONFIG.get_client_config()\n            ).get_unregistered_common_provider_models(registry.dict_models())\n            if len(unregistered_common_provider_models):\n                print(\n                    f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n                )\n\n    def update(self, local_path: str = \".\") -&gt; None:\n        \"\"\"Update metadata of a registry item.\"\"\"\n        path = Path(local_path)\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n        check_metadata(metadata_path)\n\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        namespace = CONFIG.auth.namespace\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=metadata.pop(\"name\"),\n                version=metadata.pop(\"version\"),\n            )\n        )\n\n        entry_metadata = EntryMetadataInput.model_validate(metadata)\n        result = registry.update(entry_location, entry_metadata)\n        print(json.dumps(result, indent=2))\n\n    def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n        \"\"\"Creates new registry items for unregistered common provider models.\"\"\"\n        provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n            registry.dict_models()\n        )\n        if len(provider_matches_list) == 0:\n            print(\"No new models to upload.\")\n            return\n\n        print(\"Going to create new registry items:\")\n        header = [\"entry\", \"description\"]\n        table = []\n        paths = []\n        for provider_matches in provider_matches_list:\n            provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n            _, model = get_provider_namespaced_model(provider_model)\n            assert model.namespace == \"\"\n            model.name = create_registry_name(model.name)\n            model.namespace = DEFAULT_NAMESPACE\n            version = \"1.0.0\"\n            description = \" &amp; \".join(provider_matches.values())\n            table.append(\n                [\n                    fill(f\"{model.namespace}/{model.name}/{version}\"),\n                    fill(description, 50),\n                ]\n            )\n\n            path = get_registry_folder() / model.namespace / model.name / version\n            path.mkdir(parents=True, exist_ok=True)\n            paths.append(path)\n            metadata_path = path / \"metadata.json\"\n            with open(metadata_path, \"w\") as f:\n                metadata: Dict[str, Any] = {\n                    \"name\": model.name,\n                    \"version\": version,\n                    \"description\": description,\n                    \"category\": \"model\",\n                    \"tags\": [],\n                    \"details\": {},\n                    \"show_entry\": True,\n                }\n                json.dump(metadata, f, indent=2)\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n        if dry_run:\n            print(\"Please verify, then repeat the command with --dry_run=False\")\n        else:\n            for path in paths:\n                self.upload(str(path))\n\n    def upload(self, local_path: str = \".\") -&gt; EntryLocation:\n        \"\"\"Upload item to the registry.\"\"\"\n        return registry.upload(Path(local_path), show_progress=True)\n\n    def download(self, entry_location: str, force: bool = False) -&gt; None:\n        \"\"\"Download item.\"\"\"\n        registry.download(entry_location, force=force, show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.download","title":"download","text":"<pre><code>download(entry_location: str, force: bool = False) -&gt; None\n</code></pre> <p>Download item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def download(self, entry_location: str, force: bool = False) -&gt; None:\n    \"\"\"Download item.\"\"\"\n    registry.download(entry_location, force=force, show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.info","title":"info","text":"<pre><code>info(entry: str) -&gt; None\n</code></pre> <p>Show information about an item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def info(self, entry: str) -&gt; None:\n    \"\"\"Show information about an item.\"\"\"\n    entry_location = parse_location(entry)\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        print(f\"Entry {entry} not found.\")\n        return\n\n    print(metadata.model_dump_json(indent=2))\n    if metadata.category == \"model\":\n        available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n            NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n        )\n        if len(available_provider_matches) &gt; 0:\n            header = [\"provider\", \"name\"]\n\n            table = []\n            for provider, name in available_provider_matches.items():\n                table.append(\n                    [\n                        fill(provider),\n                        fill(name),\n                    ]\n                )\n            print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.list","title":"list","text":"<pre><code>list(namespace: str = '', category: str = '', tags: str = '', total: int = 32, offset: int = 0, show_all: bool = False, show_latest_version: bool = True, star: str = '') -&gt; None\n</code></pre> <p>List available items.</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: str = \"\",\n    category: str = \"\",\n    tags: str = \"\",\n    total: int = 32,\n    offset: int = 0,\n    show_all: bool = False,\n    show_latest_version: bool = True,\n    star: str = \"\",\n) -&gt; None:\n    \"\"\"List available items.\"\"\"\n    # Make sure tags is a comma-separated list of tags\n    tags_l = parse_tags(tags)\n    tags = \",\".join(tags_l)\n\n    entries = registry.list(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total + 1,\n        offset=offset,\n        show_all=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=star,\n    )\n\n    more_rows = len(entries) &gt; total\n    entries = entries[:total]\n\n    header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n    table = []\n    for entry in entries:\n        table.append(\n            [\n                fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                fill(entry.category, 20),\n                fill(entry.description, 50),\n                fill(\", \".join(entry.tags), 20),\n            ]\n        )\n\n    if more_rows:\n        table.append([\"...\", \"...\", \"...\", \"...\"])\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n        unregistered_common_provider_models = ProviderModels(\n            CONFIG.get_client_config()\n        ).get_unregistered_common_provider_models(registry.dict_models())\n        if len(unregistered_common_provider_models):\n            print(\n                f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n            )\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template","title":"metadata_template","text":"<pre><code>metadata_template(local_path: str = '.', category: str = '', description: str = '')\n</code></pre> <p>Create a metadata template.</p> Source code in <code>nearai/cli.py</code> <pre><code>def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n    \"\"\"Create a metadata template.\"\"\"\n    path = Path(local_path)\n\n    metadata_path = path / \"metadata.json\"\n\n    version = path.name\n    pattern = r\"^(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-((?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\\+([0-9a-zA-Z-]+(?:\\.[0-9a-zA-Z-]+)*))?$\"  # noqa: E501\n    assert re.match(pattern, version), f\"Invalid semantic version format: {version}\"\n    name = path.parent.name\n    assert not re.match(pattern, name), f\"Invalid agent name: {name}\"\n\n    with open(metadata_path, \"w\") as f:\n        metadata: Dict[str, Any] = {\n            \"name\": name,\n            \"version\": version,\n            \"description\": description,\n            \"category\": category,\n            \"tags\": [],\n            \"details\": {},\n            \"show_entry\": True,\n        }\n\n        if category == \"agent\":\n            metadata[\"details\"][\"agent\"] = {}\n            metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                \"title\": name,\n                \"description\": description,\n            }\n            metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                \"model\": DEFAULT_MODEL,\n                \"model_provider\": DEFAULT_PROVIDER,\n                \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                \"max_iterations\": 1,\n            }\n            metadata[\"details\"][\"agent\"][\"framework\"] = \"base\"\n\n        json.dump(metadata, f, indent=2)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.update","title":"update","text":"<pre><code>update(local_path: str = '.') -&gt; None\n</code></pre> <p>Update metadata of a registry item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def update(self, local_path: str = \".\") -&gt; None:\n    \"\"\"Update metadata of a registry item.\"\"\"\n    path = Path(local_path)\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n    check_metadata(metadata_path)\n\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    namespace = CONFIG.auth.namespace\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=metadata.pop(\"name\"),\n            version=metadata.pop(\"version\"),\n        )\n    )\n\n    entry_metadata = EntryMetadataInput.model_validate(metadata)\n    result = registry.update(entry_location, entry_metadata)\n    print(json.dumps(result, indent=2))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload","title":"upload","text":"<pre><code>upload(local_path: str = '.') -&gt; EntryLocation\n</code></pre> <p>Upload item to the registry.</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload(self, local_path: str = \".\") -&gt; EntryLocation:\n    \"\"\"Upload item to the registry.\"\"\"\n    return registry.upload(Path(local_path), show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload_unregistered_common_provider_models","title":"upload_unregistered_common_provider_models","text":"<pre><code>upload_unregistered_common_provider_models(dry_run: bool = True) -&gt; None\n</code></pre> <p>Creates new registry items for unregistered common provider models.</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n    \"\"\"Creates new registry items for unregistered common provider models.\"\"\"\n    provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n        registry.dict_models()\n    )\n    if len(provider_matches_list) == 0:\n        print(\"No new models to upload.\")\n        return\n\n    print(\"Going to create new registry items:\")\n    header = [\"entry\", \"description\"]\n    table = []\n    paths = []\n    for provider_matches in provider_matches_list:\n        provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n        _, model = get_provider_namespaced_model(provider_model)\n        assert model.namespace == \"\"\n        model.name = create_registry_name(model.name)\n        model.namespace = DEFAULT_NAMESPACE\n        version = \"1.0.0\"\n        description = \" &amp; \".join(provider_matches.values())\n        table.append(\n            [\n                fill(f\"{model.namespace}/{model.name}/{version}\"),\n                fill(description, 50),\n            ]\n        )\n\n        path = get_registry_folder() / model.namespace / model.name / version\n        path.mkdir(parents=True, exist_ok=True)\n        paths.append(path)\n        metadata_path = path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": model.name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": \"model\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n            json.dump(metadata, f, indent=2)\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n    if dry_run:\n        print(\"Please verify, then repeat the command with --dry_run=False\")\n    else:\n        for path in paths:\n            self.upload(str(path))\n</code></pre>"},{"location":"api/#nearai.cli.check_update","title":"check_update","text":"<pre><code>check_update()\n</code></pre> <p>Check if there is a new version of nearai CLI available.</p> Source code in <code>nearai/cli.py</code> <pre><code>def check_update():\n    \"\"\"Check if there is a new version of nearai CLI available.\"\"\"\n    try:\n        api = DefaultApi()\n        latest = api.version_v1_version_get()\n        current = importlib.metadata.version(\"nearai\")\n\n        if latest != current:\n            print(f\"New version of nearai CLI available: {latest}. Current version: {current}\")\n            print(\"Run `pip install --upgrade nearai` to update.\")\n\n    except Exception as _:\n        pass\n</code></pre>"},{"location":"api/#nearai.config","title":"config","text":""},{"location":"api/#nearai.config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/config.py</code> <pre><code>class Config(BaseModel):\n    origin: Optional[str] = None\n    api_url: Optional[str] = \"https://api.near.ai\"\n    inference_url: str = \"http://localhost:5000/v1/\"\n    inference_api_key: str = \"n/a\"\n    scheduler_account_id: str = \"nearaischeduler.near\"\n    nearai_hub: NearAiHubConfig = NearAiHubConfig()\n    confirm_commands: bool = True\n    auth: Optional[AuthData] = None\n    num_inference_retries: int = 1\n\n    def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n        \"\"\"Update the config with the given dictionary.\"\"\"\n        dict_repr = self.model_dump()\n        keys = list(map(map_key, dict_repr.keys()))\n\n        for key in keys:\n            value = extra_config.get(key, None)\n\n            if value:\n                # This will skip empty values, even if they are set in the `extra_config`\n                dict_repr[key] = value\n\n        return Config.model_validate(dict_repr)\n\n    def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n        \"\"\"Get the value of a key in the config if it exists.\"\"\"\n        return getattr(self, key, default)\n\n    def get_client_config(self) -&gt; ClientConfig:  # noqa: D102\n        return ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n            custom_llm_provider=CONFIG.nearai_hub.custom_llm_provider,\n            default_provider=CONFIG.nearai_hub.default_provider,\n            num_inference_retries=CONFIG.num_inference_retries,\n        )\n</code></pre>"},{"location":"api/#nearai.config.Config.get","title":"get","text":"<pre><code>get(key: str, default: Optional[Any] = None) -&gt; Optional[Any]\n</code></pre> <p>Get the value of a key in the config if it exists.</p> Source code in <code>nearai/config.py</code> <pre><code>def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n    \"\"\"Get the value of a key in the config if it exists.\"\"\"\n    return getattr(self, key, default)\n</code></pre>"},{"location":"api/#nearai.config.Config.update_with","title":"update_with","text":"<pre><code>update_with(extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; Config\n</code></pre> <p>Update the config with the given dictionary.</p> Source code in <code>nearai/config.py</code> <pre><code>def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n    \"\"\"Update the config with the given dictionary.\"\"\"\n    dict_repr = self.model_dump()\n    keys = list(map(map_key, dict_repr.keys()))\n\n    for key in keys:\n        value = extra_config.get(key, None)\n\n        if value:\n            # This will skip empty values, even if they are set in the `extra_config`\n            dict_repr[key] = value\n\n    return Config.model_validate(dict_repr)\n</code></pre>"},{"location":"api/#nearai.config.NearAiHubConfig","title":"NearAiHubConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>NearAiHub Config.</p> <p>login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.</p> <p>api_key (Optional[str]): The API key to use if Near Auth is not being utilized</p> <p>base_url (Optional[str]): NEAR AI Hub url</p> <p>default_provider (Optional[str]): Default provider name</p> <p>default_model (Optional[str]): Default model name</p> <p>custom_llm_provider (Optional[str]): provider to be used by litellm proxy</p> Source code in <code>nearai/config.py</code> <pre><code>class NearAiHubConfig(BaseModel):\n    \"\"\"NearAiHub Config.\n\n    login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.\n\n    api_key (Optional[str]): The API key to use if Near Auth is not being utilized\n\n    base_url (Optional[str]): NEAR AI Hub url\n\n    default_provider (Optional[str]): Default provider name\n\n    default_model (Optional[str]): Default model name\n\n    custom_llm_provider (Optional[str]): provider to be used by litellm proxy\n    \"\"\"\n\n    base_url: str = \"https://api.near.ai/v1\"\n    default_provider: str = DEFAULT_PROVIDER\n    default_model: str = DEFAULT_PROVIDER_MODEL\n    custom_llm_provider: str = \"openai\"\n    login_with_near: Optional[bool] = True\n    api_key: Optional[str] = \"\"\n</code></pre>"},{"location":"api/#nearai.dataset","title":"dataset","text":""},{"location":"api/#nearai.dataset.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(name: str, verbose: bool = True) -&gt; Path\n</code></pre> <p>Download the dataset from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the dataset. The format should be namespace/name/version. :return: The path to the downloaded dataset</p> Source code in <code>nearai/dataset.py</code> <pre><code>def get_dataset(name: str, verbose: bool = True) -&gt; Path:\n    \"\"\"Download the dataset from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the dataset. The format should be namespace/name/version.\n    :return: The path to the downloaded dataset\n    \"\"\"\n    return registry.download(name, verbose=verbose)\n</code></pre>"},{"location":"api/#nearai.dataset.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]\n</code></pre> <p>Load a dataset from the registry.</p> Source code in <code>nearai/dataset.py</code> <pre><code>def load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]:\n    \"\"\"Load a dataset from the registry.\"\"\"\n    path = get_dataset(alias_or_name, verbose=verbose)\n    return load_from_disk(path.as_posix())\n</code></pre>"},{"location":"api/#nearai.delegation","title":"delegation","text":""},{"location":"api/#nearai.delegation.OnBehalfOf","title":"OnBehalfOf","text":"<p>Create a context manager that allows you to delegate actions to another account.</p> <pre><code>with OnBehalfOf(\"scheduler.ai\"):\n    # Upload is done on behalf of scheduler.ai\n    # If delegation permission is not granted, this will raise an exception\n    registry.upload()\n</code></pre> Source code in <code>nearai/delegation.py</code> <pre><code>class OnBehalfOf:\n    \"\"\"Create a context manager that allows you to delegate actions to another account.\n\n    ```python\n    with OnBehalfOf(\"scheduler.ai\"):\n        # Upload is done on behalf of scheduler.ai\n        # If delegation permission is not granted, this will raise an exception\n        registry.upload()\n    ```\n    \"\"\"\n\n    def __init__(self, on_behalf_of: str):\n        \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n        self.target_on_behalf_of = on_behalf_of\n        self.original_access_token = None\n\n    def __enter__(self):\n        \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n        default_client = ApiClient.get_default()\n        self.original_access_token = default_client.configuration.access_token\n\n        if not isinstance(self.original_access_token, str):\n            return\n\n        assert self.original_access_token.startswith(\"Bearer \")\n        auth = self.original_access_token[len(\"Bearer \") :]\n        auth_data = AuthData.model_validate_json(auth)\n        auth_data.on_behalf_of = self.target_on_behalf_of\n        new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n        default_client.configuration.access_token = new_access_token\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Reset the default client to the original account.\"\"\"\n        default_client = ApiClient.get_default()\n        default_client.configuration.access_token = self.original_access_token\n        self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Set the default client to the account we are acting on behalf of.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __enter__(self):\n    \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n    default_client = ApiClient.get_default()\n    self.original_access_token = default_client.configuration.access_token\n\n    if not isinstance(self.original_access_token, str):\n        return\n\n    assert self.original_access_token.startswith(\"Bearer \")\n    auth = self.original_access_token[len(\"Bearer \") :]\n    auth_data = AuthData.model_validate_json(auth)\n    auth_data.on_behalf_of = self.target_on_behalf_of\n    new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n    default_client.configuration.access_token = new_access_token\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Reset the default client to the original account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Reset the default client to the original account.\"\"\"\n    default_client = ApiClient.get_default()\n    default_client.configuration.access_token = self.original_access_token\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__init__","title":"__init__","text":"<pre><code>__init__(on_behalf_of: str)\n</code></pre> <p>Context manager that creates a scope where all actions are done on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __init__(self, on_behalf_of: str):\n    \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n    self.target_on_behalf_of = on_behalf_of\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.check_on_behalf_of","title":"check_on_behalf_of","text":"<pre><code>check_on_behalf_of()\n</code></pre> <p>Check if the request is being made on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def check_on_behalf_of():\n    \"\"\"Check if the request is being made on behalf of another account.\"\"\"\n    api = DelegationApi()\n    return api.api_client.configuration.access_token\n</code></pre>"},{"location":"api/#nearai.delegation.revoke_delegation","title":"revoke_delegation","text":"<pre><code>revoke_delegation(delegate_account_id: str)\n</code></pre> <p>Revoke delegation to a specific account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def revoke_delegation(delegate_account_id: str):\n    \"\"\"Revoke delegation to a specific account.\"\"\"\n    DelegationApi().revoke_delegation_v1_delegation_revoke_delegation_post(delegate_account_id)\n</code></pre>"},{"location":"api/#nearai.evaluation","title":"evaluation","text":""},{"location":"api/#nearai.evaluation._print_metrics_tables","title":"_print_metrics_tables","text":"<pre><code>_print_metrics_tables(rows: List[Dict[str, str]], metric_names: List[str], num_columns: int, all_key_columns: bool, metric_name_max_length: int)\n</code></pre> <p>Builds table(s) and prints them.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _print_metrics_tables(\n    rows: List[Dict[str, str]],\n    metric_names: List[str],\n    num_columns: int,\n    all_key_columns: bool,\n    metric_name_max_length: int,\n):\n    \"\"\"Builds table(s) and prints them.\"\"\"\n    # Shorten metric names\n    short_metric_names = [_shorten_metric_name(name, metric_name_max_length) for name in metric_names]\n\n    # Prepare the base header and rows\n    base_header = [\"model\", \"agent\"]\n    if all_key_columns:\n        base_header.extend([\"namespace\", \"version\", \"provider\"])\n\n    base_rows = []\n    for row in rows:\n        base_row = [fill(row.pop(\"model\", \"\")), fill(row.pop(\"agent\", \"\"))]\n        namespace = row.pop(\"namespace\", \"\")\n        version = row.pop(\"version\", \"\")\n        provider = row.pop(\"provider\", \"\")\n        if all_key_columns:\n            base_row.extend([fill(namespace), fill(version), fill(provider)])\n        base_rows.append((base_row, row))\n\n    n_metrics_per_table = max(1, num_columns - len(base_header))\n    # Split metrics into groups\n    metric_groups = list(\n        zip(\n            [\n                short_metric_names[i : i + n_metrics_per_table]\n                for i in range(0, len(short_metric_names), n_metrics_per_table)\n            ],\n            [metric_names[i : i + n_metrics_per_table] for i in range(0, len(metric_names), n_metrics_per_table)],\n        )\n    )\n\n    # Print tables\n    for short_group, full_group in metric_groups:\n        header = base_header + short_group\n        table = []\n        for base_row, row_metrics in base_rows:\n            table_row = base_row + [fill(str(row_metrics.get(metric, \"\"))) for metric in full_group]\n            table.append(table_row)\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.evaluation._shorten_metric_name","title":"_shorten_metric_name","text":"<pre><code>_shorten_metric_name(name: str, max_length: int) -&gt; str\n</code></pre> <p>Shortens metric name if needed.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _shorten_metric_name(name: str, max_length: int) -&gt; str:\n    \"\"\"Shortens metric name if needed.\"\"\"\n    if len(name) &lt;= max_length:\n        return name\n    keep = max_length - 2  # 2 dots\n    beginning = keep // 3\n    ending = keep - beginning\n    return name[:beginning] + \"..\" + name[-ending:]\n</code></pre>"},{"location":"api/#nearai.evaluation.load_benchmark_entry_info","title":"load_benchmark_entry_info","text":"<pre><code>load_benchmark_entry_info(info: str) -&gt; Any\n</code></pre> <p>Deserializes benchmark info entry from db data.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def load_benchmark_entry_info(info: str) -&gt; Any:\n    \"\"\"Deserializes benchmark info entry from db data.\"\"\"\n    first_decode = json.loads(info)\n    try:\n        second_decode = json.loads(first_decode)\n        return second_decode\n    except json.JSONDecodeError as e:\n        if \"Unterminated string\" in str(e):\n            last_brace = first_decode.rfind(\"}\")\n            if last_brace != -1:\n                try:\n                    return json.loads(first_decode[: last_brace + 1])\n                except json.JSONDecodeError as e:\n                    pass\n    return first_decode\n</code></pre>"},{"location":"api/#nearai.evaluation.print_evaluation_table","title":"print_evaluation_table","text":"<pre><code>print_evaluation_table(rows: List[Dict[str, str]], columns: List[str], important_columns: List[str], all_key_columns: bool, all_metrics: bool, num_columns: int, metric_name_max_length: int) -&gt; None\n</code></pre> <p>Prints table of evaluations.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def print_evaluation_table(\n    rows: List[Dict[str, str]],\n    columns: List[str],\n    important_columns: List[str],\n    all_key_columns: bool,\n    all_metrics: bool,\n    num_columns: int,\n    metric_name_max_length: int,\n) -&gt; None:\n    \"\"\"Prints table of evaluations.\"\"\"\n    metric_names = columns[5:] if all_metrics else important_columns[2:]\n    _print_metrics_tables(rows, metric_names, num_columns, all_key_columns, metric_name_max_length)\n</code></pre>"},{"location":"api/#nearai.evaluation.record_evaluation_metrics","title":"record_evaluation_metrics","text":"<pre><code>record_evaluation_metrics(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], prepend_evaluation_name: bool = True) -&gt; None\n</code></pre> <p>Uploads evaluation metrics into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_evaluation_metrics(\n    solver_strategy: SolverStrategy,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    prepend_evaluation_name: bool = True,\n) -&gt; None:\n    \"\"\"Uploads evaluation metrics into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    model = \"\"\n    agent = \"\"\n    version = \"\"\n    model = solver_strategy.model_name\n    agent = solver_strategy.agent_name()\n    version = solver_strategy.agent_version()\n\n    upload_evaluation(\n        evaluation_name,\n        benchmark_id,\n        data_tasks,\n        metrics if not prepend_evaluation_name else _prepend_name_to_metrics(evaluation_name, metrics),\n        model,\n        agent,\n        solver_strategy.evaluated_entry_namespace(),\n        version,\n        solver_strategy.model_provider(),\n    )\n</code></pre>"},{"location":"api/#nearai.evaluation.record_single_score_evaluation","title":"record_single_score_evaluation","text":"<pre><code>record_single_score_evaluation(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float) -&gt; None\n</code></pre> <p>Uploads single score evaluation into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_single_score_evaluation(\n    solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float\n) -&gt; None:\n    \"\"\"Uploads single score evaluation into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    record_evaluation_metrics(solver_strategy, benchmark_id, data_tasks, {evaluation_name: score}, False)\n</code></pre>"},{"location":"api/#nearai.evaluation.upload_evaluation","title":"upload_evaluation","text":"<pre><code>upload_evaluation(evaluation_name: str, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], model: str = '', agent: str = '', namespace: str = '', version: str = '', provider: str = '') -&gt; None\n</code></pre> <p>Uploads evaluation into registry.</p> <p><code>evaluation_name</code>: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\". <code>metrics</code>: metrics from evaluation. <code>model</code>: model that was used. <code>agent</code>: agent that was evaluated, in any. <code>namespace</code>: namespace of evaluated agent or evaluated model. <code>version</code>: version of evaluated agent or evaluated model. <code>provider</code>: provider of model used; pass <code>local</code> if running locally.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def upload_evaluation(\n    evaluation_name: str,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    model: str = \"\",\n    agent: str = \"\",\n    namespace: str = \"\",\n    version: str = \"\",\n    provider: str = \"\",\n) -&gt; None:\n    \"\"\"Uploads evaluation into registry.\n\n    `evaluation_name`: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\".\n    `metrics`: metrics from evaluation.\n    `model`: model that was used.\n    `agent`: agent that was evaluated, in any.\n    `namespace`: namespace of evaluated agent or evaluated model.\n    `version`: version of evaluated agent or evaluated model.\n    `provider`: provider of model used; pass `local` if running locally.\n    \"\"\"\n    key = f\"evaluation_{evaluation_name}\"\n    metrics[EVALUATED_ENTRY_METADATA] = {}\n    if agent != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"agent\"] = agent\n        key += f\"_agent_{agent}\"\n    if model != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"model\"] = model\n        key += f\"_model_{model}\"\n    if namespace != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"namespace\"] = namespace\n        key += f\"_namespace_{namespace}\"\n    if version != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"version\"] = version\n        key += f\"_version_{version}\"\n    if provider != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"provider\"] = provider\n        key += f\"_provider_{provider}\"\n\n    entry_path = get_registry_folder() / key\n    # Create folder entry_path if not present\n    entry_path.mkdir(parents=True, exist_ok=True)\n    # Write file metrics.json inside\n    metrics_file = entry_path / \"metrics.json\"\n    with metrics_file.open(\"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # Get solutions from cache in benchmark.py\n    cache = BenchmarkApi().get_benchmark_result_v1_benchmark_get_result_get(benchmark_id)\n    solutions = []\n    for result in cache:\n        try:\n            solution = {\n                \"datum\": data_tasks[result.index],\n                \"status\": result.solved,\n                \"info\": load_benchmark_entry_info(result.info) if result.info else {},\n            }\n            solutions.append(solution)\n        except (AttributeError, json.JSONDecodeError, TypeError) as e:\n            print(f\"Exception while creating solutions data: {str(e)}.\")\n            # Skip entries that can't be properly formatted\n            continue\n\n    # Write solutions file\n    solutions_file = entry_path / \"solutions.json\"\n    with solutions_file.open(\"w\") as f:\n        json.dump(solutions, f, indent=2)\n\n    metadata_path = entry_path / \"metadata.json\"\n    # TODO(#273): Currently that will not update existing evaluation.\n    with open(metadata_path, \"w\") as f:\n        json.dump(\n            {\n                \"name\": key,\n                \"version\": \"0.1.0\",\n                \"description\": \"\",\n                \"category\": \"evaluation\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            },\n            f,\n            indent=2,\n        )\n\n    registry.upload(Path(entry_path), show_progress=True)\n</code></pre>"},{"location":"api/#nearai.finetune","title":"finetune","text":""},{"location":"api/#nearai.finetune.FinetuneCli","title":"FinetuneCli","text":"Source code in <code>nearai/finetune/__init__.py</code> <pre><code>class FinetuneCli:\n    def start(\n        self,\n        model: str,\n        tokenizer: str,\n        dataset: str,\n        num_procs: int,\n        format: str,\n        upload_checkpoint: bool = True,\n        num_nodes: int = 1,\n        job_id: Optional[str] = None,\n        checkpoint: Optional[str] = None,\n        **dataset_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Start a finetuning job on the current node.\n\n        Args:\n        ----\n            model: Name of a model in the registry. Base model to finetune.\n            tokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\n            dataset: Name of a dataset in the registry.\n            num_procs: Number of GPUs to use for training\n            format: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\n            upload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\n            num_nodes: Number of nodes to use for training. Default is 1.\n            job_id: Unique identifier for the job. Default is None.\n            checkpoint: Name of the model checkpoint to start from. Default is None.\n            dataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n\n        \"\"\"  # noqa: E501\n        from nearai.dataset import get_dataset\n\n        assert num_nodes &gt;= 1\n\n        # Prepare job id folder\n        if job_id is None:\n            job_id = \"job\"\n        job_id = f\"{job_id}-{timestamp()}-{randint(10**8, 10**9 - 1)}\"\n        job_folder = DATA_FOLDER / \"finetune\" / job_id\n        job_folder.mkdir(parents=True, exist_ok=True)\n\n        # Either use the provided config file template or load one predefined one\n        if Path(format).exists():\n            config_template_path = Path(format)\n        else:\n            configs = ETC_FOLDER / \"finetune\"\n            config_template_path = configs / f\"{format}.yml\"\n\n        if not config_template_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_template_path}\")\n\n        CONFIG_TEMPLATE = config_template_path.read_text()  # noqa: N806\n\n        # Download model\n        model_path = get_model(model)\n\n        # Download tokenizer\n        tokenizer_path = registry.download(tokenizer) / \"tokenizer.model\"\n        assert tokenizer_path.exists(), f\"tokenizer.model not found in {tokenizer_path}\"\n\n        # Download checkpoint if any\n        checkpoint_path = get_model(checkpoint) if checkpoint else \"null\"\n        resume_checkpoint = checkpoint_path != \"null\"\n\n        # Download dataset\n        dataset_path = get_dataset(dataset)\n\n        # Set up output directories\n        checkpoint_output_dir = job_folder / \"checkpoint_output\"\n        logging_output_dir = job_folder / \"logs\"\n        logging_output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Prepare config file\n        dataset_args_dict = deepcopy(dataset_kwargs)\n\n        dataset_args_dict[\"_component_\"] = dataset_args_dict.pop(\"method\")\n        dataset_args_dict[\"source\"] = str(dataset_path.absolute())\n        dataset_args = \"\\n\".join(f\"  {key}: {value}\" for key, value in dataset_args_dict.items())\n\n        config = job_folder / \"config.yaml\"\n        with open(config, \"w\") as f:\n            f.write(\n                CONFIG_TEMPLATE.format(\n                    TOKENIZER=str(tokenizer_path),\n                    MODEL=str(model_path),\n                    RECIPE_CHECKPOINT=checkpoint_path,\n                    RESUME_FROM_CHECKPOINT=resume_checkpoint,\n                    CHECKPOINT_OUTPUT_DIR=str(checkpoint_output_dir),\n                    DATASET_ARGS=dataset_args,\n                    LOGGING_OUTPUT_DIR=str(logging_output_dir),\n                )\n            )\n\n        # Spawn background thread to read logs and push to database\n        threading.Thread(target=find_new_logs_background, args=(logging_output_dir, job_id)).start()\n\n        print(\"Starting job at\", job_folder)\n        if num_nodes == 1:\n            run(\n                [\n                    \"tune\",\n                    \"run\",\n                    \"--nproc_per_node\",\n                    str(num_procs),\n                    \"lora_finetune_distributed\",\n                    \"--config\",\n                    str(config),\n                ]\n            )\n        else:\n            # Fetch rank and master addr from environment variables\n            raise NotImplementedError()\n\n        global BACKGROUND_PROCESS\n        BACKGROUND_PROCESS = False\n\n        if upload_checkpoint:\n            registry.upload(\n                job_folder,\n                EntryMetadata.from_dict(\n                    {\n                        \"name\": f\"finetune-{job_id}\",\n                        \"version\": \"0.0.1\",\n                        \"description\": f\"Finetuned checkpoint from base mode {model} using dataset {dataset}\",\n                        \"category\": \"finetune\",\n                        \"tags\": [\"finetune\", f\"base-model-{model}\", f\"base-dataset-{dataset}\"],\n                        \"details\": dict(\n                            model=model,\n                            tokenizer=tokenizer,\n                            dataset=dataset,\n                            num_procs=num_procs,\n                            format=format,\n                            num_nodes=num_nodes,\n                            checkpoint=checkpoint,\n                            **dataset_kwargs,\n                        ),\n                        \"show_entry\": True,\n                    }\n                ),\n                show_progress=True,\n            )\n\n    def inspect(self, job_id: str) -&gt; None:  # noqa: D102\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/#nearai.finetune.FinetuneCli.start","title":"start","text":"<pre><code>start(model: str, tokenizer: str, dataset: str, num_procs: int, format: str, upload_checkpoint: bool = True, num_nodes: int = 1, job_id: Optional[str] = None, checkpoint: Optional[str] = None, **dataset_kwargs: Any) -&gt; None\n</code></pre> <p>Start a finetuning job on the current node.</p> <pre><code>model: Name of a model in the registry. Base model to finetune.\ntokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\ndataset: Name of a dataset in the registry.\nnum_procs: Number of GPUs to use for training\nformat: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\nupload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\nnum_nodes: Number of nodes to use for training. Default is 1.\njob_id: Unique identifier for the job. Default is None.\ncheckpoint: Name of the model checkpoint to start from. Default is None.\ndataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n</code></pre> Source code in <code>nearai/finetune/__init__.py</code> <pre><code>def start(\n    self,\n    model: str,\n    tokenizer: str,\n    dataset: str,\n    num_procs: int,\n    format: str,\n    upload_checkpoint: bool = True,\n    num_nodes: int = 1,\n    job_id: Optional[str] = None,\n    checkpoint: Optional[str] = None,\n    **dataset_kwargs: Any,\n) -&gt; None:\n    \"\"\"Start a finetuning job on the current node.\n\n    Args:\n    ----\n        model: Name of a model in the registry. Base model to finetune.\n        tokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\n        dataset: Name of a dataset in the registry.\n        num_procs: Number of GPUs to use for training\n        format: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\n        upload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\n        num_nodes: Number of nodes to use for training. Default is 1.\n        job_id: Unique identifier for the job. Default is None.\n        checkpoint: Name of the model checkpoint to start from. Default is None.\n        dataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n\n    \"\"\"  # noqa: E501\n    from nearai.dataset import get_dataset\n\n    assert num_nodes &gt;= 1\n\n    # Prepare job id folder\n    if job_id is None:\n        job_id = \"job\"\n    job_id = f\"{job_id}-{timestamp()}-{randint(10**8, 10**9 - 1)}\"\n    job_folder = DATA_FOLDER / \"finetune\" / job_id\n    job_folder.mkdir(parents=True, exist_ok=True)\n\n    # Either use the provided config file template or load one predefined one\n    if Path(format).exists():\n        config_template_path = Path(format)\n    else:\n        configs = ETC_FOLDER / \"finetune\"\n        config_template_path = configs / f\"{format}.yml\"\n\n    if not config_template_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {config_template_path}\")\n\n    CONFIG_TEMPLATE = config_template_path.read_text()  # noqa: N806\n\n    # Download model\n    model_path = get_model(model)\n\n    # Download tokenizer\n    tokenizer_path = registry.download(tokenizer) / \"tokenizer.model\"\n    assert tokenizer_path.exists(), f\"tokenizer.model not found in {tokenizer_path}\"\n\n    # Download checkpoint if any\n    checkpoint_path = get_model(checkpoint) if checkpoint else \"null\"\n    resume_checkpoint = checkpoint_path != \"null\"\n\n    # Download dataset\n    dataset_path = get_dataset(dataset)\n\n    # Set up output directories\n    checkpoint_output_dir = job_folder / \"checkpoint_output\"\n    logging_output_dir = job_folder / \"logs\"\n    logging_output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Prepare config file\n    dataset_args_dict = deepcopy(dataset_kwargs)\n\n    dataset_args_dict[\"_component_\"] = dataset_args_dict.pop(\"method\")\n    dataset_args_dict[\"source\"] = str(dataset_path.absolute())\n    dataset_args = \"\\n\".join(f\"  {key}: {value}\" for key, value in dataset_args_dict.items())\n\n    config = job_folder / \"config.yaml\"\n    with open(config, \"w\") as f:\n        f.write(\n            CONFIG_TEMPLATE.format(\n                TOKENIZER=str(tokenizer_path),\n                MODEL=str(model_path),\n                RECIPE_CHECKPOINT=checkpoint_path,\n                RESUME_FROM_CHECKPOINT=resume_checkpoint,\n                CHECKPOINT_OUTPUT_DIR=str(checkpoint_output_dir),\n                DATASET_ARGS=dataset_args,\n                LOGGING_OUTPUT_DIR=str(logging_output_dir),\n            )\n        )\n\n    # Spawn background thread to read logs and push to database\n    threading.Thread(target=find_new_logs_background, args=(logging_output_dir, job_id)).start()\n\n    print(\"Starting job at\", job_folder)\n    if num_nodes == 1:\n        run(\n            [\n                \"tune\",\n                \"run\",\n                \"--nproc_per_node\",\n                str(num_procs),\n                \"lora_finetune_distributed\",\n                \"--config\",\n                str(config),\n            ]\n        )\n    else:\n        # Fetch rank and master addr from environment variables\n        raise NotImplementedError()\n\n    global BACKGROUND_PROCESS\n    BACKGROUND_PROCESS = False\n\n    if upload_checkpoint:\n        registry.upload(\n            job_folder,\n            EntryMetadata.from_dict(\n                {\n                    \"name\": f\"finetune-{job_id}\",\n                    \"version\": \"0.0.1\",\n                    \"description\": f\"Finetuned checkpoint from base mode {model} using dataset {dataset}\",\n                    \"category\": \"finetune\",\n                    \"tags\": [\"finetune\", f\"base-model-{model}\", f\"base-dataset-{dataset}\"],\n                    \"details\": dict(\n                        model=model,\n                        tokenizer=tokenizer,\n                        dataset=dataset,\n                        num_procs=num_procs,\n                        format=format,\n                        num_nodes=num_nodes,\n                        checkpoint=checkpoint,\n                        **dataset_kwargs,\n                    ),\n                    \"show_entry\": True,\n                }\n            ),\n            show_progress=True,\n        )\n</code></pre>"},{"location":"api/#nearai.finetune.parse_line","title":"parse_line","text":"<pre><code>parse_line(line: str) -&gt; Tuple[int, dict[str, float]]\n</code></pre> <p>Example of line to be parsed.</p> <p>Step 33 | loss:1.5400923490524292 lr:9.9e-05 tokens_per_second_per_gpu:101.22285588141214</p> Source code in <code>nearai/finetune/__init__.py</code> <pre><code>def parse_line(line: str) -&gt; Tuple[int, dict[str, float]]:\n    \"\"\"Example of line to be parsed.\n\n    Step 33 | loss:1.5400923490524292 lr:9.9e-05 tokens_per_second_per_gpu:101.22285588141214\n    \"\"\"\n    step_raw, metrics_raw = map(str.strip, line.strip(\" \\n\").split(\"|\"))\n    step = int(step_raw.split(\" \")[-1])\n    metrics = {metric[0]: float(metric[1]) for metric in map(lambda metric: metric.split(\":\"), metrics_raw.split(\" \"))}\n    return step, metrics\n</code></pre>"},{"location":"api/#nearai.finetune.text_completion","title":"text_completion","text":""},{"location":"api/#nearai.finetune.text_completion.TextCompletionDataset","title":"TextCompletionDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Freeform dataset for any unstructured text corpus. Quickly load any dataset from Hugging Face or local disk and tokenize it for your model.</p> <pre><code>tokenizer (BaseTokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.\nsource (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``\n    (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)\ncolumn (str): name of column in the sample that contains the text data. This is typically required\n    for Hugging Face datasets or tabular data. For local datasets with a single column, use the default \"text\",\n    which is what is assigned by Hugging Face datasets when loaded into memory. Default is \"text\".\nmax_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.\n    Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory\n    and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.\n**load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.\n</code></pre> Source code in <code>nearai/finetune/text_completion.py</code> <pre><code>class TextCompletionDataset(Dataset):\n    \"\"\"Freeform dataset for any unstructured text corpus. Quickly load any dataset from Hugging Face or local disk and tokenize it for your model.\n\n    Args:\n    ----\n        tokenizer (BaseTokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.\n        source (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``\n            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)\n        column (str): name of column in the sample that contains the text data. This is typically required\n            for Hugging Face datasets or tabular data. For local datasets with a single column, use the default \"text\",\n            which is what is assigned by Hugging Face datasets when loaded into memory. Default is \"text\".\n        max_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.\n            Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory\n            and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.\n        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.\n\n    \"\"\"  # noqa: E501\n\n    def __init__(  # noqa: D107\n        self,\n        tokenizer: BaseTokenizer,\n        source: str,\n        column: str = \"text\",\n        split: Optional[str] = None,\n        max_seq_len: Optional[int] = None,\n        **load_dataset_kwargs: Dict[str, Any],\n    ) -&gt; None:\n        self._tokenizer = tokenizer\n        self._data = load_from_disk(source, **load_dataset_kwargs)\n        if split is not None:\n            self._data = self._data[split]\n        self.max_seq_len = max_seq_len\n        self._column = column\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return len(self._data)\n\n    def __getitem__(self, index: int) -&gt; Dict[str, List[int]]:  # noqa: D105\n        sample = self._data[index]\n        return self._prepare_sample(sample)\n\n    def _prepare_sample(self, sample: Mapping[str, Any]) -&gt; Dict[str, List[int]]:\n        prompt = sample[self._column]\n        tokens = self._tokenizer.encode(text=prompt, add_bos=True, add_eos=True)\n\n        # Truncate if needed, but don't coerce EOS id\n        if self.max_seq_len is not None:\n            tokens = truncate(tokens, self.max_seq_len - 1)\n\n        # No need to offset labels by 1 - happens in the recipe\n        labels = tokens.copy()\n\n        return {\"tokens\": tokens, \"labels\": labels}\n</code></pre>"},{"location":"api/#nearai.finetune.text_completion.truncate","title":"truncate","text":"<pre><code>truncate(tokens: List[Any], max_seq_len: int, eos_id: Optional[Any] = None) -&gt; List[Any]\n</code></pre> <p>Truncate a list of tokens to a maximum length. If eos_id is provided, the last token will be replaced with eos_id.</p> <pre><code>tokens (List[Any]): list of tokens to truncate\nmax_seq_len (int): maximum length of the list\neos_id (Optional[Any]): token to replace the last token with. If None, the\n    last token will not be replaced. Default is None.\n</code></pre> <pre><code>List[Any]: truncated list of tokens\n</code></pre> Source code in <code>nearai/finetune/text_completion.py</code> <pre><code>def truncate(\n    tokens: List[Any],\n    max_seq_len: int,\n    eos_id: Optional[Any] = None,\n) -&gt; List[Any]:\n    \"\"\"Truncate a list of tokens to a maximum length. If eos_id is provided, the last token will be replaced with eos_id.\n\n    Args:\n    ----\n        tokens (List[Any]): list of tokens to truncate\n        max_seq_len (int): maximum length of the list\n        eos_id (Optional[Any]): token to replace the last token with. If None, the\n            last token will not be replaced. Default is None.\n\n    Returns:\n    -------\n        List[Any]: truncated list of tokens\n\n    \"\"\"  # noqa: E501\n    tokens_truncated = tokens[:max_seq_len]\n    if eos_id is not None and tokens_truncated[-1] != eos_id:\n        tokens_truncated[-1] = eos_id\n    return tokens_truncated\n</code></pre>"},{"location":"api/#nearai.hub","title":"hub","text":""},{"location":"api/#nearai.hub.Hub","title":"Hub","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/hub.py</code> <pre><code>class Hub(object):\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n        self.info = None\n        self.provider = None\n        self.model = None\n        self.endpoint = None\n        self.query = None\n        self._config = config\n\n    def parse_hub_chat_params(self, kwargs):\n        \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        self.query = kwargs.get(\"query\")\n        self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n        self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n        self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n        self.info = kwargs.get(\"info\", False)\n\n    def chat(self, kwargs):\n        \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n        try:\n            self.parse_hub_chat_params(kwargs)\n\n            if not self.query:\n                return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n            if self._config.nearai_hub is None:\n                self._config.nearai_hub = NearAiHubConfig()\n\n            data = {\n                \"max_tokens\": 256,\n                \"temperature\": 1,\n                \"frequency_penalty\": 0,\n                \"n\": 1,\n                \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n                \"model\": self.model,\n            }\n\n            auth = self._config.auth\n\n            if self._config.nearai_hub.login_with_near:\n                bearer_token = auth.generate_bearer_token()\n                headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n                data[\"provider\"] = self.provider\n            elif self._config.nearai_hub.api_key:\n                headers = {\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n                }\n            else:\n                return print(\"Illegal NEAR AI Hub Config\")\n\n            if self.info:\n                print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n            response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n            completion = response.json()\n\n            print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initializes the Hub class with the given configuration.</p> Source code in <code>nearai/hub.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n    self.info = None\n    self.provider = None\n    self.model = None\n    self.endpoint = None\n    self.query = None\n    self._config = config\n</code></pre>"},{"location":"api/#nearai.hub.Hub.chat","title":"chat","text":"<pre><code>chat(kwargs)\n</code></pre> <p>Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.</p> Source code in <code>nearai/hub.py</code> <pre><code>def chat(self, kwargs):\n    \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n    try:\n        self.parse_hub_chat_params(kwargs)\n\n        if not self.query:\n            return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        data = {\n            \"max_tokens\": 256,\n            \"temperature\": 1,\n            \"frequency_penalty\": 0,\n            \"n\": 1,\n            \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n            \"model\": self.model,\n        }\n\n        auth = self._config.auth\n\n        if self._config.nearai_hub.login_with_near:\n            bearer_token = auth.generate_bearer_token()\n            headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n            data[\"provider\"] = self.provider\n        elif self._config.nearai_hub.api_key:\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n            }\n        else:\n            return print(\"Illegal NEAR AI Hub Config\")\n\n        if self.info:\n            print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n        response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n        completion = response.json()\n\n        print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.parse_hub_chat_params","title":"parse_hub_chat_params","text":"<pre><code>parse_hub_chat_params(kwargs)\n</code></pre> <p>Parses and sets instance attributes from the given keyword arguments, using default values if needed.</p> Source code in <code>nearai/hub.py</code> <pre><code>def parse_hub_chat_params(self, kwargs):\n    \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n    if self._config.nearai_hub is None:\n        self._config.nearai_hub = NearAiHubConfig()\n\n    self.query = kwargs.get(\"query\")\n    self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n    self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n    self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n    self.info = kwargs.get(\"info\", False)\n</code></pre>"},{"location":"api/#nearai.lib","title":"lib","text":""},{"location":"api/#nearai.lib.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.login","title":"login","text":""},{"location":"api/#nearai.login.AuthHandler","title":"AuthHandler","text":"<p>               Bases: <code>SimpleHTTPRequestHandler</code></p> Source code in <code>nearai/login.py</code> <pre><code>class AuthHandler(http.server.SimpleHTTPRequestHandler):\n    def log_message(self, format, *args):\n        \"\"\"Webserver logging method.\"\"\"\n        pass  # Override to suppress logging\n\n    def do_GET(self):  # noqa: N802\n        \"\"\"Webserver GET method.\"\"\"\n        global NONCE, PORT\n\n        script_path = Path(__file__).resolve()\n        assets_folder = script_path.parent / \"assets\"\n\n        if self.path.startswith(\"/capture\"):\n            with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n        if self.path.startswith(\"/auth\"):\n            parsed_url = urlparse.urlparse(self.path)\n            fragment = parsed_url.query\n            params = urlparse.parse_qs(fragment)\n\n            required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n            if all(param in params for param in required_params):\n                update_auth_config(\n                    params[\"accountId\"][0],\n                    params[\"signature\"][0],\n                    params[\"publicKey\"][0],\n                    callback_url=generate_callback_url(PORT),\n                    nonce=NONCE,\n                )\n            else:\n                print(\"Required parameters not found\")\n\n            with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n            # Give the server some time to read the response before shutting it down\n            def shutdown_server():\n                global httpd\n                time.sleep(2)  # Wait 2 seconds before shutting down\n                if httpd:\n                    httpd.shutdown()\n\n            threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.do_GET","title":"do_GET","text":"<pre><code>do_GET()\n</code></pre> <p>Webserver GET method.</p> Source code in <code>nearai/login.py</code> <pre><code>def do_GET(self):  # noqa: N802\n    \"\"\"Webserver GET method.\"\"\"\n    global NONCE, PORT\n\n    script_path = Path(__file__).resolve()\n    assets_folder = script_path.parent / \"assets\"\n\n    if self.path.startswith(\"/capture\"):\n        with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n    if self.path.startswith(\"/auth\"):\n        parsed_url = urlparse.urlparse(self.path)\n        fragment = parsed_url.query\n        params = urlparse.parse_qs(fragment)\n\n        required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n        if all(param in params for param in required_params):\n            update_auth_config(\n                params[\"accountId\"][0],\n                params[\"signature\"][0],\n                params[\"publicKey\"][0],\n                callback_url=generate_callback_url(PORT),\n                nonce=NONCE,\n            )\n        else:\n            print(\"Required parameters not found\")\n\n        with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n        # Give the server some time to read the response before shutting it down\n        def shutdown_server():\n            global httpd\n            time.sleep(2)  # Wait 2 seconds before shutting down\n            if httpd:\n                httpd.shutdown()\n\n        threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.log_message","title":"log_message","text":"<pre><code>log_message(format, *args)\n</code></pre> <p>Webserver logging method.</p> Source code in <code>nearai/login.py</code> <pre><code>def log_message(self, format, *args):\n    \"\"\"Webserver logging method.\"\"\"\n    pass  # Override to suppress logging\n</code></pre>"},{"location":"api/#nearai.login.find_open_port","title":"find_open_port","text":"<pre><code>find_open_port() -&gt; int\n</code></pre> <p>Finds and returns an open port number by binding to a free port on the local machine.</p> Source code in <code>nearai/login.py</code> <pre><code>def find_open_port() -&gt; int:\n    \"\"\"Finds and returns an open port number by binding to a free port on the local machine.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"\", 0))\n        return s.getsockname()[1]\n</code></pre>"},{"location":"api/#nearai.login.generate_and_save_signature","title":"generate_and_save_signature","text":"<pre><code>generate_and_save_signature(account_id, private_key)\n</code></pre> <p>Generates a signature for the given account ID and private key, then updates the auth configuration.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_and_save_signature(account_id, private_key):\n    \"\"\"Generates a signature for the given account ID and private key, then updates the auth configuration.\"\"\"\n    nonce = generate_nonce()\n    payload = near.Payload(MESSAGE, nonce, RECIPIENT, None)\n\n    signature, public_key = near.create_signature(private_key, payload)\n\n    if update_auth_config(account_id, signature, public_key, None, nonce):\n        print_login_status()\n</code></pre>"},{"location":"api/#nearai.login.generate_callback_url","title":"generate_callback_url","text":"<pre><code>generate_callback_url(port)\n</code></pre> <p>Generates a callback URL using the specified port number.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_callback_url(port):\n    \"\"\"Generates a callback URL using the specified port number.\"\"\"\n    return f\"http://localhost:{port}/capture\"\n</code></pre>"},{"location":"api/#nearai.login.generate_nonce","title":"generate_nonce","text":"<pre><code>generate_nonce()\n</code></pre> <p>Generates a nonce based on the current time in milliseconds.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_nonce():\n    \"\"\"Generates a nonce based on the current time in milliseconds.\"\"\"\n    return str(int(time.time() * 1000))\n</code></pre>"},{"location":"api/#nearai.login.login_with_file_credentials","title":"login_with_file_credentials","text":"<pre><code>login_with_file_credentials(account_id)\n</code></pre> <p>Logs in using credentials from a file for the specified account ID, generating and saving a signature.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_file_credentials(account_id):\n    \"\"\"Logs in using credentials from a file for the specified account ID, generating and saving a signature.\"\"\"\n    file_path = os.path.expanduser(os.path.join(\"~/.near-credentials/\", \"mainnet\", f\"{account_id}.json\"))\n\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as file:\n            content = file.read()\n            account_data = json.loads(content)\n            private_key = account_data.get(\"private_key\", None)\n            if not private_key:\n                return print(f\"Private key is missing for {account_id} on mainnet\")\n            generate_and_save_signature(account_id, account_data[\"private_key\"])\n\n    else:\n        return print(f\"Account data is missing for {account_id}\")\n</code></pre>"},{"location":"api/#nearai.login.login_with_near_auth","title":"login_with_near_auth","text":"<pre><code>login_with_near_auth(remote, auth_url)\n</code></pre> <p>Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_near_auth(remote, auth_url):\n    \"\"\"Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.\"\"\"  # noqa: E501\n    global NONCE, PORT\n    NONCE = generate_nonce()\n\n    params = {\n        \"message\": MESSAGE,\n        \"nonce\": NONCE,\n        \"recipient\": RECIPIENT,\n    }\n\n    if not remote:\n        PORT = find_open_port()\n\n        global httpd\n        with socketserver.TCPServer((\"\", PORT), AuthHandler) as httpd:\n            params[\"callbackUrl\"] = f\"http://localhost:{PORT}/capture\"\n\n            encoded_params = urlparse.urlencode(params)\n\n            print_url_message(f\"{auth_url}?{encoded_params}\")\n\n            httpd.serve_forever()\n\n    else:\n        encoded_params = urlparse.urlencode(params)\n\n        print_url_message(f\"{auth_url}?{encoded_params}\")\n        print(\"After visiting the URL, follow the instructions to save your auth signature\")\n</code></pre>"},{"location":"api/#nearai.login.print_login_status","title":"print_login_status","text":"<pre><code>print_login_status()\n</code></pre> <p>Prints the current authentication status if available in the config file.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_login_status():\n    \"\"\"Prints the current authentication status if available in the config file.\"\"\"\n    config = load_config_file()\n    if config.get(\"auth\") and config[\"auth\"].get(\"account_id\"):\n        print(f'Auth data for: {config[\"auth\"][\"account_id\"]}')\n        print(f'signature: {config[\"auth\"][\"signature\"]}')\n        print(f'public_key: {config[\"auth\"][\"public_key\"]}')\n        print(f'nonce: {config[\"auth\"][\"nonce\"]}')\n        print(f'message: {config[\"auth\"][\"message\"]}')\n        print(f'recipient: {config[\"auth\"][\"recipient\"]}')\n    else:\n        print(\"Near auth details not found\")\n</code></pre>"},{"location":"api/#nearai.login.print_url_message","title":"print_url_message","text":"<pre><code>print_url_message(url)\n</code></pre> <p>Prints a message instructing the user to visit the given URL to complete the login process.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_url_message(url):\n    \"\"\"Prints a message instructing the user to visit the given URL to complete the login process.\"\"\"\n    print(f\"Please visit the following URL to complete the login process: {url}\")\n</code></pre>"},{"location":"api/#nearai.login.update_auth_config","title":"update_auth_config","text":"<pre><code>update_auth_config(account_id, signature, public_key, callback_url, nonce)\n</code></pre> <p>Update authentication configuration if the provided signature is valid.</p> Source code in <code>nearai/login.py</code> <pre><code>def update_auth_config(account_id, signature, public_key, callback_url, nonce):\n    \"\"\"Update authentication configuration if the provided signature is valid.\"\"\"\n    if near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        MESSAGE,\n        nonce,\n        RECIPIENT,\n        callback_url,\n    ):\n        config = load_config_file()\n\n        auth = AuthData.model_validate(\n            {\n                \"account_id\": account_id,\n                \"signature\": signature,\n                \"public_key\": public_key,\n                \"callback_url\": callback_url,\n                \"nonce\": nonce,\n                \"recipient\": RECIPIENT,\n                \"message\": MESSAGE,\n            }\n        )\n\n        config[\"auth\"] = auth.model_dump()\n        save_config_file(config)\n\n        print(f\"Auth data has been successfully saved! You are now logged in with account ID: {account_id}\")\n        return True\n    else:\n        print(\"Signature verification failed. Abort\")\n        return False\n</code></pre>"},{"location":"api/#nearai.model","title":"model","text":""},{"location":"api/#nearai.model.get_model","title":"get_model","text":"<pre><code>get_model(name: str) -&gt; Path\n</code></pre> <p>Download the model from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the model. The format should be namespace/name/version. :return: The path to the downloaded model</p> Source code in <code>nearai/model.py</code> <pre><code>def get_model(name: str) -&gt; Path:\n    \"\"\"Download the model from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the model. The format should be namespace/name/version.\n    :return: The path to the downloaded model\n    \"\"\"\n    return registry.download(name)\n</code></pre>"},{"location":"api/#nearai.registry","title":"registry","text":""},{"location":"api/#nearai.registry.Registry","title":"Registry","text":"Source code in <code>nearai/registry.py</code> <pre><code>class Registry:\n    def __init__(self):\n        \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n        self.download_folder = DATA_FOLDER / \"registry\"\n        self.api = RegistryApi()\n\n        if not self.download_folder.exists():\n            self.download_folder.mkdir(parents=True, exist_ok=True)\n\n    def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n        \"\"\"Update metadata of a entry in the registry.\"\"\"\n        result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n            BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n        )\n        return result\n\n    def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n        \"\"\"Get metadata of a entry in the registry.\"\"\"\n        try:\n            return self.api.download_metadata_v1_registry_download_metadata_post(\n                BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n            )\n        except NotFoundException:\n            return None\n\n    def upload_file(self, entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool:\n        \"\"\"Upload a file to the registry.\"\"\"\n        with open(local_path, \"rb\") as file:\n            data = file.read()\n\n            try:\n                self.api.upload_file_v1_registry_upload_file_post(\n                    path=str(path),\n                    file=data,\n                    namespace=entry_location.namespace,\n                    name=entry_location.name,\n                    version=entry_location.version,\n                )\n                return True\n            except BadRequestException as e:\n                if isinstance(e.body, str) and \"already exists\" in e.body:\n                    return False\n\n                raise e\n\n    def download_file(self, entry_location: EntryLocation, path: Path, local_path: Path):\n        \"\"\"Download a file from the registry.\"\"\"\n        result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n            BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n                dict(\n                    entry_location=entry_location,\n                    path=str(path),\n                )\n            )\n        )\n\n        local_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(local_path, \"wb\") as f:\n            copyfileobj(result, f)\n\n    def download(\n        self,\n        entry_location: Union[str, EntryLocation],\n        force: bool = False,\n        show_progress: bool = False,\n        verbose: bool = True,\n    ) -&gt; Path:\n        \"\"\"Download entry from the registry locally.\"\"\"\n        if isinstance(entry_location, str):\n            entry_location = parse_location(entry_location)\n\n        download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n        if download_path.exists():\n            if not force:\n                if verbose:\n                    print(\n                        f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                    )\n                return download_path\n\n        files = registry.list_files(entry_location)\n\n        download_path.mkdir(parents=True, exist_ok=True)\n\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            raise ValueError(f\"Entry {entry_location} not found.\")\n\n        metadata_path = download_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json(indent=2))\n\n        for file in (pbar := tqdm(files, disable=not show_progress)):\n            pbar.set_description(file)\n            registry.download_file(entry_location, file, download_path / file)\n\n        return download_path\n\n    def upload(\n        self,\n        local_path: Path,\n        metadata: Optional[EntryMetadata] = None,\n        show_progress: bool = False,\n    ) -&gt; EntryLocation:\n        \"\"\"Upload entry to the registry.\n\n        If metadata is provided it will overwrite the metadata in the directory,\n        otherwise it will use the metadata.json found on the root of the directory.\n        \"\"\"\n        path = Path(local_path).absolute()\n\n        if not path.exists():\n            # try path in local registry if original path not exists\n            path = get_registry_folder() / local_path\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n\n        if metadata is not None:\n            with open(metadata_path, \"w\") as f:\n                f.write(metadata.model_dump_json(indent=2))\n\n        check_metadata(metadata_path)\n\n        with open(metadata_path) as f:\n            plain_metadata: Dict[str, Any] = json.load(f)\n\n        namespace = get_namespace(local_path)\n        name = plain_metadata.pop(\"name\")\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=name,\n                version=plain_metadata.pop(\"version\"),\n            )\n        )\n\n        entry_metadata = EntryMetadataInput.model_validate(plain_metadata)\n        source = entry_metadata.details.get(\"_source\", None)\n\n        if source is not None:\n            print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n            exit(1)\n\n        if self.info(entry_location) is None:\n            # New entry location. Check for similar names in registry.\n            entries = self.list_all_visible()\n            canonical_namespace = get_canonical_name(namespace)\n            canonical_name = get_canonical_name(name)\n\n            for entry in entries:\n                if entry.name == name and entry.namespace == namespace:\n                    break\n                if (\n                    get_canonical_name(entry.name) == canonical_name\n                    and get_canonical_name(entry.namespace) == canonical_namespace\n                ):\n                    print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                    exit(1)\n\n        registry.update(entry_location, entry_metadata)\n\n        all_files = []\n        total_size = 0\n\n        # Traverse all files in the directory `path`\n        for file in path.rglob(\"*\"):\n            if not file.is_file():\n                continue\n\n            relative = file.relative_to(path)\n\n            # Don't upload metadata file.\n            if file == metadata_path:\n                continue\n\n            # Don't upload backup files.\n            if file.name.endswith(\"~\"):\n                continue\n\n            # Don't upload configuration files.\n            if relative.parts[0] == \".nearai\":\n                continue\n\n            size = file.stat().st_size\n            total_size += size\n\n            all_files.append((file, relative, size))\n\n        pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n        for file, relative, size in all_files:\n            registry.upload_file(entry_location, file, relative)\n            pbar.update(size)\n\n        return entry_location\n\n    def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n        \"\"\"List files in from an entry in the registry.\n\n        Return the relative paths to all files with respect to the root of the entry.\n        \"\"\"\n        result = self.api.list_files_v1_registry_list_files_post(\n            BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n        )\n        return [file.filename for file in result]\n\n    def list(\n        self,\n        namespace: str,\n        category: str,\n        tags: str,\n        total: int,\n        offset: int,\n        show_all: bool,\n        show_latest_version: bool,\n        starred_by: str = \"\",\n    ) -&gt; List[EntryInformation]:\n        \"\"\"List and filter entries in the registry.\"\"\"\n        return self.api.list_entries_v1_registry_list_entries_post(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total,\n            offset=offset,\n            show_hidden=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=starred_by,\n        )\n\n    def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n        \"\"\"List all visible entries.\"\"\"\n        total = 10000\n        entries = self.list(\n            namespace=\"\",\n            category=category,\n            tags=\"\",\n            total=total,\n            offset=0,\n            show_all=False,\n            show_latest_version=True,\n        )\n        assert len(entries) &lt; total\n        return entries\n\n    def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n        \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n        entries = self.list_all_visible(category=\"model\")\n        result: Dict[NamespacedName, NamespacedName] = {}\n        for entry in entries:\n            namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n            canonical_namespaced_name = namespaced_name.canonical()\n            if canonical_namespaced_name in result:\n                raise ValueError(\n                    f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n                )\n            result[canonical_namespaced_name] = namespaced_name\n        return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Create Registry object to interact with the registry programmatically.</p> Source code in <code>nearai/registry.py</code> <pre><code>def __init__(self):\n    \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n    self.download_folder = DATA_FOLDER / \"registry\"\n    self.api = RegistryApi()\n\n    if not self.download_folder.exists():\n        self.download_folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.dict_models","title":"dict_models","text":"<pre><code>dict_models() -&gt; Dict[NamespacedName, NamespacedName]\n</code></pre> <p>Returns a mapping canonical-&gt;name.</p> Source code in <code>nearai/registry.py</code> <pre><code>def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n    \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n    entries = self.list_all_visible(category=\"model\")\n    result: Dict[NamespacedName, NamespacedName] = {}\n    for entry in entries:\n        namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n        canonical_namespaced_name = namespaced_name.canonical()\n        if canonical_namespaced_name in result:\n            raise ValueError(\n                f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n            )\n        result[canonical_namespaced_name] = namespaced_name\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download","title":"download","text":"<pre><code>download(entry_location: Union[str, EntryLocation], force: bool = False, show_progress: bool = False, verbose: bool = True) -&gt; Path\n</code></pre> <p>Download entry from the registry locally.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download(\n    self,\n    entry_location: Union[str, EntryLocation],\n    force: bool = False,\n    show_progress: bool = False,\n    verbose: bool = True,\n) -&gt; Path:\n    \"\"\"Download entry from the registry locally.\"\"\"\n    if isinstance(entry_location, str):\n        entry_location = parse_location(entry_location)\n\n    download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n    if download_path.exists():\n        if not force:\n            if verbose:\n                print(\n                    f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                )\n            return download_path\n\n    files = registry.list_files(entry_location)\n\n    download_path.mkdir(parents=True, exist_ok=True)\n\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        raise ValueError(f\"Entry {entry_location} not found.\")\n\n    metadata_path = download_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        f.write(metadata.model_dump_json(indent=2))\n\n    for file in (pbar := tqdm(files, disable=not show_progress)):\n        pbar.set_description(file)\n        registry.download_file(entry_location, file, download_path / file)\n\n    return download_path\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download_file","title":"download_file","text":"<pre><code>download_file(entry_location: EntryLocation, path: Path, local_path: Path)\n</code></pre> <p>Download a file from the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download_file(self, entry_location: EntryLocation, path: Path, local_path: Path):\n    \"\"\"Download a file from the registry.\"\"\"\n    result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n        BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n            dict(\n                entry_location=entry_location,\n                path=str(path),\n            )\n        )\n    )\n\n    local_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(local_path, \"wb\") as f:\n        copyfileobj(result, f)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.info","title":"info","text":"<pre><code>info(entry_location: EntryLocation) -&gt; Optional[EntryMetadata]\n</code></pre> <p>Get metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n    \"\"\"Get metadata of a entry in the registry.\"\"\"\n    try:\n        return self.api.download_metadata_v1_registry_download_metadata_post(\n            BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n        )\n    except NotFoundException:\n        return None\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list","title":"list","text":"<pre><code>list(namespace: str, category: str, tags: str, total: int, offset: int, show_all: bool, show_latest_version: bool, starred_by: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List and filter entries in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list(\n    self,\n    namespace: str,\n    category: str,\n    tags: str,\n    total: int,\n    offset: int,\n    show_all: bool,\n    show_latest_version: bool,\n    starred_by: str = \"\",\n) -&gt; List[EntryInformation]:\n    \"\"\"List and filter entries in the registry.\"\"\"\n    return self.api.list_entries_v1_registry_list_entries_post(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total,\n        offset=offset,\n        show_hidden=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=starred_by,\n    )\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_all_visible","title":"list_all_visible","text":"<pre><code>list_all_visible(category: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List all visible entries.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n    \"\"\"List all visible entries.\"\"\"\n    total = 10000\n    entries = self.list(\n        namespace=\"\",\n        category=category,\n        tags=\"\",\n        total=total,\n        offset=0,\n        show_all=False,\n        show_latest_version=True,\n    )\n    assert len(entries) &lt; total\n    return entries\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_files","title":"list_files","text":"<pre><code>list_files(entry_location: EntryLocation) -&gt; List[str]\n</code></pre> <p>List files in from an entry in the registry.</p> <p>Return the relative paths to all files with respect to the root of the entry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n    \"\"\"List files in from an entry in the registry.\n\n    Return the relative paths to all files with respect to the root of the entry.\n    \"\"\"\n    result = self.api.list_files_v1_registry_list_files_post(\n        BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n    )\n    return [file.filename for file in result]\n</code></pre>"},{"location":"api/#nearai.registry.Registry.update","title":"update","text":"<pre><code>update(entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]\n</code></pre> <p>Update metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n    \"\"\"Update metadata of a entry in the registry.\"\"\"\n    result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n        BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n    )\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload","title":"upload","text":"<pre><code>upload(local_path: Path, metadata: Optional[EntryMetadata] = None, show_progress: bool = False) -&gt; EntryLocation\n</code></pre> <p>Upload entry to the registry.</p> <p>If metadata is provided it will overwrite the metadata in the directory, otherwise it will use the metadata.json found on the root of the directory.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload(\n    self,\n    local_path: Path,\n    metadata: Optional[EntryMetadata] = None,\n    show_progress: bool = False,\n) -&gt; EntryLocation:\n    \"\"\"Upload entry to the registry.\n\n    If metadata is provided it will overwrite the metadata in the directory,\n    otherwise it will use the metadata.json found on the root of the directory.\n    \"\"\"\n    path = Path(local_path).absolute()\n\n    if not path.exists():\n        # try path in local registry if original path not exists\n        path = get_registry_folder() / local_path\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n\n    if metadata is not None:\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json(indent=2))\n\n    check_metadata(metadata_path)\n\n    with open(metadata_path) as f:\n        plain_metadata: Dict[str, Any] = json.load(f)\n\n    namespace = get_namespace(local_path)\n    name = plain_metadata.pop(\"name\")\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=name,\n            version=plain_metadata.pop(\"version\"),\n        )\n    )\n\n    entry_metadata = EntryMetadataInput.model_validate(plain_metadata)\n    source = entry_metadata.details.get(\"_source\", None)\n\n    if source is not None:\n        print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n        exit(1)\n\n    if self.info(entry_location) is None:\n        # New entry location. Check for similar names in registry.\n        entries = self.list_all_visible()\n        canonical_namespace = get_canonical_name(namespace)\n        canonical_name = get_canonical_name(name)\n\n        for entry in entries:\n            if entry.name == name and entry.namespace == namespace:\n                break\n            if (\n                get_canonical_name(entry.name) == canonical_name\n                and get_canonical_name(entry.namespace) == canonical_namespace\n            ):\n                print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                exit(1)\n\n    registry.update(entry_location, entry_metadata)\n\n    all_files = []\n    total_size = 0\n\n    # Traverse all files in the directory `path`\n    for file in path.rglob(\"*\"):\n        if not file.is_file():\n            continue\n\n        relative = file.relative_to(path)\n\n        # Don't upload metadata file.\n        if file == metadata_path:\n            continue\n\n        # Don't upload backup files.\n        if file.name.endswith(\"~\"):\n            continue\n\n        # Don't upload configuration files.\n        if relative.parts[0] == \".nearai\":\n            continue\n\n        size = file.stat().st_size\n        total_size += size\n\n        all_files.append((file, relative, size))\n\n    pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n    for file, relative, size in all_files:\n        registry.upload_file(entry_location, file, relative)\n        pbar.update(size)\n\n    return entry_location\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload_file","title":"upload_file","text":"<pre><code>upload_file(entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool\n</code></pre> <p>Upload a file to the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload_file(self, entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool:\n    \"\"\"Upload a file to the registry.\"\"\"\n    with open(local_path, \"rb\") as file:\n        data = file.read()\n\n        try:\n            self.api.upload_file_v1_registry_upload_file_post(\n                path=str(path),\n                file=data,\n                namespace=entry_location.namespace,\n                name=entry_location.name,\n                version=entry_location.version,\n            )\n            return True\n        except BadRequestException as e:\n            if isinstance(e.body, str) and \"already exists\" in e.body:\n                return False\n\n            raise e\n</code></pre>"},{"location":"api/#nearai.registry.get_namespace","title":"get_namespace","text":"<pre><code>get_namespace(local_path: Path) -&gt; str\n</code></pre> <p>Returns namespace of an item or user namespace.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_namespace(local_path: Path) -&gt; str:\n    \"\"\"Returns namespace of an item or user namespace.\"\"\"\n    registry_folder = get_registry_folder()\n\n    try:\n        # Check if the path matches the expected structure\n        relative_path = local_path.relative_to(registry_folder)\n        parts = relative_path.parts\n\n        # If the path has 3 parts (namespace, item_name, version),\n        # return the first part as the namespace\n        if len(parts) == 3:\n            return str(parts[0])\n    except ValueError:\n        # relative_to() raises ValueError if local_path is not relative to registry_folder\n        pass\n\n    # If we couldn't extract a namespace from the path, return the default\n    if CONFIG.auth is None:\n        raise ValueError(\"AuthData is None\")\n    return CONFIG.auth.namespace\n</code></pre>"},{"location":"api/#nearai.registry.get_registry_folder","title":"get_registry_folder","text":"<pre><code>get_registry_folder() -&gt; Path\n</code></pre> <p>Path to local registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_registry_folder() -&gt; Path:\n    \"\"\"Path to local registry.\"\"\"\n    return DATA_FOLDER / REGISTRY_FOLDER\n</code></pre>"},{"location":"api/#nearai.shared","title":"shared","text":""},{"location":"api/#nearai.shared.auth_data","title":"auth_data","text":""},{"location":"api/#nearai.shared.auth_data.AuthData","title":"AuthData","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>class AuthData(BaseModel):\n    account_id: str\n    signature: str\n    public_key: str\n    callback_url: str\n    nonce: str\n    recipient: str\n    message: str\n    on_behalf_of: Optional[str] = None\n\n    def generate_bearer_token(self):\n        \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n        required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n        for key in required_keys:\n            if getattr(self, key) is None:\n                raise ValueError(f\"Missing required auth data: {key}\")\n\n        if self.on_behalf_of is not None:\n            required_keys.add(\"on_behalf_of\")\n\n        bearer_data = {key: getattr(self, key) for key in required_keys}\n\n        return json.dumps(bearer_data)\n\n    @property\n    def namespace(self):\n        \"\"\"Get the account ID for the auth data.\n\n        In case you are running a request on behalf of another account, this will return the account ID of the account.\n        \"\"\"\n        if self.on_behalf_of is not None:\n            return self.on_behalf_of\n        return self.account_id\n</code></pre>"},{"location":"api/#nearai.shared.auth_data.AuthData.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Get the account ID for the auth data.</p> <p>In case you are running a request on behalf of another account, this will return the account ID of the account.</p>"},{"location":"api/#nearai.shared.auth_data.AuthData.generate_bearer_token","title":"generate_bearer_token","text":"<pre><code>generate_bearer_token()\n</code></pre> <p>Generates a JSON-encoded bearer token containing authentication data.</p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>def generate_bearer_token(self):\n    \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n    required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n    for key in required_keys:\n        if getattr(self, key) is None:\n            raise ValueError(f\"Missing required auth data: {key}\")\n\n    if self.on_behalf_of is not None:\n        required_keys.add(\"on_behalf_of\")\n\n    bearer_data = {key: getattr(self, key) for key in required_keys}\n\n    return json.dumps(bearer_data)\n</code></pre>"},{"location":"api/#nearai.shared.cache","title":"cache","text":""},{"location":"api/#nearai.shared.cache.mem_cache_with_timeout","title":"mem_cache_with_timeout","text":"<pre><code>mem_cache_with_timeout(timeout: int)\n</code></pre> <p>Decorator to cache function results for a specified timeout period.</p> Source code in <code>nearai/shared/cache.py</code> <pre><code>def mem_cache_with_timeout(timeout: int):\n    \"\"\"Decorator to cache function results for a specified timeout period.\"\"\"\n\n    def decorator(func):\n        cache = {}\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            now = time.time()\n            key = (args, frozenset(kwargs.items()))\n            if key in cache:\n                result, timestamp = cache[key]\n                if now - timestamp &lt; timeout:\n                    return result\n            result = func(*args, **kwargs)\n            cache[key] = (result, now)\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/#nearai.shared.client_config","title":"client_config","text":""},{"location":"api/#nearai.shared.client_config.ClientConfig","title":"ClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>class ClientConfig(BaseModel):\n    base_url: str = \"https://api.near.ai/v1\"\n    custom_llm_provider: str = \"openai\"\n    auth: Optional[AuthData] = None\n    default_provider: Optional[str] = None  # future: remove in favor of api decision\n    num_inference_retries: int = 1\n\n    def get_hub_client(self):\n        \"\"\"Get the hub client.\"\"\"\n        signature = f\"Bearer {self.auth.model_dump_json()}\"\n        base_url = self.base_url\n        return openai.OpenAI(\n            base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n        )\n</code></pre>"},{"location":"api/#nearai.shared.client_config.ClientConfig.get_hub_client","title":"get_hub_client","text":"<pre><code>get_hub_client()\n</code></pre> <p>Get the hub client.</p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>def get_hub_client(self):\n    \"\"\"Get the hub client.\"\"\"\n    signature = f\"Bearer {self.auth.model_dump_json()}\"\n    base_url = self.base_url\n    return openai.OpenAI(\n        base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client","title":"inference_client","text":""},{"location":"api/#nearai.shared.inference_client.InferenceClient","title":"InferenceClient","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>class InferenceClient(object):\n    def __init__(self, config: ClientConfig, runner_api_key: str = \"\", agent_identifier: str = \"\") -&gt; None:  # noqa: D107\n        self._config = config\n        self.runner_api_key = runner_api_key\n        self.agent_identifier = agent_identifier\n        if config.auth is not None:\n            auth_bearer_token = config.auth.generate_bearer_token()\n            new_token = json.loads(auth_bearer_token)\n            new_token[\"runner_data\"] = json.dumps({\"agent\": agent_identifier, \"runner_api_key\": self.runner_api_key})\n            auth_bearer_token = json.dumps(new_token)\n            self._auth = auth_bearer_token\n        else:\n            self._auth = None\n        self.client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n\n    # This makes sense in the CLI where we don't mind doing this request and caching it.\n    # In the aws_runner this is an extra request every time we run.\n    # TODO(#233): add a choice of a provider model in aws_runner, and then this step can be skipped.\n    @cached_property\n    def provider_models(self) -&gt; ProviderModels:  # noqa: D102\n        return ProviderModels(self._config)\n\n    def get_agent_public_key(self, agent_name: str) -&gt; str:\n        \"\"\"Request agent public key.\"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = {\"agent_name\": agent_name}\n\n        endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, params=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to get agent public key: {e}\") from None\n\n    def completions(\n        self,\n        model: str,\n        messages: Iterable[ChatCompletionMessageParam],\n        stream: bool = False,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Takes a `model` and `messages` and returns completions.\n\n        `model` can be:\n        1. full path `provider::model_full_path`.\n        2. `model_short_name`. Default provider will be used.\n        \"\"\"\n        provider, model = self.provider_models.match_provider_model(model)\n\n        if temperature is None:\n            temperature = DEFAULT_MODEL_TEMPERATURE\n\n        if max_tokens is None:\n            max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n        # NOTE(#246): this is to disable \"Provider List\" messages.\n        litellm.suppress_debug_info = True\n\n        for i in range(0, self._config.num_inference_retries):\n            try:\n                result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(\n                    model,\n                    messages,\n                    stream=stream,\n                    custom_llm_provider=self._config.custom_llm_provider,\n                    input_cost_per_token=0,\n                    output_cost_per_token=0,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                    base_url=self._config.base_url,\n                    provider=provider,\n                    api_key=self._auth,\n                    **kwargs,\n                )\n                break\n            except Exception as e:\n                if i == self._config.num_inference_retries - 1:\n                    raise ValueError(f\"Bad request: {e}\") from None\n\n        return result\n\n    def query_vector_store(\n        self, vector_store_id: str, query: str, full_files: bool = False\n    ) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n        \"\"\"Query a vector store.\"\"\"\n        if self._config is None:\n            raise ValueError(\"Missing NEAR AI Hub config\")\n\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        data = {\"query\": query, \"full_files\": full_files}\n\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Error querying vector store: {e}\") from None\n\n    def upload_file(\n        self,\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n        encoding: str = \"utf-8\",\n        file_name=\"file.txt\",\n        file_type=\"text/plain\",\n    ) -&gt; FileObject:\n        \"\"\"Uploads a file.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        file_data = io.BytesIO(file_content.encode(encoding))\n        return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n\n    def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n        \"\"\"Adds a file to vector store.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.beta.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n\n    def create_vector_store_from_source(\n        self,\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name (str): The name of the vector store.\n            source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n            source_auth (Optional[str]): The source authentication token.\n            chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n            expires_after (Optional[ExpiresAfter]): The expiration policy.\n            metadata (Optional[Dict[str, str]]): Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        print(f\"Creating vector store from source: {source}\")\n        headers = {\n            \"Authorization\": f\"Bearer {self._auth}\",\n            \"Content-Type\": \"application/json\",\n        }\n        data = {\n            \"name\": name,\n            \"source\": source,\n            \"source_auth\": source_auth,\n            \"chunking_strategy\": chunking_strategy,\n            \"expires_after\": expires_after,\n            \"metadata\": metadata,\n        }\n        endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            print(response.json())\n            response.raise_for_status()\n            return VectorStore(**response.json())\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to create vector store: {e}\") from None\n\n    def create_vector_store(\n        self,\n        name: str,\n        file_ids: List[str],\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, NotGiven] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates Vector Store.\n\n        :param name: Vector store name.\n        :param file_ids: Files to be added to the vector store.\n        :param expires_after: Expiration policy.\n        :param chunking_strategy: Chunking strategy.\n        :param metadata: Additional metadata.\n        :return: Returns the created vector store or error.\n        \"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.beta.vector_stores.create(\n            file_ids=file_ids,\n            name=name,\n            expires_after=expires_after,\n            chunking_strategy=chunking_strategy,\n            metadata=metadata,\n        )\n\n    def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n        response = requests.get(endpoint)\n        response.raise_for_status()\n        return VectorStore(**response.json())\n\n    def create_thread(self, messages):\n        \"\"\"Create a thread.\"\"\"\n        return self.client.beta.threads.create(messages=messages)\n\n    def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n        \"\"\"Create a message in a thread.\"\"\"\n        return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n\n    def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n        \"\"\"Create a thread and run the assistant.\"\"\"\n        thread = self.create_thread(messages)\n        return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n\n    def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n        \"\"\"List messages in a thread.\"\"\"\n        return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n\n    def threads_fork(self, thread_id: str):\n        \"\"\"Fork a thread.\"\"\"\n        forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n        return forked_thread\n\n    def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n        \"\"\"Create a run in a thread.\"\"\"\n        return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n\n    def run_agent(self, current_run_id: str, child_thread_id: str, assistant_id: str):\n        \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n        return self.client.beta.threads.runs.create(\n            thread_id=child_thread_id,\n            assistant_id=assistant_id,\n            extra_body={\"parent_run_id\": current_run_id},\n        )\n\n    def query_user_memory(self, query: str):\n        \"\"\"Query the user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory/query\",\n            body={\"query\": query},\n            cast_to=str,\n        )\n\n    def add_user_memory(self, memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory\",\n            body={\"memory\": memory},\n            cast_to=str,\n        )\n\n    def generate_image(self, prompt: str):\n        \"\"\"Generate an image.\"\"\"\n        return self.client.images.generate(prompt=prompt)\n\n    def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n        \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/agent_data\",\n            body={\n                \"key\": key,\n                \"value\": agent_data,\n            },\n            cast_to=Dict[str, Any],\n        )\n\n    def get_agent_data(self):\n        \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data\",\n            cast_to=Dict[str, str],\n        )\n\n    def get_agent_data_by_key(self, key: str):\n        \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data/{key}\",\n            cast_to=Dict[str, str],\n        )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_file_to_vector_store","title":"add_file_to_vector_store","text":"<pre><code>add_file_to_vector_store(vector_store_id: str, file_id: str) -&gt; VectorStoreFile\n</code></pre> <p>Adds a file to vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n    \"\"\"Adds a file to vector store.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.beta.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_user_memory","title":"add_user_memory","text":"<pre><code>add_user_memory(memory: str)\n</code></pre> <p>Add user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_user_memory(self, memory: str):\n    \"\"\"Add user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory\",\n        body={\"memory\": memory},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.completions","title":"completions","text":"<pre><code>completions(model: str, messages: Iterable[ChatCompletionMessageParam], stream: bool = False, temperature: Optional[float] = None, max_tokens: Optional[int] = None, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Takes a <code>model</code> and <code>messages</code> and returns completions.</p> <p><code>model</code> can be: 1. full path <code>provider::model_full_path</code>. 2. <code>model_short_name</code>. Default provider will be used.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def completions(\n    self,\n    model: str,\n    messages: Iterable[ChatCompletionMessageParam],\n    stream: bool = False,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Takes a `model` and `messages` and returns completions.\n\n    `model` can be:\n    1. full path `provider::model_full_path`.\n    2. `model_short_name`. Default provider will be used.\n    \"\"\"\n    provider, model = self.provider_models.match_provider_model(model)\n\n    if temperature is None:\n        temperature = DEFAULT_MODEL_TEMPERATURE\n\n    if max_tokens is None:\n        max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n    # NOTE(#246): this is to disable \"Provider List\" messages.\n    litellm.suppress_debug_info = True\n\n    for i in range(0, self._config.num_inference_retries):\n        try:\n            result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(\n                model,\n                messages,\n                stream=stream,\n                custom_llm_provider=self._config.custom_llm_provider,\n                input_cost_per_token=0,\n                output_cost_per_token=0,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                base_url=self._config.base_url,\n                provider=provider,\n                api_key=self._auth,\n                **kwargs,\n            )\n            break\n        except Exception as e:\n            if i == self._config.num_inference_retries - 1:\n                raise ValueError(f\"Bad request: {e}\") from None\n\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_thread","title":"create_thread","text":"<pre><code>create_thread(messages)\n</code></pre> <p>Create a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_thread(self, messages):\n    \"\"\"Create a thread.\"\"\"\n    return self.client.beta.threads.create(messages=messages)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store","title":"create_vector_store","text":"<pre><code>create_vector_store(name: str, file_ids: List[str], expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN, chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, NotGiven] = NOT_GIVEN, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates Vector Store.</p> <p>:param name: Vector store name. :param file_ids: Files to be added to the vector store. :param expires_after: Expiration policy. :param chunking_strategy: Chunking strategy. :param metadata: Additional metadata. :return: Returns the created vector store or error.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store(\n    self,\n    name: str,\n    file_ids: List[str],\n    expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n    chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, NotGiven] = NOT_GIVEN,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates Vector Store.\n\n    :param name: Vector store name.\n    :param file_ids: Files to be added to the vector store.\n    :param expires_after: Expiration policy.\n    :param chunking_strategy: Chunking strategy.\n    :param metadata: Additional metadata.\n    :return: Returns the created vector store or error.\n    \"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.beta.vector_stores.create(\n        file_ids=file_ids,\n        name=name,\n        expires_after=expires_after,\n        chunking_strategy=chunking_strategy,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store_from_source","title":"create_vector_store_from_source","text":"<pre><code>create_vector_store_from_source(name: str, source: Union[GitHubSource, GitLabSource], source_auth: Optional[str] = None, chunking_strategy: Optional[ChunkingStrategy] = None, expires_after: Optional[ExpiresAfter] = None, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates a vector store from the given source.</p> <pre><code>name (str): The name of the vector store.\nsource (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\nsource_auth (Optional[str]): The source authentication token.\nchunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\nexpires_after (Optional[ExpiresAfter]): The expiration policy.\nmetadata (Optional[Dict[str, str]]): Additional metadata.\n</code></pre> <pre><code>VectorStore: The created vector store.\n</code></pre> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store_from_source(\n    self,\n    name: str,\n    source: Union[GitHubSource, GitLabSource],\n    source_auth: Optional[str] = None,\n    chunking_strategy: Optional[ChunkingStrategy] = None,\n    expires_after: Optional[ExpiresAfter] = None,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates a vector store from the given source.\n\n    Args:\n    ----\n        name (str): The name of the vector store.\n        source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n        source_auth (Optional[str]): The source authentication token.\n        chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n        expires_after (Optional[ExpiresAfter]): The expiration policy.\n        metadata (Optional[Dict[str, str]]): Additional metadata.\n\n    Returns:\n    -------\n        VectorStore: The created vector store.\n\n    \"\"\"\n    print(f\"Creating vector store from source: {source}\")\n    headers = {\n        \"Authorization\": f\"Bearer {self._auth}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"name\": name,\n        \"source\": source,\n        \"source_auth\": source_auth,\n        \"chunking_strategy\": chunking_strategy,\n        \"expires_after\": expires_after,\n        \"metadata\": metadata,\n    }\n    endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        print(response.json())\n        response.raise_for_status()\n        return VectorStore(**response.json())\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to create vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.generate_image","title":"generate_image","text":"<pre><code>generate_image(prompt: str)\n</code></pre> <p>Generate an image.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def generate_image(self, prompt: str):\n    \"\"\"Generate an image.\"\"\"\n    return self.client.images.generate(prompt=prompt)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data","title":"get_agent_data","text":"<pre><code>get_agent_data()\n</code></pre> <p>Get agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data(self):\n    \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data_by_key","title":"get_agent_data_by_key","text":"<pre><code>get_agent_data_by_key(key: str)\n</code></pre> <p>Get agent data by key for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data_by_key(self, key: str):\n    \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data/{key}\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_public_key","title":"get_agent_public_key","text":"<pre><code>get_agent_public_key(agent_name: str) -&gt; str\n</code></pre> <p>Request agent public key.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_public_key(self, agent_name: str) -&gt; str:\n    \"\"\"Request agent public key.\"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n\n    data = {\"agent_name\": agent_name}\n\n    endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, params=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to get agent public key: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_vector_store","title":"get_vector_store","text":"<pre><code>get_vector_store(vector_store_id: str) -&gt; VectorStore\n</code></pre> <p>Gets a vector store by id.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n    \"\"\"Gets a vector store by id.\"\"\"\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n    response = requests.get(endpoint)\n    response.raise_for_status()\n    return VectorStore(**response.json())\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_user_memory","title":"query_user_memory","text":"<pre><code>query_user_memory(query: str)\n</code></pre> <p>Query the user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_user_memory(self, query: str):\n    \"\"\"Query the user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory/query\",\n        body={\"query\": query},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_vector_store","title":"query_vector_store","text":"<pre><code>query_vector_store(vector_store_id: str, query: str, full_files: bool = False) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]\n</code></pre> <p>Query a vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_vector_store(\n    self, vector_store_id: str, query: str, full_files: bool = False\n) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n    \"\"\"Query a vector store.\"\"\"\n    if self._config is None:\n        raise ValueError(\"Missing NEAR AI Hub config\")\n\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    data = {\"query\": query, \"full_files\": full_files}\n\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error querying vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.run_agent","title":"run_agent","text":"<pre><code>run_agent(current_run_id: str, child_thread_id: str, assistant_id: str)\n</code></pre> <p>Starts a child agent run from a parent agent run.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def run_agent(self, current_run_id: str, child_thread_id: str, assistant_id: str):\n    \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n    return self.client.beta.threads.runs.create(\n        thread_id=child_thread_id,\n        assistant_id=assistant_id,\n        extra_body={\"parent_run_id\": current_run_id},\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.save_agent_data","title":"save_agent_data","text":"<pre><code>save_agent_data(key: str, agent_data: Dict[str, Any])\n</code></pre> <p>Save agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n    \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/agent_data\",\n        body={\n            \"key\": key,\n            \"value\": agent_data,\n        },\n        cast_to=Dict[str, Any],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_create_and_run_poll","title":"threads_create_and_run_poll","text":"<pre><code>threads_create_and_run_poll(assistant_id: str, model: str, messages: List[ChatCompletionMessageParam])\n</code></pre> <p>Create a thread and run the assistant.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n    \"\"\"Create a thread and run the assistant.\"\"\"\n    thread = self.create_thread(messages)\n    return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_fork","title":"threads_fork","text":"<pre><code>threads_fork(thread_id: str)\n</code></pre> <p>Fork a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_fork(self, thread_id: str):\n    \"\"\"Fork a thread.\"\"\"\n    forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n    return forked_thread\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_list_messages","title":"threads_list_messages","text":"<pre><code>threads_list_messages(thread_id: str, order: Literal['asc', 'desc'] = 'asc')\n</code></pre> <p>List messages in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n    \"\"\"List messages in a thread.\"\"\"\n    return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_messages_create","title":"threads_messages_create","text":"<pre><code>threads_messages_create(thread_id: str, content: str, role: Literal['user', 'assistant'])\n</code></pre> <p>Create a message in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n    \"\"\"Create a message in a thread.\"\"\"\n    return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_runs_create","title":"threads_runs_create","text":"<pre><code>threads_runs_create(thread_id: str, assistant_id: str, model: str)\n</code></pre> <p>Create a run in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n    \"\"\"Create a run in a thread.\"\"\"\n    return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.upload_file","title":"upload_file","text":"<pre><code>upload_file(file_content: str, purpose: Literal['assistants', 'batch', 'fine-tune', 'vision'], encoding: str = 'utf-8', file_name='file.txt', file_type='text/plain') -&gt; FileObject\n</code></pre> <p>Uploads a file.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def upload_file(\n    self,\n    file_content: str,\n    purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n    encoding: str = \"utf-8\",\n    file_name=\"file.txt\",\n    file_type=\"text/plain\",\n) -&gt; FileObject:\n    \"\"\"Uploads a file.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    file_data = io.BytesIO(file_content.encode(encoding))\n    return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n</code></pre>"},{"location":"api/#nearai.shared.models","title":"models","text":""},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam","title":"AutoFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class AutoFileChunkingStrategyParam(TypedDict, total=False):\n    type: Required[Literal[\"auto\"]]\n    \"\"\"Always `auto`.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['auto']]\n</code></pre> <p>Always <code>auto</code>.</p>"},{"location":"api/#nearai.shared.models.ChunkingStrategy","title":"ChunkingStrategy","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the chunking strategy for vector stores.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ChunkingStrategy(BaseModel):\n    \"\"\"Defines the chunking strategy for vector stores.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest","title":"CreateVectorStoreRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a new vector store.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class CreateVectorStoreRequest(BaseModel):\n    \"\"\"Request model for creating a new vector store.\"\"\"\n\n    chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n    \"\"\"The chunking strategy to use for the vector store.\"\"\"\n    expires_after: Optional[ExpiresAfter] = None\n    \"\"\"The expiration time for the vector store.\"\"\"\n    file_ids: Optional[List[str]] = None\n    \"\"\"The file IDs to attach to the vector store.\"\"\"\n    metadata: Optional[Dict[str, str]] = None\n    \"\"\"The metadata to attach to the vector store.\"\"\"\n    name: str\n    \"\"\"The name of the vector store.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.chunking_strategy","title":"chunking_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n</code></pre> <p>The chunking strategy to use for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.expires_after","title":"expires_after  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expires_after: Optional[ExpiresAfter] = None\n</code></pre> <p>The expiration time for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.file_ids","title":"file_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_ids: Optional[List[str]] = None\n</code></pre> <p>The file IDs to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[Dict[str, str]] = None\n</code></pre> <p>The metadata to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the vector store.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter","title":"ExpiresAfter","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ExpiresAfter(TypedDict, total=False):\n    anchor: Required[Literal[\"last_active_at\"]]\n    \"\"\"Anchor timestamp after which the expiration policy applies.\n\n    Supported anchors: `last_active_at`.\n    \"\"\"\n\n    days: Required[int]\n    \"\"\"The number of days after the anchor time that the vector store will expire.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.ExpiresAfter.anchor","title":"anchor  <code>instance-attribute</code>","text":"<pre><code>anchor: Required[Literal['last_active_at']]\n</code></pre> <p>Anchor timestamp after which the expiration policy applies.</p> <p>Supported anchors: <code>last_active_at</code>.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter.days","title":"days  <code>instance-attribute</code>","text":"<pre><code>days: Required[int]\n</code></pre> <p>The number of days after the anchor time that the vector store will expire.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam","title":"StaticFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class StaticFileChunkingStrategyParam(TypedDict, total=False):\n    chunk_overlap_tokens: Required[int]\n    \"\"\"The number of tokens that overlap between chunks. The default value is `400`.\n\n    Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n    \"\"\"\n\n    max_chunk_size_tokens: Required[int]\n    \"\"\"The maximum number of tokens in each chunk.\n\n    The default value is `800`. The minimum value is `100` and the maximum value is\n    `4096`.\n    \"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.chunk_overlap_tokens","title":"chunk_overlap_tokens  <code>instance-attribute</code>","text":"<pre><code>chunk_overlap_tokens: Required[int]\n</code></pre> <p>The number of tokens that overlap between chunks. The default value is <code>400</code>.</p> <p>Note that the overlap must not exceed half of <code>max_chunk_size_tokens</code>.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.max_chunk_size_tokens","title":"max_chunk_size_tokens  <code>instance-attribute</code>","text":"<pre><code>max_chunk_size_tokens: Required[int]\n</code></pre> <p>The maximum number of tokens in each chunk.</p> <p>The default value is <code>800</code>. The minimum value is <code>100</code> and the maximum value is <code>4096</code>.</p>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate","title":"VectorStoreFileCreate","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a vector store file.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class VectorStoreFileCreate(BaseModel):\n    \"\"\"Request model for creating a vector store file.\"\"\"\n\n    file_id: str\n    \"\"\"File ID returned from upload file endpoint.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>File ID returned from upload file endpoint.</p>"},{"location":"api/#nearai.shared.naming","title":"naming","text":""},{"location":"api/#nearai.shared.naming.NamespacedName","title":"NamespacedName","text":"Source code in <code>nearai/shared/naming.py</code> <pre><code>class NamespacedName:\n    def __init__(self, name: str, namespace: str = \"\"):  # noqa: D107\n        self.name = name\n        self.namespace = namespace\n\n    def __eq__(self, other):  # noqa: D105\n        if not isinstance(other, NamespacedName):\n            return NotImplemented\n        return self.name == other.name and self.namespace == other.namespace\n\n    def __hash__(self):  # noqa: D105\n        return hash((self.name, self.namespace))\n\n    def __str__(self):  # noqa: D105\n        if self.namespace:\n            return f\"{self.namespace}/{self.name}\"\n        return self.name\n\n    def __repr__(self):  # noqa: D105\n        return f\"NamespacedName(name='{self.name}', namespace='{self.namespace}')\"\n\n    def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n        \"\"\"Returns canonical NamespacedName.\"\"\"\n        return NamespacedName(\n            name=get_canonical_name(self.name),\n            namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n        )\n</code></pre>"},{"location":"api/#nearai.shared.naming.NamespacedName.canonical","title":"canonical","text":"<pre><code>canonical() -&gt; NamespacedName\n</code></pre> <p>Returns canonical NamespacedName.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n    \"\"\"Returns canonical NamespacedName.\"\"\"\n    return NamespacedName(\n        name=get_canonical_name(self.name),\n        namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n    )\n</code></pre>"},{"location":"api/#nearai.shared.naming.create_registry_name","title":"create_registry_name","text":"<pre><code>create_registry_name(name: str) -&gt; str\n</code></pre> <p>Formats <code>name</code> for a suitable registry name.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def create_registry_name(name: str) -&gt; str:\n    \"\"\"Formats `name` for a suitable registry name.\"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;-&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1-\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '-'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1-\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '-'\n    name = re.sub(r\"[^a-z0-9-]\", \"\", name)\n    # Convert 'metallama' or 'meta-llama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    name = name.replace(\"meta-llama\", \"llama\")\n    # Convert 'qwenq' or 'qwen-q' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    name = name.replace(\"qwen-q\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.naming.get_canonical_name","title":"get_canonical_name","text":"<pre><code>get_canonical_name(name: str) -&gt; str\n</code></pre> <p>Returns a name that can be used for matching entities.</p> <p>Applies such transformations: 1. All letters lowercase. 2. Convert '.' between digits to 'p'. 3. Convert 'v' -&gt; '' 4. Remove all non-alphanumeric characters except between digits.     Use '_' between digits. 5. Convert 'metallama' -&gt; 'llama'. <p>e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def get_canonical_name(name: str) -&gt; str:\n    \"\"\"Returns a name that can be used for matching entities.\n\n    Applies such transformations:\n    1. All letters lowercase.\n    2. Convert '.' between digits to 'p'.\n    3. Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    4. Remove all non-alphanumeric characters except between digits.\n        Use '_' between digits.\n    5. Convert 'metallama' -&gt; 'llama'.\n\n    e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"\n    \"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;_&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1_\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '_'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1_\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '_'\n    name = re.sub(r\"[^a-z0-9_]\", \"\", name)\n    # Remove any remaining underscores that are not between digits\n    name = re.sub(r\"(?&lt;!\\d)_|_(?!\\d)\", \"\", name)\n    # Convert 'metallama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    # Convert 'qwenq' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.near","title":"near","text":""},{"location":"api/#nearai.shared.near.sign","title":"sign","text":""},{"location":"api/#nearai.shared.near.sign.SignatureVerificationResult","title":"SignatureVerificationResult","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>class SignatureVerificationResult(Enum):\n    TRUE = True\n    FALSE = False\n    VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE = \"verify_access_key_owner_not_available\"\n\n    @classmethod\n    def from_bool(cls, value: bool):\n        \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n        return cls.TRUE if value else cls.FALSE\n\n    def __bool__(self):\n        \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n        return self == SignatureVerificationResult.TRUE\n</code></pre> __bool__ \u00b6 <pre><code>__bool__()\n</code></pre> <p>Overrides the behavior when checking for truthiness.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def __bool__(self):\n    \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n    return self == SignatureVerificationResult.TRUE\n</code></pre> from_bool <code>classmethod</code> \u00b6 <pre><code>from_bool(value: bool)\n</code></pre> <p>Gets VerificationResult based on a boolean value.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@classmethod\ndef from_bool(cls, value: bool):\n    \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n    return cls.TRUE if value else cls.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.convert_nonce","title":"convert_nonce","text":"<pre><code>convert_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Converts a given value to a 32-byte nonce.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def convert_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Converts a given value to a 32-byte nonce.\"\"\"\n    if isinstance(value, bytes):\n        if len(value) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(value) &lt; 32:\n            value = value.rjust(32, b\"0\")\n        return value\n    elif isinstance(value, str):\n        nonce_bytes = value.encode(\"utf-8\")\n        if len(nonce_bytes) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(nonce_bytes) &lt; 32:\n            nonce_bytes = nonce_bytes.rjust(32, b\"0\")\n        return nonce_bytes\n    elif isinstance(value, list):\n        if len(value) != 32:\n            raise ValueError(\"Invalid nonce length\")\n        return bytes(value)\n    else:\n        raise ValueError(\"Invalid nonce format\")\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_inference_signature","title":"create_inference_signature","text":"<pre><code>create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given extended inference payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given extended inference payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_signature","title":"create_signature","text":"<pre><code>create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    # Extract and decode the private key\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    # Use only the first 32 bytes as the seed\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_completion_signature","title":"validate_completion_signature","text":"<pre><code>validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        return True\n    except nacl.exceptions.BadSignatureError:\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_nonce","title":"validate_nonce","text":"<pre><code>validate_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Ensures that the nonce is a valid timestamp.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Ensures that the nonce is a valid timestamp.\"\"\"\n    nonce = convert_nonce(value)\n    nonce_int = int(nonce.decode(\"utf-8\"))\n\n    now = int(time.time() * 1000)\n\n    if nonce_int &gt; now:\n        raise ValueError(\"Nonce is in the future\")\n    if now - nonce_int &gt; 10 * 365 * 24 * 60 * 60 * 1000:\n        \"\"\"If the timestamp is older than 10 years, it is considered invalid. Forcing apps to use unique nonces.\"\"\"\n        raise ValueError(\"Nonce is too old\")\n\n    return nonce\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_signature","title":"validate_signature","text":"<pre><code>validate_signature(public_key: str, signature: str, payload: Payload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_signature(public_key: str, signature: str, payload: Payload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        # print(\"Signature is valid.\")\n        return True\n    except nacl.exceptions.BadSignatureError:\n        # print(\"Signature was forged or corrupt.\")\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_access_key_owner","title":"verify_access_key_owner","text":"<pre><code>verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies if a given public key belongs to a specified account ID using FastNEAR API.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@mem_cache_with_timeout(300)\ndef verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies if a given public key belongs to a specified account ID using FastNEAR API.\"\"\"\n    try:\n        logger.info(f\"Verifying access key owner for public key: {public_key}, account_id: {account_id}\")\n        url = f\"https://api.fastnear.com/v0/public_key/{public_key}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        content = response.json()\n        account_ids = content.get(\"account_ids\", [])\n        key_owner_verified = account_id in account_ids\n        if not key_owner_verified:\n            logger.info(\"Key's owner verification failed. Only NEAR Mainnet accounts are supported.\")\n        return SignatureVerificationResult.from_bool(key_owner_verified)\n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        logger.error(f\"Other error occurred: {err}\")\n\n    return SignatureVerificationResult.VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(account_id, public_key, signature, message, nonce, recipient, callback_url) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies a signed message and ensures the public key belongs to the specified account.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def verify_signed_message(\n    account_id, public_key, signature, message, nonce, recipient, callback_url\n) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies a signed message and ensures the public key belongs to the specified account.\"\"\"\n    is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, callback_url))\n\n    if not is_valid and callback_url is not None:\n        is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, None))\n\n    if is_valid:\n        # verify that key belongs to `account_id`\n        return verify_access_key_owner(public_key, account_id)\n\n    # TODO verifies that key is a FULL ACCESS KEY\n\n    return SignatureVerificationResult.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.provider_models","title":"provider_models","text":""},{"location":"api/#nearai.shared.provider_models.ProviderModels","title":"ProviderModels","text":"Source code in <code>nearai/shared/provider_models.py</code> <pre><code>class ProviderModels:\n    def __init__(self, config: ClientConfig) -&gt; None:  # noqa: D107\n        self._config = config\n\n    @cached_property\n    def provider_models(self) -&gt; Dict[NamespacedName, Dict[str, str]]:\n        \"\"\"Returns a mapping canonical-&gt;provider-&gt;model_full_name.\"\"\"\n        client = self._config.get_hub_client()\n\n        try:\n            models = client.models.list()\n\n            assert len(models.data) &gt; 0, \"No models found\"\n            result: Dict[NamespacedName, Dict[str, str]] = {}\n            for model in models.data:\n                provider, namespaced_model = get_provider_namespaced_model(model.id)\n                namespaced_model = namespaced_model.canonical()\n                if namespaced_model not in result:\n                    result[namespaced_model] = {}\n                if provider in result[namespaced_model]:\n                    raise ValueError(f\"Duplicate entry for provider {provider} and model {namespaced_model}\")\n                result[namespaced_model][provider] = model.id\n\n            return result\n\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Error fetching models: {str(e)}\") from e\n\n    def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n        \"\"\"Returns provider matches for `model`.\"\"\"\n        return self.provider_models.get(model.canonical(), {})\n\n    def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n        \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n        `model` may take different formats. Supported ones:\n        1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n        2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n        3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n        4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n        5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n        6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n        \"\"\"\n        if provider == \"\":\n            provider = None\n        matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n        namespaced_model = namespaced_model.canonical()\n        if namespaced_model not in self.provider_models:\n            raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n        available_matches = self.provider_models[namespaced_model]\n        if matched_provider not in available_matches:\n            for match in available_matches.keys():\n                matched_provider = match\n                break\n        if provider and provider != matched_provider:\n            raise ValueError(\n                f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n            )\n        return matched_provider, available_matches[matched_provider]\n\n    def get_unregistered_common_provider_models(\n        self, registry_models: Dict[NamespacedName, NamespacedName]\n    ) -&gt; List[Dict[str, str]]:\n        \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n        result: List[Dict[str, str]] = []\n        for namespaced_name, available_matches in self.provider_models.items():\n            if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n                continue\n            result.append(available_matches)\n        return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.provider_models","title":"provider_models  <code>cached</code> <code>property</code>","text":"<pre><code>provider_models: Dict[NamespacedName, Dict[str, str]]\n</code></pre> <p>Returns a mapping canonical-&gt;provider-&gt;model_full_name.</p>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.available_provider_matches","title":"available_provider_matches","text":"<pre><code>available_provider_matches(model: NamespacedName) -&gt; Dict[str, str]\n</code></pre> <p>Returns provider matches for <code>model</code>.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n    \"\"\"Returns provider matches for `model`.\"\"\"\n    return self.provider_models.get(model.canonical(), {})\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.get_unregistered_common_provider_models","title":"get_unregistered_common_provider_models","text":"<pre><code>get_unregistered_common_provider_models(registry_models: Dict[NamespacedName, NamespacedName]) -&gt; List[Dict[str, str]]\n</code></pre> <p>Returns provider matches for unregistered provider models with default namespace.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_unregistered_common_provider_models(\n    self, registry_models: Dict[NamespacedName, NamespacedName]\n) -&gt; List[Dict[str, str]]:\n    \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n    result: List[Dict[str, str]] = []\n    for namespaced_name, available_matches in self.provider_models.items():\n        if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n            continue\n        result.append(available_matches)\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.match_provider_model","title":"match_provider_model","text":"<pre><code>match_provider_model(model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]\n</code></pre> <p>Returns provider and model_full_path for given <code>model</code> and optional <code>provider</code>.</p> <p><code>model</code> may take different formats. Supported ones: 1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\" 2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\" 3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\" 4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\" 5. model_name as used in registry, e.g. \"llama-3-70b-instruct\" 6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n    \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n    `model` may take different formats. Supported ones:\n    1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n    2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n    3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n    4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n    5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n    6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n    \"\"\"\n    if provider == \"\":\n        provider = None\n    matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n    namespaced_model = namespaced_model.canonical()\n    if namespaced_model not in self.provider_models:\n        raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n    available_matches = self.provider_models[namespaced_model]\n    if matched_provider not in available_matches:\n        for match in available_matches.keys():\n            matched_provider = match\n            break\n    if provider and provider != matched_provider:\n        raise ValueError(\n            f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n        )\n    return matched_provider, available_matches[matched_provider]\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_model","title":"get_provider_model","text":"<pre><code>get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]\n</code></pre> <p>Splits the <code>model</code> string based on a predefined separator and returns the components.</p> <pre><code>provider (Optional[str]): The default provider name. Can be `None` if the provider\n                          is included in the `model` string.\nmodel (str): The model identifier, which may include the provider name separated by\n             a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n</code></pre> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]:\n    \"\"\"Splits the `model` string based on a predefined separator and returns the components.\n\n    Args:\n    ----\n        provider (Optional[str]): The default provider name. Can be `None` if the provider\n                                  is included in the `model` string.\n        model (str): The model identifier, which may include the provider name separated by\n                     a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n\n    \"\"\"\n    if PROVIDER_MODEL_SEP in model:\n        parts = model.split(PROVIDER_MODEL_SEP)\n        assert len(parts) == 2\n        return parts[0], parts[1]\n    return provider, model\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_namespaced_model","title":"get_provider_namespaced_model","text":"<pre><code>get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]\n</code></pre> <p>Given <code>provider_model</code> returns provider and namespaced model.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]:\n    \"\"\"Given `provider_model` returns provider and namespaced model.\"\"\"\n    provider_model = provider_model.replace(\"accounts/\", \"\")\n    provider_model = provider_model.replace(\"fireworks/\", \"\")\n    provider_model = provider_model.replace(\"models/\", \"\")\n    provider_opt, model = get_provider_model(DEFAULT_PROVIDER if not provider else provider, provider_model)\n    provider = cast(str, provider_opt)\n    if provider == \"hyperbolic\":\n        model = re.sub(r\".*/\", \"\", model)\n        return provider, NamespacedName(model)\n    if provider == \"fireworks\":\n        parts = model.split(\"/\")\n        if len(parts) == 1:\n            return provider, NamespacedName(name=parts[0])\n        elif len(parts) == 2:\n            return provider, NamespacedName(namespace=parts[0], name=parts[1])\n        else:\n            raise ValueError(f\"Invalid model format for Fireworks: {model}\")\n    if provider == \"local\":\n        model = re.sub(r\".*/\", \"\", model)\n        return provider, NamespacedName(name=model)\n    raise ValueError(f\"Unrecognized provider: {provider}\")\n</code></pre>"},{"location":"api/#nearai.solvers","title":"solvers","text":""},{"location":"api/#nearai.solvers.DDOTSV0Solver","title":"DDOTSV0Solver","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for competitive programming problems live on DDOTS.</p> <p>This dataset will run agents in an Agent environment previously prepared.</p> <p>workspace/     .id             -- Id of the problem     PROBLEM.txt     -- Description of the problem</p> <p>The agent should call env.submit_python(code) to submit the code to the DDOTS server.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSV0Solver(SolverStrategy):\n    \"\"\"Solver strategy for competitive programming problems live on DDOTS.\n\n    This dataset will run agents in an Agent environment previously prepared.\n\n    workspace/\n        .id             -- Id of the problem\n        PROBLEM.txt     -- Description of the problem\n\n    The agent should call env.submit_python(code) to submit the code to the DDOTS server.\n\n    \"\"\"\n\n    def __init__(self, dataset_ref: Dataset, agents: str, max_iterations: int, save_snapshots: bool = False):  # noqa: D107\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        self.agents = [Agent.load_agent(agent, client_config) for agent in agents.split(\",\")]\n        self.max_iterations = max_iterations\n\n        date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        rnd_id = random.randint(10**8, 10**9 - 1)\n        self._saved_trajectories = DATA_FOLDER / \"data\" / \"ddots_v0_trajectories\" / f\"{date}_{rnd_id}\"\n        self._saved_trajectories.mkdir(parents=True, exist_ok=True)\n\n        self.save_snapshots = save_snapshots\n        print(\"Saving trajectories to\", self._saved_trajectories)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"ddots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"ddots_codeforces_small/v0\", \"datasets/ddots_codeforces_medium_A_B/v0\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        problem_id = datum[\"problem_id\"]\n        description = datum[\"description\"]\n\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        client = InferenceClient(client_config)\n        env = DDOTSEnvironment(self.agents, problem_id, description, client)\n        env.write_file(\".solved\", str(False))\n\n        try:\n            env.run(description, max_iterations=self.max_iterations)\n            env.write_file(\".solved\", str(env.solved))\n\n        except Exception as e:\n            print(f\"Error running task: {e}\")\n\n        finally:\n            if self.save_snapshots:\n                snapshot = env.create_snapshot()\n                with open(self._saved_trajectories / f\"{problem_id}.tar.gz\", \"wb\") as f:\n                    f.write(snapshot)\n\n        return env.solved\n</code></pre>"},{"location":"api/#nearai.solvers.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i+1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        assert \"/\" not in name\n        return name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy","title":"SolverStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for solver strategies.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategy(ABC, metaclass=SolverStrategyMeta):\n    \"\"\"Abstract class for solver strategies.\"\"\"\n\n    def __init__(self, model: str = \"\", agent: str = \"\") -&gt; None:\n        CONFIG.confirm_commands = False\n        self.client_config = CONFIG.get_client_config()\n        self.client = InferenceClient(self.client_config)\n        assert model != \"\" or agent != \"\"\n        self.dataset_evaluation_name = \"\"\n\n        self.provider = \"\"\n        self.model_namespace = \"\"\n        self.model_full_path = \"\"\n        self.model_name = \"\"\n        if model != \"\":\n            self.provider, self.model_full_path = self.client.provider_models.match_provider_model(model)\n            self.provider, namespaced_model = get_provider_namespaced_model(self.model_full_path, self.provider)\n            self.model_namespace = namespaced_model.namespace\n            self.model_name = namespaced_model.name\n\n        self.agent = agent\n        self.agent_params = {\n            \"api_url\": CONFIG.api_url,\n            \"data_source\": \"local_files\",\n            \"temperature\": 0.0,\n            \"record_run\": False,\n            \"verbose\": False,\n            \"change_to_agent_temp_dir\": False,\n        }\n        if self.model_full_path:\n            self.agent_params[\"model\"] = self.model_full_path\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Returns the name of the solver strategy.\"\"\"\n        return type(self).__name__\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:\n        return SolverScoringMethod.TrueOrFalseList\n\n    @abstractmethod\n    def evaluation_name(self) -&gt; str:\n        \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n        ...\n\n    @abstractmethod\n    def compatible_datasets(self) -&gt; List[str]:\n        \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n        ...\n\n    def agent_name(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.parent.name\n\n    def agent_version(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.name\n\n    def evaluated_entry_namespace(self) -&gt; str:\n        \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n        if self.agent:\n            path = Path(self.agent)\n            return path.parent.parent.name\n        return self.model_namespace\n\n    def model_provider(self) -&gt; str:\n        \"\"\"Returns model provider.\"\"\"\n        if self.provider != \"\":\n            return self.provider\n        if self.agent != \"\":\n            agent_obj = Agent.load_agent(self.agent, self.client_config)\n            return agent_obj.model_provider\n        return \"\"\n\n    @abstractmethod\n    def solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n        \"\"\"Solves the task for the given datum.\"\"\"\n        ...\n\n    def get_custom_tasks(self) -&gt; List[dict]:\n        \"\"\"Custom tasks for custom benchmark.\"\"\"\n        if self.scoring_method == SolverScoringMethod.Custom:\n            raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n        else:\n            raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Given results for all datums, returns evaluation metrics.\n\n        Not used by TrueOrFalseList scoring method.\n        Do not prepend with evaluation_name. If hierarchical, use slashes /.\n        Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n        \"\"\"\n        raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n\n    def start_inference_session(self, task_id: str) -&gt; SolverInferenceSession:\n        return SolverInferenceSession(\n            self.agent, self.agent_params, self.model_full_path, self.client, self.evaluation_name()\n        ).start_inference_session(task_id)\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the solver strategy.</p>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_name","title":"agent_name","text":"<pre><code>agent_name() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_name(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.parent.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_version","title":"agent_version","text":"<pre><code>agent_version() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_version(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.compatible_datasets","title":"compatible_datasets  <code>abstractmethod</code>","text":"<pre><code>compatible_datasets() -&gt; List[str]\n</code></pre> <p>Returns the list of datasets that the solver strategy is compatible with.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef compatible_datasets(self) -&gt; List[str]:\n    \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluated_entry_namespace","title":"evaluated_entry_namespace","text":"<pre><code>evaluated_entry_namespace() -&gt; str\n</code></pre> <p>Returns namespace of a model or agent to be evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def evaluated_entry_namespace(self) -&gt; str:\n    \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n    if self.agent:\n        path = Path(self.agent)\n        return path.parent.parent.name\n    return self.model_namespace\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluation_name","title":"evaluation_name  <code>abstractmethod</code>","text":"<pre><code>evaluation_name() -&gt; str\n</code></pre> <p>Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef evaluation_name(self) -&gt; str:\n    \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_custom_tasks","title":"get_custom_tasks","text":"<pre><code>get_custom_tasks() -&gt; List[dict]\n</code></pre> <p>Custom tasks for custom benchmark.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_custom_tasks(self) -&gt; List[dict]:\n    \"\"\"Custom tasks for custom benchmark.\"\"\"\n    if self.scoring_method == SolverScoringMethod.Custom:\n        raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n    else:\n        raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_evaluation_metrics","title":"get_evaluation_metrics","text":"<pre><code>get_evaluation_metrics(tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]\n</code></pre> <p>Given results for all datums, returns evaluation metrics.</p> <p>Not used by TrueOrFalseList scoring method. Do not prepend with evaluation_name. If hierarchical, use slashes /. Expected metrics is a dict of scores, e.g.: {\"average\": , \"group/coding\": }. Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Given results for all datums, returns evaluation metrics.\n\n    Not used by TrueOrFalseList scoring method.\n    Do not prepend with evaluation_name. If hierarchical, use slashes /.\n    Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n    \"\"\"\n    raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.model_provider","title":"model_provider","text":"<pre><code>model_provider() -&gt; str\n</code></pre> <p>Returns model provider.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def model_provider(self) -&gt; str:\n    \"\"\"Returns model provider.\"\"\"\n    if self.provider != \"\":\n        return self.provider\n    if self.agent != \"\":\n        agent_obj = Agent.load_agent(self.agent, self.client_config)\n        return agent_obj.model_provider\n    return \"\"\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.solve","title":"solve  <code>abstractmethod</code>","text":"<pre><code>solve(datum: dict) -&gt; Union[bool, Tuple[bool, Any]]\n</code></pre> <p>Solves the task for the given datum.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n    \"\"\"Solves the task for the given datum.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategyMeta","title":"SolverStrategyMeta","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass that automatically registers subclasses in the SolverStrategyRegistry.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategyMeta(ABCMeta):\n    \"\"\"Metaclass that automatically registers subclasses in the SolverStrategyRegistry.\"\"\"\n\n    def __new__(cls, name: str, bases: tuple, namespace: dict) -&gt; Any:\n        new_class = super().__new__(cls, name, bases, namespace)\n        if bases != (ABC,):  # Avoid registering the abstract base class itself\n            SolverStrategyRegistry[new_class.__name__] = new_class  # type: ignore\n        return new_class\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver","title":"ddot_v0_solver","text":""},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSEnvironment","title":"DDOTSEnvironment","text":"<p>               Bases: <code>Environment</code></p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSEnvironment(Environment):\n    def __init__(self, agents: List[Agent], problem_id: str, description: str, client):  # noqa: D107\n        self.tdir = TemporaryDirectory()\n        self.hub_client = get_hub_client()\n        thread = self.hub_client.beta.threads.create()\n        super().__init__(\n            self.tdir.name,\n            agents,\n            client,\n            self.hub_client,\n            thread.id,\n            \"todo\",\n            approvals={\"confirm_execution\": lambda _: False},\n        )\n\n        self.problem_id = problem_id\n        self.solved = False\n\n        files = {\n            \".id\": problem_id,\n            \"PROBLEM.txt\": description,\n            \"solution.py\": \"\",\n            \"test.in\": \"\",\n            \"test.sh\": \"#!/bin/bash\\npython3 solution.py &lt; test.in\",\n        }\n        for fname, content in files.items():\n            with open(self.tdir.name + \"/\" + fname, \"w\") as f:\n                f.write(content)\n\n    async def async_submit(self, code: str) -&gt; Tuple[bool, str]:  # noqa: D102\n        submission_id = await submit_problem(self.problem_id, code, Extensions.PYTHON)\n\n        try:\n            await is_output_ready(submission_id)\n        except Exception:\n            print(\"WARNING: Submission took too long to execute on DDOTS\")\n            self.mark_done()\n            return False, \"Submission took too long to execute on the platform\"\n\n        ok = await submission_accepted(submission_id)\n\n        if ok:\n            self.solved = True\n            self.mark_done()\n            return True, \"\"\n\n        output = await get_output(submission_id)\n\n        return False, output\n\n    def submit_python(self, code: str) -&gt; Tuple[bool, str]:\n        \"\"\"Returns True if the submission was accepted, False otherwise.\n\n        The second element of the tuple is the output of the checker if the submission was rejected.\n        \"\"\"\n        return asyncio.run(self.async_submit(code))\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSEnvironment.submit_python","title":"submit_python","text":"<pre><code>submit_python(code: str) -&gt; Tuple[bool, str]\n</code></pre> <p>Returns True if the submission was accepted, False otherwise.</p> <p>The second element of the tuple is the output of the checker if the submission was rejected.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>def submit_python(self, code: str) -&gt; Tuple[bool, str]:\n    \"\"\"Returns True if the submission was accepted, False otherwise.\n\n    The second element of the tuple is the output of the checker if the submission was rejected.\n    \"\"\"\n    return asyncio.run(self.async_submit(code))\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSV0Solver","title":"DDOTSV0Solver","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for competitive programming problems live on DDOTS.</p> <p>This dataset will run agents in an Agent environment previously prepared.</p> <p>workspace/     .id             -- Id of the problem     PROBLEM.txt     -- Description of the problem</p> <p>The agent should call env.submit_python(code) to submit the code to the DDOTS server.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSV0Solver(SolverStrategy):\n    \"\"\"Solver strategy for competitive programming problems live on DDOTS.\n\n    This dataset will run agents in an Agent environment previously prepared.\n\n    workspace/\n        .id             -- Id of the problem\n        PROBLEM.txt     -- Description of the problem\n\n    The agent should call env.submit_python(code) to submit the code to the DDOTS server.\n\n    \"\"\"\n\n    def __init__(self, dataset_ref: Dataset, agents: str, max_iterations: int, save_snapshots: bool = False):  # noqa: D107\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        self.agents = [Agent.load_agent(agent, client_config) for agent in agents.split(\",\")]\n        self.max_iterations = max_iterations\n\n        date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        rnd_id = random.randint(10**8, 10**9 - 1)\n        self._saved_trajectories = DATA_FOLDER / \"data\" / \"ddots_v0_trajectories\" / f\"{date}_{rnd_id}\"\n        self._saved_trajectories.mkdir(parents=True, exist_ok=True)\n\n        self.save_snapshots = save_snapshots\n        print(\"Saving trajectories to\", self._saved_trajectories)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"ddots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"ddots_codeforces_small/v0\", \"datasets/ddots_codeforces_medium_A_B/v0\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        problem_id = datum[\"problem_id\"]\n        description = datum[\"description\"]\n\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        client = InferenceClient(client_config)\n        env = DDOTSEnvironment(self.agents, problem_id, description, client)\n        env.write_file(\".solved\", str(False))\n\n        try:\n            env.run(description, max_iterations=self.max_iterations)\n            env.write_file(\".solved\", str(env.solved))\n\n        except Exception as e:\n            print(f\"Error running task: {e}\")\n\n        finally:\n            if self.save_snapshots:\n                snapshot = env.create_snapshot()\n                with open(self._saved_trajectories / f\"{problem_id}.tar.gz\", \"wb\") as f:\n                    f.write(snapshot)\n\n        return env.solved\n</code></pre>"},{"location":"api/#nearai.solvers.gsm8k_solver","title":"gsm8k_solver","text":""},{"location":"api/#nearai.solvers.gsm8k_solver.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.hellaswag_solver","title":"hellaswag_solver","text":""},{"location":"api/#nearai.solvers.hellaswag_solver.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver","title":"lean_solver","text":""},{"location":"api/#nearai.solvers.lean_solver.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i+1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver.load_theorem","title":"load_theorem","text":"<pre><code>load_theorem(task: LeanDatum) -&gt; str\n</code></pre> <p>Use local copy of the repository.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>def load_theorem(task: LeanDatum) -&gt; str:\n    \"\"\"Use local copy of the repository.\"\"\"\n    repo = LeanGitRepo(task.url, task.commit)\n    theorem = Theorem(repo, task.filename, task.theorem)\n    with Dojo(theorem) as (_, state):\n        return state.pp\n</code></pre>"},{"location":"api/#nearai.solvers.livebench_solver","title":"livebench_solver","text":""},{"location":"api/#nearai.solvers.livebench_solver.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        assert \"/\" not in name\n        return name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.mbpp_solver","title":"mbpp_solver","text":""},{"location":"api/#nearai.solvers.mbpp_solver.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.mmlu_solver","title":"mmlu_solver","text":""},{"location":"api/#nearai.solvers.mmlu_solver.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.tests","title":"tests","text":""},{"location":"api/#nearai.tests.test_provider_models","title":"test_provider_models","text":""},{"location":"api/#nearai.tests.test_provider_models.TestMatchProviderModel","title":"TestMatchProviderModel","text":"<p>               Bases: <code>TestCase</code></p> <p>Unit tests for get_provider_namespaced_model.</p> Source code in <code>nearai/tests/test_provider_models.py</code> <pre><code>class TestMatchProviderModel(unittest.TestCase):\n    \"\"\"Unit tests for get_provider_namespaced_model.\"\"\"\n\n    def __init__(self, method_name=\"runTest\"):  # noqa: D107\n        super().__init__(method_name)\n        self.provider_models = ProviderModels(CONFIG.get_client_config())\n\n    def test_fireworks(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"fireworks::accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-v3-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"yi-01-ai/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n\n    def test_hyperbolic(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::StableDiffusion\"),\n            (\"hyperbolic\", \"hyperbolic::StableDiffusion\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n\n    def test_registry_with_multiple_providers(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"near.ai/llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n</code></pre>"},{"location":"contributing/","title":"Contribute to <code>nearai</code>","text":"<p>Everyone is welcome to contribute, and we value everybody's contribution. Code contributions are not the only way to help the community. Answering questions, helping others, and improving documentation are also immensely valuable.</p> <p>It also helps us if you spread the word! Reference the library in blog posts about the awesome projects it made possible, or even simply \u2b50\ufe0f the repository to say thank you.</p> <p>This guide was heavily inspired by the huggingface transformers guide to contributing.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to contribute","text":"<p>There are several ways you can contribute to <code>nearai</code>:</p> <ul> <li>Contribute to <code>nearai</code></li> <li>Ways to contribute</li> <li>Fixing outstanding issues</li> <li>Submitting a bug-related issue or feature request<ul> <li>Did you find a bug?</li> <li>Do you want a new feature?</li> </ul> </li> <li>Contribute Documentation</li> <li>Create a Pull Request<ul> <li>Pull request checklist</li> <li>Sync a forked repository with upstream main</li> </ul> </li> </ul>"},{"location":"contributing/#fixing-outstanding-issues","title":"Fixing outstanding issues","text":"<p>If you notice an issue with the existing code and have a fix in mind, feel free to start contributing and open a Pull Request!</p>"},{"location":"contributing/#submitting-a-bug-related-issue-or-feature-request","title":"Submitting a bug-related issue or feature request","text":"<p>Do your best to follow these guidelines when submitting a bug-related issue or a feature request. It will make it easier for us to come back to you quickly and with good feedback.</p>"},{"location":"contributing/#did-you-find-a-bug","title":"Did you find a bug?","text":"<p><code>nearai</code> is alpha software. This means there is a possibility of encountering issues in the code. With help from users like you who report problems, we can make it more robust and reliable.</p> <p>Before you report an issue, we would really appreciate it if you could make sure the bug was not already reported (use the search bar on GitHub under Issues). Your issue should also be related to bugs in the library itself, and not your code.</p> <p>Once you've confirmed the bug hasn't already been reported, please include the following information in your issue so we can quickly resolve it:</p> <ul> <li>What did you do?</li> <li>What did you expect to happen?</li> <li>What happened instead?</li> <li>Your OS type and version and Python, PyTorch and versions where applicable.</li> <li>A short, self-contained, code snippet that allows us to reproduce the bug in   less than 30s.</li> <li>The full traceback if an exception is raised.</li> <li>Attach any other additional information, like screenshots, you think may help.</li> </ul> <p>To get the OS and software versions automatically, run the following command:</p> <pre><code>uname -a\n</code></pre>"},{"location":"contributing/#do-you-want-a-new-feature","title":"Do you want a new feature?","text":"<p>If there is a new feature you'd like to see in <code>nearai</code>, please open an issue and describe:</p> <ol> <li>What is the motivation behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?</li> </ol> <p>Whatever it is, we'd love to hear about it!</p> <ol> <li>Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.</li> <li>Provide a code snippet that demonstrates the feature usage.</li> <li>If the feature is related to a paper, please include a link.</li> </ol>"},{"location":"contributing/#contribute-documentation","title":"Contribute Documentation","text":"<p>If you discover any errors or omissions in our documentation, please open an issue and describe:</p> <ul> <li>Which explanation or code snippet is incorrect</li> <li>What concept is not clear or missing</li> <li>If you know, what would be the correct explanation or code snippet</li> </ul> <p>If you think you can contribute a fix for the issue, please feel free to open a Pull Request.</p> <p>To preview your changes locally, you will need to install all the dependencies for the documentation, particularly:</p> <ul> <li><code>mkdocs</code></li> <li><code>mkdocs-material</code></li> <li><code>mkdocs-autorefs</code></li> <li><code>mkdocs-minify-plugin</code></li> <li><code>mkdocsstrings</code></li> </ul> <p>All these dependencies can be easily installed through <code>pip</code> or <code>poetry</code>:</p> pippoetry <pre><code>pip install mkdocs mkdocs-material mkdocs-autorefs mkdocs-minify-plugin \"mkdocstrings[python]\" \"mkdocs-material[imaging]\"\n</code></pre> <pre><code>poetry install --with docs\n</code></pre> <p>Then simply test your changes locally using <code>mkdocs serve</code></p> <p>Cairo Graphics</p> <p>If you encounter a problem with <code>cairo</code>, please follow the mkdocs-material Requirements Guide</p>"},{"location":"contributing/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Before writing any code, we strongly advise you to search through the existing PRs or issues to make sure nobody is already working on the same thing. If you are unsure, it is always a good idea to open an issue to get some feedback.</p> <p>You will need basic <code>git</code> proficiency to contribute to <code>nearai</code>. While <code>git</code> is not the easiest tool to use, it has the greatest manual. Type <code>git --help</code> in a shell and enjoy! If you prefer books, Pro Git is a very good reference. We also recommend asking any available AGI to help you with <code>git</code>.</p> <p>Follow the steps below to start contributing:</p> <ol> <li> <p>Fork the repository by    clicking on the Fork button on the repository's page. This creates a copy of the code    under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> </li> </ol> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/nearai.git\ncd nearai\ngit remote add upstream https://github.com/nearai/nearai.git\n</code></pre> <ol> <li>Create a new branch to hold your development changes:</li> </ol> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> <p>\ud83d\udea8 Do not work on the <code>main</code> branch!</p> <ol> <li> <p>Set up a development environment (follow steps in the README):</p> </li> <li> <p>Develop the features in your branch.</p> </li> </ol> <p>As you work on your code, you should make sure it functions as intended.</p> <p><code>nearai</code> relies on <code>ruff</code> and <code>mypy</code> to format and type check its source code    consistently. After you make your changes and are ready to PR them, ensure that    your code is formatted and type-checked by running:</p> <pre><code>./scripts/lint_format.sh\n</code></pre> <pre><code>./scripts/typecheck.sh\n</code></pre> <p>Once you're happy with your changes, add the changed files with <code>git add</code> and    record your changes locally with <code>git commit</code>:</p> <pre><code>git add modified_file.py\ngit commit\n</code></pre> <p>Please remember to write good commit    messages to clearly communicate the changes you made!</p> <p>To keep your copy of the code up to date with the original    repository, rebase your branch on <code>upstream/branch</code> before you open a pull request or if requested by a maintainer:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Push your changes to your branch:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <p>If you've already opened a pull request, you'll need to force push with the <code>--force</code> flag. Otherwise, if the pull request hasn't been opened yet, you can just push your changes normally.</p> <ol> <li> <p>Now you can go to your fork of the repository on GitHub and click on Pull Request to open a pull request. Make sure you tick off all the boxes on our checklist below. When you're ready, you can send your changes to the project maintainers for review.</p> </li> <li> <p>It's ok if maintainers request changes, it happens to our core contributors    too! So everyone can see the changes in the pull request, work in your local    branch and push the changes to your fork. They will automatically appear in    the pull request.</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<ul> <li>The pull request title should summarize your contribution.</li> <li>If your pull request addresses an issue, please mention the issue number in the pull request description to make sure they are linked (and people viewing the issue know you are working on it).</li> <li>To indicate a work in progress please prefix the title with <code>[WIP]</code>. These are useful to avoid duplicated work, and to differentiate it from PRs ready to be merged.</li> <li>Don't add any images, videos and other non-text files that'll significantly weigh down the repository. Instead, reference them by URL.</li> </ul>"},{"location":"contributing/#sync-a-forked-repository-with-upstream-main","title":"Sync a forked repository with upstream main","text":"<p>When updating the main branch of a forked repository, please follow these steps to avoid pinging the upstream repository which adds reference notes to each upstream PR, and sends unnecessary notifications to the developers involved in these PRs.</p> <ol> <li>When possible, avoid syncing with the upstream using a branch and PR on the forked repository. Instead, merge directly into the forked main.</li> <li>If a PR is absolutely necessary, use the following steps after checking out your branch:</li> </ol> <pre><code>git checkout -b your-branch-for-syncing\ngit pull --squash --no-commit upstream main\ngit commit -m '&lt;your message without GitHub references&gt;'\ngit push --set-upstream origin your-branch-for-syncing\n</code></pre>"},{"location":"decentralization/","title":"Decentralization","text":"<p>This document outlines process of progressive decentralization of NEAR AI capabilities.</p> <p>NEAR AI project approaches decentralization in a progressive way, starting with delivering best researcher, developer and user experience possible and progressively decentralizing and enabling privacy and trust features over time.</p> Component Objective Registry Decentralized storage with support of private and encrypted items Training / Fine-tuning Leveraging decentralized set of nodes to train models in a provable way Agent runner Using trusted execution environment to run agents privately and Agent memory Using FHE to do retrieval from encrypted storage Inter-agent communication Peer-to-peer protocol that supports identity,  payments and dispute resolution"},{"location":"inference/","title":"NEAR AI Inference API (OpenAI Compatible)","text":"<p>NEAR AI provides an OpenAI-compatible API for inference, allowing you to easily integrate powerful language models into your applications. This guide covers the basic endpoints and how to use them.</p> <p>Other examples available here: examples</p>"},{"location":"inference/#getting-started","title":"Getting Started","text":"<ol> <li>Install all dependencies</li> </ol> <p>a. using <code>pip</code>:</p> <pre><code># Create a virtual environment\npython -m venv nearai_env\n\n# Activate the virtual environment\n# On Windows:\n# nearai_env\\Scripts\\activate\n# On macOS and Linux:\nsource nearai_env/bin/activate\n\n# Install the package in editable mode\npip install -e .\n</code></pre> <p>b. using poetry:</p> <pre><code>poetry install\n</code></pre> <ol> <li> <p>Set up authentication:</p> </li> <li> <p>Log in to NEAR AI using the CLI: <code>nearai login</code></p> </li> <li> <p>The auth object will be saved in <code>~/.nearai/config.json</code></p> </li> <li> <p>Import the required libraries and set up the client</p> </li> </ol> <pre><code>import openai\nimport json\nimport os\nimport nearai\n\nhub_url = \"https://api.near.ai/v1\"\n\n# Login to NEAR AI Hub using nearai CLI.\n# Read the auth object from ~/.nearai/config.json\nauth = nearai.config.load_config_file()[\"auth\"]\nsignature = json.dumps(auth)\n\nclient = openai.OpenAI(base_url=hub_url, api_key=signature)\n</code></pre>"},{"location":"inference/#list-models","title":"List Models","text":"<p>To list available models, use the <code>models.list()</code> method:</p> <pre><code>models = client.models.list()\nprint(models)\n</code></pre> <p>Different providers have different models. The format is <code>provider::account/model_name/model_version</code>. To get all unique providers, do:</p> <pre><code>providers = set([model.id.split(\"::\")[0] for model in models])\nprint(providers)\n</code></pre>"},{"location":"inference/#create-a-chat-completion","title":"Create a Chat Completion","text":"<p>To create a chat completion, use the <code>chat.completions.create()</code> method. Here's an example:</p> <pre><code>completion = client.chat.completions.create(\n  model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n</code></pre> <p>This will send a request to the specified model with the given messages and return the model's response. The response can be accessed through the <code>choices</code> array in the returned object.</p>"},{"location":"inference/#error-handling","title":"Error Handling","text":"<p>When using the API, it's important to handle potential errors. Here's an example of how to implement basic error handling:</p> <pre><code>try:\n  completion = client.chat.completions.create(\n    model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n  )\n  print(completion.choices[0].message.content)\nexcept openai.APIError as e:\n  print(f\"An API error occurred: {e}\")\nexcept Exception as e:\n  print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"inference/#additional-features","title":"Additional Features","text":"<p>The NEAR AI Inference API also supports other features such as:</p> <ol> <li>Streaming responses</li> <li>Function calling</li> <li>Custom parameters (temperature, max_tokens, etc.)</li> </ol> <p>For more information on these features, please refer to the full API documentation.</p>"},{"location":"inference/#conclusion","title":"Conclusion","text":"<p>This guide covers the basics of using the NEAR AI Inference API. By following these steps, you should be able to authenticate, list models, and create chat completions. For more advanced usage and detailed information, please refer to the complete API documentation or explore the provided examples.</p>"},{"location":"near_events/","title":"Running Agents Based on Events from the NEAR Blockchain","text":""},{"location":"near_events/#overview","title":"Overview","text":"<p>The NEAR AI HUB monitors the latest blocks from the NEAR blockchain and can trigger agents when it detects <code>EVENT_JSON</code> entries following the <code>nearai</code> standard in transactions.</p>"},{"location":"near_events/#example-of-an-event-log-entry","title":"Example of an Event Log Entry","text":"<pre><code>{\n  \"standard\": \"nearai\",\n  \"version\": \"0.1.0\",\n  \"event\": \"run_agent\",\n  \"data\": [\n    {\n      \"message\": \"Hello from NEAR Blockchain\",\n      \"agent\": \"user.near/agent-name/latest\",\n      \"max_iterations\": null,\n      \"env_vars\": null,\n      \"signer_id\": \"account.near\",\n      \"referral_id\": null,\n      \"request_id\": null,\n      \"amount\": \"0\"\n    }\n  ]\n}\n</code></pre> <p>Example Transaction.</p> <p>When such an event is detected, the agent specified in the <code>agent</code> field (e.g., <code>user.near/agent-name/latest</code>) will be automatically triggered. The agent will receive a JSON string containing the following object as its input:</p> <pre><code>{\n  \"event\": \"run_agent\",\n  \"message\": \"...\",\n  \"receipt_id\": \"...\",\n  // Other fields from the `data` object in the logs.\n}\n</code></pre> <p>To allow your agent to be invoked in this way, add a function that parses the incoming user message as a JSON string. If the required values for <code>event</code> and <code>message</code> are present, it should take the appropriate actions. The agent is not required to trust the data sent by the NEAR AI HUB and can independently verify the blockchain by reading the necessary block based on the <code>receipt_id</code>.</p>"},{"location":"vector-stores/","title":"Vector Stores","text":"<p>================</p> <p>\u26a0\ufe0f Warning: This text was generated by NEAR AI using vector store example.</p>"},{"location":"vector-stores/#introduction","title":"Introduction","text":"<p>Vector Stores are a powerful feature in NEAR AI that allows you to store and manage large amounts of data in a vectorized format. This enables efficient searching and retrieval of data, making it ideal for applications such as natural language processing, image recognition, and more.</p>"},{"location":"vector-stores/#creating-a-vector-store","title":"Creating a Vector Store","text":"<p>To create a Vector Store, you can use the <code>client.beta.vector_stores.create</code> method, passing in a name for the store and any additional metadata.</p>"},{"location":"vector-stores/#example","title":"Example","text":"<pre><code>client = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n\n# Create a vector store\nvs = client.beta.vector_stores.create(name=\"example_vector_store\")\nprint(f\"Vector store created: {vs}\")\n</code></pre>"},{"location":"vector-stores/#uploading-files","title":"Uploading Files","text":"<p>To upload files to a Vector Store, you can use the <code>client.files.create</code> method, passing in the file contents and metadata.</p>"},{"location":"vector-stores/#example_1","title":"Example","text":"<pre><code># Upload a file to the vector store\nuploaded_file = client.files.create(\n    file=open(\"example_file.txt\", \"rb\"),\n    purpose=\"assistants\",\n)\nattached_file = client.beta.vector_stores.files.create(\n    vector_store_id=vs.id,\n    file_id=uploaded_file.id,\n)\nprint(f\"File uploaded and attached: {uploaded_file.filename}\")\n</code></pre>"},{"location":"vector-stores/#retrieving-files","title":"Retrieving Files","text":"<p>To retrieve a file from a Vector Store, you can use the <code>client.files.download</code> method, passing in the file ID.</p>"},{"location":"vector-stores/#example_2","title":"Example","text":"<pre><code># Retrieve a file from the vector store\nretrieved_file = client.files.download(uploaded_file.id)\nprint(f\"File retrieved: {retrieved_file}\")\n</code></pre>"},{"location":"vector-stores/#deleting-files","title":"Deleting Files","text":"<p>To delete a file from a Vector Store, you can use the <code>client.files.delete</code> method, passing in the file ID.</p>"},{"location":"vector-stores/#example_3","title":"Example","text":"<pre><code># Delete a file from the vector store\ndeleted_file = client.files.delete(uploaded_file.id)\nprint(f\"File deleted: {deleted_file}\")\n</code></pre>"},{"location":"vector-stores/#searching-the-vector-store","title":"Searching the Vector Store","text":"<p>To search a Vector Store, you can use the <code>client.post</code> method, passing in the search query and any additional metadata.</p>"},{"location":"vector-stores/#example_4","title":"Example","text":"<pre><code># Search the vector store\nsearch_query = \"example search query\"\nsearch_response = client.post(\n    path=f\"{base_url}/vector_stores/{vs.id}/search\",\n    body={\"query\": search_query},\n    cast_to=dict,\n)\nprint(f\"Search results for '{search_query}':\")\nprint(f\"- {search_response}\")\n</code></pre>"},{"location":"vector-stores/#obtaining-llm-responses","title":"Obtaining LLM Responses","text":"<p>To obtain LLM responses using a Vector Store, you can use the <code>inference.query_vector_store</code> method, passing in the Vector Store ID, search query, and any additional metadata.</p>"},{"location":"vector-stores/#example_5","title":"Example","text":"<pre><code>def generate_llm_response(messages, processed_results) -&gt; str:\n    SYSTEM_PROMPT = \"\"\"You're an AI assistant that writes technical documentation. You can search a vector store for \n    information relevant to the user's query. Use the provided vector store results to inform your response, but don't \n    mention the vector store directly.\"\"\"\n\n    vs_results = \"\\n=========\\n\".join(\n        [f\"{result.get('chunk_text', 'No text available')}\" for result in processed_results]\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        *messages,\n        {\n            \"role\": \"system\",\n            \"content\": f\"User query: {messages[-1]['content']}\\n\\nRelevant information:\\n{vs_results}\",\n        },\n    ]\n    return inference.completions(model=\"qwen2p5-72b-instruct\", messages=messages, max_tokens=16000)\n\n# Get an LLM response using the vector store\nsearch_query = \"example search query\"\nclient_config = ClientConfig(base_url=CONFIG.nearai_hub.base_url, auth=CONFIG.auth)\ninference = InferenceRouter(client_config)\nvector_results = inference.query_vector_store(vs.id, search_query)\nprocessed_results = process_vector_results([vector_results])\nllm_response = generate_llm_response(messages, processed_results)\nprint(llm_response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <p>Note: This is just a general example and you may need to modify it to fit your specific use case.</p>"},{"location":"vector-stores/#helpful-links","title":"Helpful links*:","text":"<ul> <li>Load local files into the vector store: vector_store.py</li> <li>Load a GitHub repository into the vector store: vector_store_from_source.py</li> <li>Create this help document: vector_store_build_doc.py</li> </ul> <p>* Helpful links were provided by the editor </p>"},{"location":"agents/quickstart/","title":"Quickstart a Python Agent","text":"<p>Welcome! NEAR AI Agents are programs that can act autonomously to solve a task, while adapting and reacting to their environment.</p> <p>NEAR AI agents can use various AI models, store data to remember past interactions, communicate with other agents, use tools to interact with the environment, and much more.</p> <p>In this Quickstart we will build our first agent on NEAR AI, and learn how to interact with it.</p>"},{"location":"agents/quickstart/#pre-requisites","title":"Pre-Requisites","text":"<p>To get started, you will need to have a Near account, and install the NEAR AI CLI:</p> piplocal <pre><code>python -m pip install nearai\n</code></pre> <pre><code>git clone git@github.com:nearai/nearai.git\ncd nearai\npip install -e .\n</code></pre> NEAR Account <p>If you do not have a Near account yet, you can create one using any of the wallets at the wallet portal. If you do not know which one to choose, we recommend you to use Bitte or Meteor Wallet</p> Python Version <p>If you do not have python, or your version is not compatible, we recommend you to use miniconda or pyenv to manage your installations, as they both allow you to easily switch between python versions.</p> pyenvconda <pre><code>pyenv install 3.11\npyenv local 3.11 # or use global\n</code></pre> <pre><code>conda create -n myenv python=3.11\nconda activate myenv\n</code></pre>"},{"location":"agents/quickstart/#login-to-near-ai","title":"Login to NEAR AI","text":"<p>To create a new agent, you first need to login using your Near account:</p> <pre><code>$&gt; nearai login\n\n# Example Response:\n# Please visit the following URL to complete the login process: https://auth.near.ai?message=Welcome+to+NEAR+AI&amp;nonce=&lt;xyzxyzxyzxyzx&gt;&amp;recipient=ai.near&amp;callbackUrl=http%3A%2F%2Flocalhost%3A63130%2Fcapture\n</code></pre> <p>You'll be prompted to visit a URL to authenticate with your Near account. Select your wallet (if you don't have a wallet, check our prerequisites), and login with it.</p> <p>After successfully login, you should see the screen below. Close it and return to your terminal.</p> <p></p> Tip <p>If you have already logged in on <code>near-cli</code>, you know your account's private key, or you have the credentials on another device, you can use the following commands to login:</p> <pre><code>### Login with NEAR Account ID Only\nnearai login --accountId name.near\n\n### Login with Account ID and Private Key\nnearai login --accountId name.near --privateKey key\n\n### Login Remotely (only displays the login URL)\nnearai login --remote\n</code></pre>"},{"location":"agents/quickstart/#creating-a-new-agent","title":"Creating a New Agent","text":"<p>Now that you are logged in, lets create your first AI agent, a simple agent called <code>hello-ai</code>:</p> <pre><code>nearai agent create --name hello-ai --description \"My First NEAR AI Agent\"\n\n# Example Response:\n# Agent created at: /Users/user/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1\n</code></pre> <p>This will create a local folder with some <code>metadata</code> that describes the agent, and a python file with the agent's logic. Let's interact with the agent before we dive into its code!</p> <p>Execute the following commands in your terminal:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local\n</code></pre> <p>An interactive session will start, where you can chat with your agent... talk to it for a while and use <code>exit</code> when you are ready to continue.</p>"},{"location":"agents/quickstart/#the-agent","title":"The Agent","text":"<p>The agent is defined by two files, both located at <code>~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1</code>: </p> <ol> <li><code>metadata.json</code>: Contains information about your agent, including its underlying model</li> <li><code>agent.py</code>: This is the code that executes each time your agent receives a prompt</li> </ol> <p>By default, the agent takes the role of a \"helpful assistant\", which receives the user input and responds to it using the Llama 3.1 70B Instruct (as defined in the default metadata).</p> agent.py<pre><code>from nearai.agents.environment import Environment\n\ndef run(env: Environment):\n    # A system message guides an agent to solve specific tasks.\n    prompt = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n\n    # Use the model set in the metadata to generate a response\n    result = env.completion([prompt] + env.list_messages())\n\n    # Store the result in the chat history\n    env.add_reply(result)\n\n    # Give the prompt back to the user\n    env.request_user_input()\n\nrun(env)\n</code></pre> Default metadata.json <p>By default, agents use the </p> <pre><code>{\n  \"name\": \"hello-ai\",\n  \"version\": \"1.0.0\",\n  \"description\": \"My First Agent\",\n  \"category\": \"agent\",\n  \"tags\": [],\n  \"details\": {\n    \"agent\": {\n      \"defaults\": {\n        \"model\": \"qwen2p5-72b-instruct\",\n        \"model_provider\": \"fireworks\",\n        \"model_temperature\": 1.0,\n        \"model_max_tokens\": 16384\n      }\n    }\n  },\n  \"show_entry\": false\n}\n</code></pre> <p>Tip</p> <p>You can change the model used by the agent by modifying the <code>metadata.json</code> file, check all the available models in the NEAR AI Hub.</p>"},{"location":"agents/quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You have created your first agent on NEAR AI. Now you can modify the agent's code to help you solve a specific task. To discover everything an agent can do we recommend you to explore the following sections:</p> <ul> <li> <p>Registry: NEAR AI has an open registry, where you can find agents created by the community and even publish your own.</p> </li> <li> <p>Threads: Agents execute in conversation threads, which can contain files, messages, and interactions with other agents.</p> </li> <li> <p>The Agent Environment: Agents have access to the environment object, which allows them to interact with the user, use AI models to make inferences, call other agents, use tools, and much more. </p> </li> <li> <p>Secrets: Agents can store secrets to access external services, like APIs, databases, or other services.</p> </li> </ul>"},{"location":"agents/registry/","title":"The Registry: Finding and Publishing Agents","text":"<p>NEAR AI agents can be stored in a common registry, allowing the community to share their creations.</p> <p>Let's take a look at how we can navigate the registry, download agents, and contribute our own agents to the ecosystem.</p> <p>Note</p> <p>The registry is backed by an S3 bucket with metadata stored in a database.</p>"},{"location":"agents/registry/#finding-an-agent","title":"Finding an Agent","text":"<p>There are two main ways to navigate the registry to find agents: through the Web AI Hub, or using the NEAR AI CLI:</p> <pre><code># List all agents\nnearai registry list --category agent\n</code></pre> Example Output <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                           \u2502 description             \u2502 tags  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 zavodil.near/ai16z-docs/1.03    \u2502 AI agent with AI16Z ... \u2502 agent \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatirons.near/common-tool...   \u2502 A library of common ..  \u2502 llama \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 jayzalowitz.near/example_a...   \u2502 Example agent           \u2502       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ...                             \u2502 ...                     \u2502 ...   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Tip</p> <p>You can run the agents directly on the Web AI Hub to see how they work</p>"},{"location":"agents/registry/#filtering-agents","title":"Filtering Agents","text":"<p>You can further filter the agents by the developer that created it (<code>--namespace</code>) or the tags (<code>--tags</code>) that were added to it.</p> <p>For example, to find all agents created by <code>gagdiez.near</code> with the tag <code>template</code>, you can run:</p> <pre><code>nearai registry list  --category agent \\\n                      --namespace gagdiez.near \\\n                      --tags template \\\n                      --show_all\n</code></pre> <p>Tip</p> <p>You can use the <code>info</code> command to get more details about a specific agent, for example:</p> <pre><code>nearai registry info gagdiez.near/hello-ai/latest\n</code></pre>"},{"location":"agents/registry/#downloading-an-agent","title":"Downloading an Agent","text":"<p>Once you find an agent that you would like to download, you can use the <code>download</code> command to pull it down to your local machine. The command expects an agent of the form:</p> <pre><code>nearai registry download &lt;account.near&gt;/&lt;agent_name&gt;/&lt;version&gt;\n</code></pre> <p>Where version can be a specific version number, or <code>latest</code> to download the most recent version, for example: </p> <pre><code># Download a hello world agent\nnearai registry download gagdiez.near/hello-ai/latest\n</code></pre> <p>By default, the agent will be downloaded to the <code>~/.nearai/registry</code> local directory, for example the agent above will be downloaded to <code>~/.nearai/registry/gagdiez.near/hello-ai/latest</code>.</p> <p>Tip</p> <p>The <code>--force</code> flag allows you to overwrite the local agent with the version from the registry.</p>"},{"location":"agents/registry/#uploading-an-agent","title":"Uploading an Agent","text":"<p>If you created an agent and would like to share it with others, you can upload it to the registry. To upload an agent, you must have a logged in.</p> <p>The <code>upload</code> command expects the path to the agent's local directory, for example:</p> <pre><code>nearai registry upload ~/.nearai/registry/&lt;your-account.near&gt;/&lt;agent_folder&gt;\n</code></pre> <p>The folder must contain an <code>agent.py</code> file, where the agent's logic is written, and a <code>metadata.json</code> file that holds information such as a description, tags, and the model used by the agent.</p> Example <code>metadata.json</code> file metadata.json<pre><code>{\n\"name\": \"hello-ai\",\n\"version\": \"0.0.1\",\n\"description\": \"A friendly agent\",\n\"category\": \"agent\",\n\"tags\": [],\n\"details\": {\n  \"agent\": {\n    \"defaults\": {\n      \"model\": \"llama-v3p1-70b-instruct\",\n      \"model_provider\": \"fireworks\",\n      \"model_temperature\": 1.0,\n      \"model_max_tokens\": 16384\n    }\n  }\n},\n\"show_entry\": true\n}\n</code></pre> <p>Tags</p> <p>Remember to add tags to your agent to make it easier for others to find it in the registry, for example:</p> <pre><code>{ \"tags\": [\"travel\", \"assistant\", \"vacation\"] }\n</code></pre> <p>Danger</p> <p>All files in this folder will be uploaded to the registry! Make sure you are not including any sensitive data</p> <p>Warning</p> <p>You can't remove or overwrite a file once it's uploaded, but you can hide the entire agent by setting the <code>\"show_entry\": false</code> field in the <code>metadata.json</code> file</p>"},{"location":"agents/running/","title":"Running an Agent","text":"<p>Agents can be run locally or remotely. When running locally, you can run them interactively or as a task. When running remotely, you can use the NEAR AI API to run them.</p>"},{"location":"agents/running/#running-the-agent-locally","title":"Running the Agent Locally","text":"<p>You can execute it in two different ways: interactively or as a task.</p> <p>Know that you can also run agents directly on the Web AI Hub, you don't need to download an agent if you just want to see how they work.</p> <p>Always Review the Code</p> <p>Agents can execute arbitrary code on your machine, so please always review the agent's code before running it!</p> <p>By default, you will find the agent's code on the local directory <code>~/.nearai/registry</code>, there, check the agent's <code>agent.py</code> file either by using the <code>cat</code> command or opening it in a text editor.</p> <pre><code># Checking gagdiez.near/hello-ai/latest code\ncd ~/.nearai/registry/gagdiez.near/hello-ai/latest\ncat agent.py\n</code></pre>"},{"location":"agents/running/#interactive-run","title":"Interactive Run","text":"<p>Interactive runs execute the agent on a loop, allowing you to chat with it interactively until you decide to exit (using the <code>exit</code> command), or quit the session using <code>ctrl+c</code>.</p> <pre><code># Download the agent from the registry\nnearai registry download gagdiez.near/hello-ai/latest\n\n# Running the agent by absolute path\nnearai agent interactive ~/.nearai/registry/gagdiez.near/hello-ai/latest --local\n</code></pre>"},{"location":"agents/running/#running-as-a-task","title":"Running as a Task","text":"<p>When running an agent as a task, we simply provide an input and let the agent execute it without any user interaction.</p> <pre><code>nearai agent task ~/.nearai/registry/gagdiez.near/hello-ai/latest \"write a poem about the sorrow of losing oneself, but end on a positive note\" --local\n</code></pre>"},{"location":"agents/running/#running-the-agent-remotely","title":"Running the Agent Remotely","text":"<p>Agents can be run through the <code>/thread/runs</code>, <code>/thread/{thread_id}/runs</code> or  <code>/agent/runs</code> endpoints. The /thread syntax matches the OpenAI / LangGraph API. The /agent syntax is NEAR AI specific.</p> <p>You will need to pass a signed message to authenticate. This example uses the credentials written by <code>nearai login</code> to your <code>~/.nearai/config.json</code> file.</p> <pre><code>auth_json=$(jq -c '.auth' ~/.nearai/config.json);\n\ncurl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header \"Authorization: Bearer $auth_json\" \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"new_message\":\"Build a backgammon game\",\n    \"max_iterations\": \"1\"\n  }\nEOF\n</code></pre> <p>The full message will look like this. A <code>thread_id</code> param can also be passed to continue a previous conversation.  <pre><code>curl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header 'Authorization: Bearer {\"account_id\":\"your_account.near\",\"public_key\":\"ed25519:YOUR_PUBLIC_KEY\",\"signature\":\"A_REAL_SIGNATURE\",\"callback_url\":\"https://app.near.ai/\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"A_UNIQUE_NONCE_FOR_THIS_SIGNATURE\"}' \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"thread_id\": \"a_previous_thread_id\",\n    \"new_message\":\"Build a backgammon game\", \n    \"max_iterations\": \"2\"\n  }\nEOF\n</code></pre></p> Remote results <p>The results of the /agent/runs endpoints are either an error or the resulting thread_id.</p> <p>\"thread_579e1cf3f42742c785218106\"</p> <p>Threads follow the OpenAI / LangGraph api standard. <code>/threads/{thread_id}/messages</code> will return the messages on the thread. See the full NEAR AI OpenAPI spec here: https://api.near.ai/openapi.json</p>"},{"location":"agents/secrets/","title":"Hub Secrets (WIP)","text":"<p>Secrets enhance the agent framework by allowing both the agent author and the agent user to send private information to the runner, which is managed and executed by us.</p> <ul> <li>Reading Secrets: To read secrets, user authentication via an NEAR account is required.</li> <li>Secrets Distribution: Secrets are only provided to our runner.</li> <li>Secrets Encryption: Secrets are encrypted in the database using a master key.</li> </ul> <p>The agent has two types of input variables: agent variables and user variables. Each type has public and secret variables.</p>"},{"location":"agents/secrets/#agent-public-vars","title":"Agent public vars","text":"<p><code>metadata.json/details/env_vars</code> - Provided by the agent author. These are publicly available parameters that can be modified during updates or forks.  For example: <code>api_url</code>.</p>"},{"location":"agents/secrets/#user-public-vars","title":"User public vars","text":"<p><code>env_vars</code> - Provided by the user via CLI or URL parameter.  For example: <code>refId</code> in https://app.near.ai/agents/casino.near/game/1?refId=ad.near</p> <p>We add secrets (which can be added for all versions of the agent or for a specific one).</p>"},{"location":"agents/secrets/#agent-private-vars","title":"Agent private vars","text":"<p>The agent author can add a secret for their agent.  For example: <code>Github_API_Token</code>.</p>"},{"location":"agents/secrets/#user-private-vars","title":"User private vars","text":"<p>The user can add a secret for a specific agent (if required by the agent author).  For example: <code>private_key</code>.</p> <p>In the end, all these data end up in env_vars as a single key-value object. If multiple agents are running, each agent only sees its own secrets.</p>"},{"location":"agents/secrets/#priority-of-records","title":"Priority of Records:","text":"<ul> <li>agent_vars: Agent variables from metadata have a lower priority than agent secrets.</li> <li>user_vars: User variables from URL/CLI have a higher priority than user secrets.</li> <li>final vars: User variables have a higher priority than agent variables.</li> </ul>"},{"location":"agents/secrets/#current-endpoints","title":"Current endpoints","text":"<ul> <li>api-url/v1/hub_secrets (GET)</li> <li>api-url/v1/create_hub_secret (POST)</li> <li>api-url/v1/remove_hub_secret (POST)</li> </ul>"},{"location":"agents/threads/","title":"Threads","text":"<p>Every agent execution happens within a conversation thread, which is isolated from other threads. Threads allow agents to maintain a message history and persist files in time so the user can continue the conversation later.</p> <p>Info</p> <p>You can find how agents persist messages and files in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/threads/#starting-a-thread","title":"Starting a Thread","text":"<p>If we start an agent without specifying an existing thread, a new thread is created. Let's try this by executing an agent using the interactive mode:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local\n\n&gt; Hello, my name is Guille, please remember it\n\n# Example Output:\n# ...\n# thread_id: thread_43c64803c0a948bc9a8eb8e8\n\n# Assistant: Nice to meet you, Guille! I've made a note of your name, so feel free to ask me anything or start a conversation, and I'll be sure to address you by your name throughout our chat. How's your day going so far, Guillermo?\n</code></pre> <p>We can see in the output that a new <code>thread_id</code> - <code>thread_43c64803c0a948bc9a8eb8e8</code> - was created for this conversation.</p>"},{"location":"agents/threads/#resuming-a-thread","title":"Resuming a Thread","text":"<p>If we want to resume a conversation thread with an agent, we can specify the thread ID when running the agent:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\n&gt; What is my name?\n\n# Assistant: Your name is Guille\n</code></pre>"},{"location":"agents/threads/#messages-and-files","title":"Messages and Files","text":"<p>Agents can access and add messages and files on each thread, learn more about it in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/env/calling_other_agents/","title":"Calling another agent","text":"<p>Agents can call other agents to interact with them using the <code>run_agent</code> method. To call an agent, we need to provide the agent's account, name, and version. Optionally, we can pass a query to the agent.</p> <pre><code>result = env.run_agent(\"travel.primitives.near\", \"trip-organizer\", \"latest\", query=\"Plan a two-day trip to Buenos Aires\", fork_thread=False)\nprint(result)\n\n# thread_312f2ea5e42742c785218106\n</code></pre> <p>The result of the <code>run_agent</code> method is a string containing the thread ID where the external agent executed.</p> <p>Shared Environment</p> <p>The agent being called will receive the thread environment, meaning it can access all the messages and files from the current conversation. Moreover, the called agent will be able to add messages and files to the current thread.</p>"},{"location":"agents/env/calling_other_agents/#thread-fork","title":"Thread Fork","text":"<p>The <code>run_agent</code> method has an optional <code>fork_thread</code> parameter to control whether the called agent should have access to the current thread's messages and files. By default, <code>fork_thread</code> is set to <code>False</code>.</p> <p></p> <p>If we do fork, the agent we are calling will work on a copy of the thread, meaning that they have access to all files and messages created so far, but any message or file they create will be part of their own thread.</p> <p>If we do not fork the thread, the called agent will work in the same thread as the current agent, meaning that they have access to all files and messages created so far, and any message or file they create will be part of the current thread.</p>"},{"location":"agents/env/inference/","title":"Inference","text":"<p>The <code>completion</code> method is used to run a prompt on a specific model, using a specific provider.</p> <p>If only the prompt is provided, the inference will be run on the model and provider specified in the agent's metadata.</p> <pre><code>messages = env.list_messages()\nresult = env.completion(messages)\n\nprint(\"Messages:\", messages)\nprint(\"Result:\", result)\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n</code></pre>"},{"location":"agents/env/inference/#overriding-the-default-model","title":"Overriding the Default Model","text":"<p>To run the inference on a model different from the default one, you can pass the <code>MODEL</code> or <code>PROVIDER::MODEL</code> as second argument:</p> <pre><code>messages = env.list_messages()\nresult = env.completion([prompt] + messages, \"fireworks::qwen2p5-72b-instruct\")\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! How can I assist you today? Is there something specific you'd like to talk about or any questions you have?\n</code></pre> <p>Tip</p> <p><code>completions</code>: returns the full llm response for more control#</p> <p>Using Models Locally: LangChain / LangGraph</p> <p>The example agent langgraph-min-example has metadata that specifies the <code>langgraph-0-1-4</code> framework to run on langgraph version 1.4. In addition, the agent.py  code contains an adaptor class, <code>AgentChatModel</code> that maps LangChain inference operations to <code>env.completions</code> calls.</p>"},{"location":"agents/env/messages_files/","title":"Messages and Files","text":"<p>Agents interact with the users through messages, and can also access and create files. This page provides an overview of how agents can work with messages and files.</p> Quick Overview <ul> <li>Each run of an agent is executed in a separate thread, which contains messages and files.</li> <li>The agent can access the messages in the current thread using <code>env.list_messages()</code>.</li> <li>The agent can save temporary files to track the progress of a task.</li> <li>By default, the entire message history is stored in a file named <code>chat.txt</code>. The agent can add messages there by using <code>env.add_reply()</code>.</li> <li>During its operation, the agent creates a file named <code>.next_agent</code>, which stores the role of the next participant expected in the dialogue (either <code>user</code> or <code>agent</code>) during the next iteration of the loop. The agent can control this value using <code>env.set_next_actor()</code>.</li> <li>The agent can use local imports from the home folder or its subfolders. It is executed from a temporary folder within a temporary environment.</li> </ul>"},{"location":"agents/env/messages_files/#thread-messages","title":"Thread Messages","text":"<p>The environment provides methods for agents to access and interact with the messages in the current thread. Messages are stored in a list, with each message containing an <code>id</code>, <code>content</code>, and <code>role</code> field.</p>"},{"location":"agents/env/messages_files/#accessing-messages","title":"Accessing Messages","text":"<p>Agents can access the messages from the current thread using the <code>list_messages</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  messages = env.list_messages()\n  print(messages)\n</code></pre> Example Output <pre><code>[{'id': 'msg_9b676ae4ad324ca58794739d', 'content': 'Hi', 'role': 'user'},\n  {'id': 'msg_58693367bcee42669a85cb69', 'content': \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'role': 'assistant'},\n  {'id': 'msg_16acda223c294213bc3c814e', 'content': 'help me decide how to decorate my house!', 'role': 'user'}]\n</code></pre>"},{"location":"agents/env/messages_files/#adding-messages","title":"Adding Messages","text":"<p>Agents can add new messages to the thread using the <code>add_reply</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  env.add_reply(\"I have finished\")\n</code></pre>"},{"location":"agents/env/messages_files/#files","title":"Files","text":"<p>Agents have access to two types of files through the environment:</p> <ol> <li>Those created within the current conversation thread</li> <li>Those uploaded with the agent to the registry</li> </ol>"},{"location":"agents/env/messages_files/#storing-data","title":"Storing Data","text":"<p>To create a new file in the thread we can use the <code>write_file</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n</code></pre> Where is the file stored? <p>When running the agent locally, a temporary folder will be created to store each thread data. We can check exactly where the file is stored by using the <code>python debugger</code>:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n  import ipdb; ipdb.set_trace()  # Call the ipdb debugger\n</code></pre> <p>After running the agent, we will be dropped into the debugger, where we can check the current working directory:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\nipdb&gt; import os; os.getcwd() # Check the current working directory\n'/private/var/folders/v6/pw4e3e3r5t6h8i9oihtd9d7d1234df/T/agent_7e312s678b987sa4vc4s2zxs2s1w1345'\n</code></pre> <p>We can see that the current working directory is a temporary folder. Go ahead and start the <code>agent</code> again without the <code>--thread_id</code> parameter, you will see that the working directory changes.</p>"},{"location":"agents/env/messages_files/#accessing-files","title":"Accessing Files","text":"<p>To list the files in the thread storage, we can use the <code>list_files_from_thread</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  files = env.list_files_from_thread()\n  content = env.read_file('file.txt')\n\n  print('Files:', files)\n  print('Content of file.txt:', content)\n</code></pre> Example Output <pre><code>Files [FileObject(id='file_31aab645e3214a13b402e321', bytes=12, created_at=1734733634, filename='file.txt', object='file', purpose='assistants', status='uploaded', status_details='File information retrieved successfully')]\n\nContent of file.txt hello thread\n</code></pre>"},{"location":"agents/env/messages_files/#logging","title":"Logging","text":"<ul> <li><code>add_system_log</code>: adds a system or environment log that is then saved into \"system_log.txt\".</li> <li><code>add_agent_log</code>: any agent logs may go here. Saved into \"agent_log.txt\".</li> </ul>"},{"location":"agents/env/overview/","title":"The Environment Object","text":"<p>Each time an agents executes it receives an environment, which gives it access to features such as:</p> <ul> <li>Retrieve messages in the conversation, both from the user and the agent</li> <li>Request input from the user</li> <li>Read and write files on the agent's storage</li> <li>Call other agents</li> </ul>"},{"location":"agents/env/tools/","title":"Tools &amp; Commands","text":"<p>NEAR AI supports function based tool calling where the LLM can decide to call one of the functions (Tools) that you pass it. You can register your own function or use any of the built-in tools (list_files, read_file, write_file, exec_command, query_vector_store, request_user_input).</p> <p>The tool registry supports OpenAI style tool calling and Llama style. When a llama model is explicitly passed to completion(s)_and_run_tools a system message is added to the conversation. This system message contains the tool definitions and instructions on how to invoke them  using <code>&lt;function&gt;</code> tags.</p> <p>To tell the LLM about your tools and automatically execute them when selected by the LLM, call one of these environment methods:</p> <ul> <li><code>completion_and_run_tools</code>: Allows tools to be passed and processes any returned tool_calls by running the tool</li> <li><code>completions_and_run_tools</code>: Handles tool calls and returns the full llm response.</li> </ul> <p>By default, these methods will add both the LLM response and tool invocation responses to the message list.  You do not need to call <code>env.add_message</code> for these responses. This behavior allows the LLM to see its call then tool responses in the message list on the next iteration or next run.  This can be disabled by passing <code>add_to_messages=False</code> to the method.</p> <ul> <li><code>get_tool_registry</code>: returns the tool registry, a dictionary of tools that can be called by the agent. By default it is populated with the tools listed above for working with files and commands plus <code>request_user_input</code>. To register a function as a new tool, call <code>register_tool</code> on the tool registry, passing it your function.  <pre><code>def my_tool():\n    \"\"\"A simple tool that returns a string. This docstring helps the LLM know when to call the tool.\"\"\"\n    return \"Hello from my tool\"\n\ntool_registry = env.get_tool_registry()\ntool_registry.register_tool(my_tool)\ntool_def = tool_registry.get_tool_definition('my_tool')\nresponse = env.completions_and_run_tools(messages, tools=[tool_def])\n</code></pre></li> </ul> <p>To pass all the built in tools plus any you have registered use the <code>get_all_tool_definitions</code> method. <pre><code>all_tools = env.get_tool_registry().get_all_tool_definitions()\nresponse = env.completions_and_run_tools(messages, tools=all_tools)\n</code></pre> If you do not want to use the built-in tools, use <code>get_tool_registry(new=True)</code> <pre><code>    tool_registry = env.get_tool_registry(new=True)\n    tool_registry.register_tool(my_tool)\n    tool_registry.register_tool(my_tool2)\n    response = env.completions_and_run_tools(messages, tools=tool_registry.get_all_tool_definitions())\n</code></pre></p>"},{"location":"agents/env/tools/#terminal-commands","title":"Terminal Commands","text":"<p>Agents have access to the local terminal through the environment, the following methods are available:</p> Method Description <code>list_terminal_commands()</code> Lists the history of terminal commands executed by the agent <code>exec_command(command)</code> Executes the terminal <code>command</code> and returns the output"},{"location":"agents/env/variables/","title":"Environment Variables","text":"<p>When working with agents, managing configuration parameters through environment variables can provide a flexible way to adjust settings without altering the underlying code. This approach is particularly useful when dealing with sensitive information or configuration that needs to be customized without modifying the agent's codebase.</p>"},{"location":"agents/env/variables/#storing-environment-variables","title":"Storing Environment Variables","text":"<p>Environment variables can be stored in a metadata.json file. Here\u2019s an example of how to structure this file:</p> <pre><code>{\n  \"details\": {\n    \"env_vars\": {\n      \"id\": \"id_from_env\",\n      \"key\": \"key_from_env\"\n    }\n  }\n}\n</code></pre>"},{"location":"agents/env/variables/#accessing-environment-variables-in-code","title":"Accessing Environment Variables in Code","text":"<p>In your agent\u2019s code, you can access these environment variables using Python\u2019s os module or by accessing the env_vars dictionary directly.</p> <p>To retrieve an environment variable in the agent code:</p> <pre><code># Using os.environ\nimport os\nvalue = os.environ.get('VARIABLE_NAME', None)\n\n# Or using globals()\nvalue = globals()['env'].env_vars.get('VARIABLE_NAME')\n</code></pre> <p>This allows users to fork the agent, modify the environment variables in <code>metadata.json</code>, and achieve the desired behavior without changing the code itself.</p>"},{"location":"agents/env/variables/#running-the-agent-with-environment-variables","title":"Running the agent with Environment Variables","text":"<p>You can also pass environment variables directly when launching the agent. This can be useful for overriding or extending the variables defined in <code>metadata.json</code> and handling Sensitive Information: If your agent needs to interact with APIs or services that require secret keys or credentials, you can pass these as environment variables instead of hardcoding them. This ensures that sensitive information is not exposed in publicly accessible code.</p> <p>To run the agent with environment variables, use the following command:</p> <pre><code>nearai agent interactive user.near/agent/1 --local --env_vars='{\"foo\":\"bar\"}'\n</code></pre>"},{"location":"agents/env/variables/#example","title":"Example","text":"<p>Consider an agent <code>zavodil.near/test-env-agent/1</code> that has configurable environment variables.</p>"},{"location":"assistants/integrate/","title":"Integrate NEAR AI assistant","text":"<p>NEAR AI offers a powerful Assistant that answers questions, queries other agents, and more. You can integrate the Assistant into your own applications by using the Assistant API.</p> <p>NEAR AI Assistants API is compatible with OpenAI Assistants API.</p>"},{"location":"assistants/integrate/#step-0-login-into-near-account","title":"Step 0: Login into NEAR account","text":""},{"location":"assistants/integrate/#javascript-client-side-useful-for-wallets","title":"JavaScript, client side. Useful for wallets.","text":"<p>From client side, you can use the following function to sign the message and get the required NEAR AI authorization token.</p> <pre><code>async function login(wallet, message, nonce, recipient, callbackUrl) {\n    const signedMessage = await wallet.signMessage({\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    });\n    return {\n        signature: signedMessage.signature,\n        accountId: signedMessage.accountId,\n        publicKey: signedMessage.publicKey,\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    };\n}\n\n// Generate nonce based on current time in milliseconds\nconst nonce = String(Date.now());\nconst recipient = YOUR_RECIPIENT_ADDRESS;\nconst callbackUrl = YOUR_CALLBACK_URL;\n\n// Example usage of login function\nconst auth = await login(wallet, \"Login to NEAR AI\", nonce, recipient, callbackUrl);\n</code></pre>"},{"location":"assistants/integrate/#python","title":"Python","text":"<p>In Python, we recommend using the <code>nearai</code> CLI to login into your NEAR account. More details here.</p> <pre><code>nearai login\n</code></pre>"},{"location":"assistants/integrate/#step-1-create-a-thread","title":"Step 1: Create a Thread","text":"<p>A Thread represents a conversation between a user and one or many Assistants. You can create a Thread when a user (or your AI application) starts a conversation with your Assistant.</p>"},{"location":"assistants/integrate/#create-a-thread","title":"Create a Thread","text":"<p>In JavaScript:</p> <pre><code>import OpenAI from \"openai\";\nconst openai = new OpenAI({\n    baseURL: \"https://api.near.ai/v1\",\n    apiKey: `Bearer ${JSON.stringify(auth)}`,\n});\n\nconst thread = await openai.beta.threads.create();\n</code></pre> <p>In Python:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    api_key=\"YOUR_NEARAI_SIGNATURE\",\n    base_url=\"https://api.near.ai/v1\",\n)\n\nthread = client.beta.threads.create()\n</code></pre>"},{"location":"assistants/integrate/#step-2-add-a-message-to-the-thread","title":"Step 2: Add a Message to the Thread","text":"<p>The contents of the messages your users or applications create are added as Message objects to the Thread. Messages can contain both text and files. There is a limit of 100,000 Messages per Thread and we smartly truncate any context that does not fit into the model's context window.</p>"},{"location":"assistants/integrate/#add-a-message-to-the-thread","title":"Add a Message to the Thread","text":"<p>In JavaScript:</p> <pre><code>const message = await openai.beta.threads.messages.create(\n  thread.id,\n  {\n    role: \"user\",\n    content: \"Help me plan my trip to Tokyo\"\n  }\n);\n</code></pre> <p>In Python:</p> <pre><code>message = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Help me plan my trip to Tokyo\"\n)\n</code></pre>"},{"location":"assistants/integrate/#step-3-create-a-run","title":"Step 3: Create a Run","text":"<p>Once all the user Messages have been added to the Thread, you can Run the Thread with any Assistant. Creating a Run uses the model and tools associated with the Assistant to generate a response. These responses are added to the Thread as assistant Messages.</p> <p>Runs are asynchronous, which means you'll want to monitor their status by polling the Run object until a terminal status is reached. For convenience, the 'create and poll' SDK helpers assist both in creating the run and then polling for its completion.</p>"},{"location":"assistants/integrate/#create-a-run","title":"Create a Run","text":"<p>In JavaScript:</p> <pre><code>const assistant_id = \"near.ai/assistant/0.0.1\"\nlet run = await openai.beta.threads.runs.createAndPoll(\n  thread.id,\n  { \n    assistant_id: assistant_id,\n  }\n);\n</code></pre> <p>In Python:</p> <pre><code>assistant_id = \"near.ai/assistant/0.0.1\"\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant_id,\n)\n</code></pre> <p>Once the Run completes, you can list the Messages added to the Thread by the Assistant.</p> <pre><code>if (run.status === 'completed') {\n  const messages = await openai.beta.threads.messages.list(\n    run.thread_id\n  );\n  for (const message of messages.data.reverse()) {\n    console.log(`${message.role} &gt; ${message.content[0].text.value}`);\n  }\n} else {\n  console.log(run.status);\n}\n</code></pre> <pre><code>if run.status == 'completed':\n    messages = client.beta.threads.messages.list(run.thread_id)\n    for message in messages:\n        print(f\"{message.role} &gt; {message.content[0].text.value}\")\nelse:\n    print(run.status)\n</code></pre> <p>You may also want to list the Run Steps of this Run if you'd like to look at any tool calls made during this Run.</p>"},{"location":"assistants/overview/","title":"Assistants/Agents API overview","text":"<p>The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. </p>"},{"location":"assistants/overview/#how-assistants-work","title":"How Assistants work","text":"<p>The Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks.</p> <p>The Assistants API is in beta and we are actively working on adding more functionality.</p> <ol> <li>Assistants can call various models with specific instructions to tune their personality and capabilities.</li> <li>Assistants can access multiple tools.</li> <li>Assistants can access persistent Threads. Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model\u2019s context length. You create a Thread once, and simply append Messages to it as your users reply.</li> <li>Assistants can access files in several formats \u2014 either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create.</li> </ol>"},{"location":"assistants/overview/#key-concepts","title":"Key Concepts","text":"Object What it represents Assistant Purpose-built AI that uses various models and calls tools Thread A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model\u2019s context. Message A message created by an Assistant or a user. Messages can include text, images, and other files. Messages stored as a list on the Thread. Run An invocation of an Assistant on a Thread. The Assistant uses its configuration and the Thread\u2019s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread. Run Step A detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create Messages during its run. Examining Run Steps allows you to introspect how the Assistant is getting to its final results."},{"location":"models/benchmarks_and_evaluations/","title":"Benchmarks and Evaluations","text":"<p><code>Benchmarks</code> allow you to compare different agents and solvers on specific tasks, so you can determine which is the best fit for your needs.</p> <p><code>Evaluations</code> are the results of running benchmarks. They are stored in the registry and can be used to compare different agents and solvers.</p>"},{"location":"models/benchmarks_and_evaluations/#how-is-a-benchmark-implemented","title":"How is a benchmark implemented?","text":"<p>A benchmark is the combination of a dataset and a solver (more on this below). </p> <p>Once you have created a dataset and its solver, you can run the benchmark using the <code>benchmark</code> command.</p> <p>For example, to run the <code>mpbb</code> benchmark on the <code>llama v3</code>, you can use:</p> <pre><code>nearai benchmark run mbpp MBPPSolverStrategy \\\n    --model llama-v3-70b-instruct \\\n    --subset=train \\\n    --max_concurrent=1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#adding-a-benchmark-dataset","title":"Adding a benchmark dataset","text":"<p><code>nearai</code> leverages huggingface datasets as the primitive when operating with datasets + benchmarks (see <code>load_dataset</code>). This means that to add a new benchmark, you need to create a new dataset and register it with the <code>nearai</code> registry (we will go over this in Implementing the \"3 digit addition\" benchmark).</p> <p>There is also a support for datasets of custom format.</p>"},{"location":"models/benchmarks_and_evaluations/#adding-a-solver","title":"Adding a solver","text":"<p>To implement a solver, you will need to implement the SolverStrategy interface under the <code>nearai.solvers</code> module. The most important method the solver should implement is the <code>solve</code> method. This method should take a datum, run your implementation specific agentic strategy / strategies, and return a result.</p>"},{"location":"models/benchmarks_and_evaluations/#implementing-the-3-digit-addition-benchmark","title":"Implementing the \"3 digit addition\" benchmark","text":"<p>In this section we will be implementing a benchmark we'll call \"3 digit addition\". The goal of the benchmark is to test an agents ability to add two 1-3 digit numbers together. The dataset will consist of 1000 examples of 3 digit addition problems and their solutions. The solver will adjudicate the agent answers and return a single accuracy score. While this benchmark is simple and can be solved with a simple program, it serves as a good example of how to implement a benchmark in <code>nearai</code>.</p>"},{"location":"models/benchmarks_and_evaluations/#step-1-creating-the-dataset","title":"Step 1: Creating the dataset","text":"<p>To create this dataset, we will first synthetically generate the data. We will then register the dataset with the <code>nearai</code> registry.</p> <pre><code>import random\nfrom itertools import product\n\nimport datasets\n\nSAMPLE_SIZE = 1000\nSEED = 42\nPATH = \"3_digit_addition\"\n\nrandom.seed(SEED)\ndatasets.Dataset.from_generator(\n    lambda: iter(\n        {\n            \"input\": f\"{a} + {b}\",\n            \"output\": str(a + b)\n        }\n        for a, b in random.sample(list(product(range(1000), range(1000))), SAMPLE_SIZE)\n    ),\n    features=datasets.Features(\n        {\n            \"input\": datasets.Value(\"string\"),\n            \"output\": datasets.Value(\"string\")\n        }\n    )\n).save_to_disk(PATH)\n</code></pre> <p>Now to upload the dataset to the registry we'll run the command:</p> <pre><code>nearai registry upload ./3_digit_addition\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#step-2-creating-the-solver","title":"Step 2: Creating the solver","text":"<p>To create the solver, we will implement the <code>SolverStrategy</code> interface. The solver will take in a datum, parse the input, execute any setup for the agent, run the agent, and return the correctness of the agents result.</p> Remember <p>To ensure this solver is registered with <code>nearai</code>:</p> <ol> <li>Write this implementation in the <code>nearai.solvers</code> module.</li> <li>Import it in the <code>__init__.py</code> file in the <code>nearai.solvers</code> module.</li> </ol> <pre><code># ... other imports ...\nfrom pydantic import BaseModel\nfrom huggingface import Dataset\nfrom nearai.solvers import SolverStrategy\n\nfrom typing import Dict, List\n\nclass ThreeDigitAdditionDatum(BaseModel):\n    input: str\n    output: str\n\nclass ThreeDigitAdditionSolver(SolverStrategy):\n    \"\"\"Solver for the 3 digit addition benchmark.\"\"\"\n\n    def __init__(self, dataset_ref: Dataset, model: str = \"\", agent: str = \"\"):\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:\n        return \"3_digit_addition\"\n\n    def compatible_datasets(self) -&gt; List[str]:\n        return [\"3_digit_addition\"]\n\n    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result only.\"\"\"\n        result = session.run_task(goal).strip()\n        return result == datum.output\n</code></pre> <p>The code above can run for both models and agents. If both <code>model</code> and <code>agent</code> are given, the <code>model</code> value will be inserted into <code>agent</code> metadata.</p> <p>To check agent functionality to write files: <pre><code>    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result in a file called 'result.txt'.\"\"\"\n        session.run_task(goal)\n        with open(os.path.join(session.path, \"result.txt\"), \"r\") as f:\n            result = f.read().strip()\n        return result == datum.output\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#step-3-running-the-benchmark","title":"Step 3: Running the benchmark","text":"<p>To run the benchmark, we will use the <code>nearai</code> CLI. We will specify the dataset and solver we want to use.</p> <pre><code>nearai benchmark run near.ai/3_digit_addition/1.00 ThreeDigitAdditionSolver --agent ~/.nearai/registry/&lt;my_agent&gt;\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#benchmarks-cache","title":"Benchmarks Cache","text":"<p>Benchmark individual tasks and completion are cached in registry or locally. To see registry benchmark completion caches:</p> <pre><code>nearai benchmark list\n</code></pre> <p>To force execution and overwrite cache pass <code>--force</code> flag.</p> <pre><code>nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --force\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#example-runs","title":"Example runs","text":"<pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\n$ nearai benchmark run near.ai/mmlu/1.0.0 MMLUSolverStrategy --model 'llama-v3p1-405b-instruct' --subset test\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'qwen2p5-72b-instruct' --subset test --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n$ nearai benchmark run near.ai/live_bench/1.0.0 LiveBenchSolverStrategy --model 'qwen2p5-72b-instruct' --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#evaluations","title":"Evaluations","text":""},{"location":"models/benchmarks_and_evaluations/#recording-benchmark-result-as-an-evaluation","title":"Recording benchmark result as an evaluation","text":"<p>To record benchmark results as an evaluation, pass <code>--record</code>. It is strongly recommended to pass this flag after verifying successful run of the benchmark.</p> <pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\nFinal score: 131/500 - 26.20%\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --record\n</code></pre> <p>That creates new evaluation entry in the registry: <pre><code>$ nearai registry list --category=evaluation\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                                                                  \u2502 category   \u2502 description   \u2502 tags   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 alomonos.near/evaluation_mbpp_model_llama-v3p2-1b-                     \u2502 evaluation \u2502               \u2502        \u2502\n\u2502 instruct_provider_fireworks/0.0.1                                      \u2502            \u2502               \u2502        \u2502\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#view-evaluation-table","title":"View evaluation table","text":"<p>To view evaluation table in CLI: <pre><code>$ nearai evaluation table --num_columns=8\n$ nearai evaluation table --all_key_columns --num_columns=8\n$ nearai evaluation table --all_metrics\n</code></pre></p> <p>https://app.near.ai/evaluations has a functionality to choose any columns.</p>"},{"location":"models/benchmarks_and_evaluations/#submit-an-experiment","title":"Submit an experiment","text":"<p>To submit a new experiment run:</p> <pre><code>nearai submit --command &lt;COMMAND&gt; --name &lt;EXPERIMENT_NAME&gt; [--nodes &lt;NUMBER_OF_NODES&gt;] [--cluster &lt;CLUSTER&gt;]\n</code></pre> <p>This will submit a new experiment. The command must be executed from a folder that is a git repository (public github repositories, and private github repositories on the same organization as nearai are supported). The current commit will be used for running the command so make sure it is already available online. The diff with respect to the current commit will be applied remotely (new files are not included in the diff).</p> <p>On each node the environment variable <code>ASSIGNED_SUPERVISORS</code> will be available with a comma separated list of supervisors that are running the experiment. The current supervisor can be accessed via <code>nearai.CONFIG.supervisor_id</code>. See examples/prepare_data.py for an example.</p>"},{"location":"models/benchmarks_and_evaluations/#issues","title":"Issues","text":"<ul> <li>Overwriting existing evaluation entry is currently not supported</li> <li>litellm.Timeout errors when running benchmark</li> <li>Feature request: tag individual evaluation metrics</li> <li>Feature request: add view for a metric</li> <li>Feature request: add cost of running benchmark to evaluation results as a separate metric</li> <li>Feature request: hide evaluation results for hidden agents and models</li> <li>Capabilities Benchmarks Tracking: list of benchmarks we want to add</li> </ul>"},{"location":"models/fine_tuning/","title":"<code>nearai</code> fine-tuning guide","text":"<p>As a part of the <code>nearai</code> project, we provide a collection of tools to fine-tune and evaluate models. Fine-tuning is a set of techniques to tune model parameters in a parameter-efficient way to improve model performance on specific tasks. More commonly, fine-tuning is used to modify the behavior of a pre-trained model. Some examples of this are to produce structured output (JSON, XLM, etc), to produce stylized output (poetic, neutral, etc), or to respond properly to instruction based prompts.</p> <p>In this guide, we will walk through the process of fine-tuning <code>llama-3-8b-instruct</code> on the orca-math-word-problems-200k dataset to improve its performance on the gsm8k benchmark.</p>"},{"location":"models/fine_tuning/#step-1-create-the-dataset","title":"Step 1: Create the dataset","text":"<p>The two datasets we will be using are orca-math-word-problems-200k and gsm8k. Both datasets are a collection of word based math problems + answers. For convenience, we will download the datasets from huggingface, save it to disk, and then upload it into the <code>nearai</code> registry.</p> <pre><code>import re\nfrom textwrap import dedent\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\nds_math_word_problems = load_dataset(\"HuggingFaceH4/orca-math-word-problems-200k\")\nds_gsm8k = load_dataset(\"openai/gsm8k\", \"main\")['train']\n\n## create new column by concatenating the 'question' and 'answer' columns\ndef to_q_a(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": x[\"answer\"]\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_math_word_problems = ds_math_word_problems.map(to_q_a)\n\ndef to_q_a_gsm8k(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": re.sub(r\"&lt;&lt;.*?&gt;&gt;\", \"\", x[\"answer\"])\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_gsm8k = ds_gsm8k.map(to_q_a_gsm8k)\n\n## combine the datasets\nds_combined = concatenate_datasets([ds_math_word_problems['train_sft'], ds_gsm8k])\nds_combined = ds_combined.remove_columns([col for col in ds_combined.column_names if col != \"question_and_answer\"])\n\n# Add a split on 'train'\nds_combined = ds_combined.train_test_split(test_size=0.0001, seed=42)\nds_dict = DatasetDict({\n    'train': ds_combined['train'],\n    'validation': ds_combined['test']\n})\nds_dict.save_to_disk(\"orca_math_gsm8k_train\")\n</code></pre> <pre><code>nearai registry upload ./orca_math_gsm8k_train\n</code></pre>"},{"location":"models/fine_tuning/#step-2-fine-tune-the-model","title":"Step 2: Fine-tune the model","text":"<p>Under the hood, <code>nearai</code> uses torchtune to manage the fine-tuning process. To launch a fine-tuning job you can use <code>nearai finetune</code>.</p> <p>Here is the command we used to fine-tune <code>llama-3-8b-instruct</code> on our combined <code>orca-math-word-problems-200k</code> and <code>gsm8k</code> dataset on an 8-GPU machine:</p> <pre><code>poetry run python3 -m nearai finetune start \\\n    --model llama-3-8b-instruct \\\n    --format llama3-8b \\\n    --tokenizer llama-3-8b-instruct/tokenizer.model \\\n    --dataset ./orca_math_gsm8k_train \\\n    --method nearai.finetune.text_completion.dataset \\\n    --column question_and_answer \\\n    --split train \\\n    --num_procs 8\n</code></pre> <p>To change the configuration of the fine-tuning job, edit <code>etc/finetune/llama3-8b.yml</code>.</p> <p>Included in the output of the command is the path to the fine-tuned model checkpoint. In this case, the path was <code>~.nearai/finetune/job-2024-08-29_20-58-08-207756662/checkpoint_output</code>. The path may/will be different based on the time you run the command.</p>"},{"location":"models/fine_tuning/#step-3-serve-the-fine-tuned-model","title":"Step 3: Serve the fine-tuned model","text":"<p>To serve fine-tuned models, we use vllm. Once we serve the fine-tuned model + the baseline model, we will benchmark it against both.</p> <pre><code>poetry run python3 -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --enable-lora \\\n    --lora-modules mynewlora=&lt;path_to_checkpoint&gt; \\\n    --tensor-parallel 8\n</code></pre> <p>Now we will run the <code>gsm8k</code> benchmark on both the baseline model and the fine-tuned model using <code>nearai benchmark</code>. The solvers will call our fine-tuned model and the baseline model through the vllm server.</p> <pre><code>python3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::meta-llama/Meta-Llama-3-8B-Instruct\n\npython3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::mynewlora\n</code></pre> <p>And we can see the results of the benchmark. And we can see that the fine-tuned model performs better than the baseline model.</p> <pre><code># meta-llama/Meta-Llama-3-8B-Instruct\nCorrect/Seen - 1061/1319 - 80.44%\n\n# fine tuned llama3-8b-instruct\nCorrect/Seen - 966/1319 - 73.24%\n</code></pre> <p>From these results, we can see that our fine-tuned model needs improvement to perform better than the baseline model.</p>"}]}