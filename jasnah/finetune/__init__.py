import os
import threading
import time
from collections import defaultdict
from copy import deepcopy
from pathlib import Path
from random import randint
from subprocess import run
from typing import Optional

import jasnah
from jasnah import timestamp
from jasnah.config import CONFIG, DATA_FOLDER, ETC_FOLDER
from jasnah.dataset import get_dataset
from jasnah.model import get_model
from jasnah.registry import registry
from jasnah.server import ServerClient


class FinetuneCli:
    def submit(
        self,
        model: str,
        tokenizer: str,
        dataset: str,
        num_procs: int,
        num_nodes: int = 1,
        job_id: Optional[str] = None,
        checkpoint: Optional[str] = None,
        epochs: int = 1,
    ):
        """Submit a finetuning job to the cluster"""
        client = ServerClient(CONFIG.server_url)

        result = client.submit(
            "finetune-task",
            "https://github.com/nearai/jasnah-cli.git",
            "main",
            f"jasnah-cli finetune start --model {model} --tokenizer {tokenizer} --dataset {dataset} --num_procs {num_procs} --num_nodes {num_nodes} --job_id {job_id} --checkpoint {checkpoint} --epochs {epochs}",
            CONFIG.user_name,
            None,
            num_nodes,
        )

        print(result)

    def start(
        self,
        model: str,
        tokenizer: str,
        dataset: str,
        num_procs: int,
        format: str,
        upload_checkpoint: bool = True,
        num_nodes: int = 1,
        job_id: Optional[str] = None,
        checkpoint: Optional[str] = None,
        **dataset_kwargs,
    ):
        """Start a finetuning job on the current node

        Args:

            model (str): Name of a model in the registry. Base model to finetune.
            tokenizer (str): Name of a tokenizer in the registry. Using tokenizer.model format.
            dataset (str): Name of a dataset in the registry.
            num_procs (int): Number of GPUs to use for training
            format (str): Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.
            num_nodes (int): Number of nodes to use for training. Default is 1.
            checkpoint (str): Name of the model checkpoint to start from. Default is None.
            dataset_kwargs (Dict[str, Any]): Additional keyword arguments to pass to the dataset constructor.
        """
        assert num_nodes >= 1

        # Prepare job id folder
        if job_id is None:
            job_id = "job"
        job_id = f"{job_id}-{timestamp()}-{randint(10**8, 10**9 - 1)}"
        job_folder = DATA_FOLDER / "finetune" / job_id
        job_folder.mkdir(parents=True, exist_ok=True)

        # Either use the provided config file template or load one predefined one
        if Path(format).exists():
            config_template_path = Path(format)
        else:
            configs = ETC_FOLDER / "finetune"
            config_template_path = configs / f"{format}.yml"

        if not config_template_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_template_path}")

        CONFIG_TEMPLATE = config_template_path.read_text()

        # Download model
        model_path = get_model(model)

        # Download tokenizer
        tokenizer_path = registry.download(tokenizer) / "tokenizer.model"
        assert tokenizer_path.exists(), f"tokenizer.model not found in {tokenizer_path}"

        # Download checkpoint if any
        checkpoint_path = get_model(checkpoint) if checkpoint else "null"
        resume_checkpoint = checkpoint_path != "null"

        # Download dataset
        dataset_path = get_dataset(dataset)

        # Set up output directories
        checkpoint_output_dir = job_folder / "checkpoint_output"
        logging_output_dir = job_folder / "logs"
        logging_output_dir.mkdir(parents=True, exist_ok=True)

        # Prepare config file
        dataset_args_dict = deepcopy(dataset_kwargs)

        dataset_args_dict["_component_"] = dataset_args_dict.pop("method")
        dataset_args_dict["source"] = str(dataset_path.absolute())
        dataset_args = "\n".join(f"  {key}: {value}" for key, value in dataset_args_dict.items())

        config = job_folder / "config.yaml"
        with open(config, "w") as f:
            f.write(
                CONFIG_TEMPLATE.format(
                    TOKENIZER=str(tokenizer_path),
                    MODEL=str(model_path),
                    RECIPE_CHECKPOINT=checkpoint_path,
                    RESUME_FROM_CHECKPOINT=resume_checkpoint,
                    CHECKPOINT_OUTPUT_DIR=str(checkpoint_output_dir),
                    DATASET_ARGS=dataset_args,
                    LOGGING_OUTPUT_DIR=str(logging_output_dir),
                )
            )

        # Spawn background thread to read logs and push to database
        threading.Thread(target=find_new_logs_background, args=(logging_output_dir, job_id)).start()

        print("Starting job at", job_folder)
        if num_nodes == 1:
            run(
                [
                    "tune",
                    "run",
                    "--nproc_per_node",
                    str(num_procs),
                    "lora_finetune_distributed",
                    "--config",
                    str(config),
                ]
            )
        else:
            # Fetch rank and master addr from environment variables
            raise NotImplementedError()

        global BACKGROUND_PROCESS
        BACKGROUND_PROCESS = False

        description = f"Fintuned {model} on {dataset} using {tokenizer} GPUs"

        if upload_checkpoint:
            registry.upload(
                path=job_folder,
                s3_path=f"checkpoints/finetune/{job_id}",
                author=CONFIG.user_name,
                description=description,
                name=job_id,
                details=dict(
                    model=model,
                    tokenizer=tokenizer,
                    dataset=dataset,
                    num_procs=num_procs,
                    format=format,
                    num_nodes=num_nodes,
                    checkpoint=checkpoint,
                    **dataset_kwargs,
                ),
                # By default the entry is not shown when using jasnah-cli registry list
                # but the entry is still accessible.
                show_entry=False,
                tags=["finetune"],
            )

    def inspect(self, job_id: str):
        raise NotImplementedError()


read_logs = defaultdict(int)
BACKGROUND_PROCESS = True


def parse_line(line):
    """
    Example of line to be parsed

    Step 33 | loss:1.5400923490524292 lr:9.9e-05 tokens_per_second_per_gpu:101.22285588141214
    """
    step, metrics = map(str.strip, line.strip(" \n").split("|"))
    step = int(step.split(" ")[-1])
    metrics = {metric[0]: float(metric[1]) for metric in map(lambda metric: metric.split(":"), metrics.split(" "))}
    return step, metrics


def find_new_logs(path: Path, experiment_id: str):
    for file in os.listdir(path):
        file_path = os.path.join(path, file)

        if not os.path.isfile(file_path):
            continue

        if not file.endswith(".txt"):
            continue

        read_lines = read_logs[file]
        num_lines = 0

        with open(file_path) as f:
            for line in f:
                num_lines += 1
                if num_lines <= read_lines:
                    continue

                try:
                    line = line.strip(" \n")
                    step, metrics = parse_line(line)
                    jasnah.log(target="tensorboard", step=step, experiment_id=experiment_id, **metrics)
                except:
                    continue

        read_logs[file] = num_lines


def find_new_logs_background(path: Path, experiment_id: str):
    while BACKGROUND_PROCESS:
        find_new_logs(path, experiment_id)
        time.sleep(1)
    find_new_logs(path, experiment_id)
